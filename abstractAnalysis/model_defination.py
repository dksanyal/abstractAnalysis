from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score
from keras_contrib.layers import CRF
from keras.layers import Dense, Input, Lambda, merge, dot, Subtract, Flatten, LSTM, CuDNNLSTM, BatchNormalization, Layer
from keras.layers.embeddings import Embedding
from keras.layers.recurrent import GRU
from keras.layers.core import Dropout, Activation, Reshape
from keras.layers.wrappers import Bidirectional, TimeDistributed
from keras.layers.merge import Concatenate, Add, Multiply
from keras.utils import plot_model
from keras.models import Model, model_from_json, load_model
from keras import initializers, regularizers, constraints, optimizers, layers
import keras.backend as K
import traceback, pdb
import numpy as np
import model_defination


class abstract_segmentation_model():
	def __init__(self, model_name, dataset, embeddings):
		self.model 			= self.get_model(model_name, dataset, embeddings)
		self.save_location 	= 'saved_model/model_weights'

	def get_model(self, model_name, dataset, embeddings):
		# pdb.set_trace()
		try:
			return getattr(model_defination, model_name)().get_model_defination(dataset, embeddings)
		except AttributeError as e:
			return 	model_initial().get_model_defination(dataset, embeddings)

	def save_weights(self, sufix = ''):
		self.model.save_weights(self.save_location+sufix+".h5")
		# self.model.save('saved_model/model'+sufix+'.h5')
		print("Saved model and model weights to disk")

	def load_weights(self, sufix= ''):
		try:
			from pathlib import Path
			saved_model = Path(self.save_location+sufix+".h5")
			if saved_model.is_file():
				self.model.load_weights(self.save_location+sufix+".h5")
				print("Loaded model weights from disk")
				return #self.model
			else :
				print("Saved weights not found.")
		except Exception as e:
			print("Saved weights incompatable.")
			# traceback.print_exc()
			# pdb.set_trace() 
			return #None

	def get_results(self, pred, gold):
		non_pad_pred 	= []
		non_pad_gold	= []
		for (i,abstract) in enumerate(gold):
			 for (j,label) in enumerate(abstract):
			 	g_l = np.argmax(label)  
			 	if g_l != 0:
			 		p_l = np.argmax(pred[i,j])
			 		non_pad_pred.append(p_l)
			 		non_pad_gold.append(g_l)
		CM 	= confusion_matrix(non_pad_gold, non_pad_pred)
		acc = accuracy_score(non_pad_gold, non_pad_pred)

		return acc, CM, non_pad_pred, non_pad_gold


# class model_stub():
# 	def

## Dernoncourt et al.

class model_initial():
	def get_model_defination(self, dataset, embeddings):
		# try:
					
		# Build the model
		print('Building the model...')
		
		main_input = Input(shape=[dataset.abs_len, dataset.maxlen], dtype='int32', name='input') # (None, 36)
		char_input = Input(shape=[dataset.abs_len, dataset.maxlen,  dataset.maxlen_word], dtype='int32', name='char_input') # (None, 36, 25)
		# print 'passed checkpoint 1: input\n\n'
		# pdb.set_trace()
		main_input_r = Lambda(lambda x: K.reshape(x, shape=(-1, dataset.maxlen)))(main_input)

		char_input_r = Lambda(lambda x: K.reshape(x, shape=(-1, dataset.maxlen, dataset.maxlen_word)))(char_input)

		embeds, _, _  = embeddings.init_weights(dataset.idx2word)

		# print 'passed checkpoint 2: embedding init\n\n'

		embed = Embedding(input_dim= dataset.vocsize, output_dim= embeddings.embed_dim, input_length= dataset.maxlen,
						  weights=[embeds], mask_zero=False, name='embedding', trainable=True)(main_input_r)

		embed = Lambda(lambda x: K.reshape(x, shape=[-1, dataset.abs_len, dataset.maxlen, embeddings.embed_dim]))(embed)

		# embed = Dropout(0.5, name='embed_dropout')(embed)


		char_embed =  Embedding(input_dim=dataset.charsize, output_dim=embeddings.char_embed_dim, embeddings_initializer='lecun_uniform', 
						input_length=[dataset.maxlen, dataset.maxlen_word], mask_zero=False, name='char_embedding')(char_input_r)
	 
		char_embed_shape = char_embed.shape
		char_embed = Lambda(lambda x: K.reshape(x, shape=(-1, dataset.maxlen_word, embeddings.char_embed_dim)))(char_embed)
			
		# pdb.set_trace()


		biLSTM_char_embed = Bidirectional(CuDNNLSTM(embeddings.char_embed_dim, return_sequences=False))(char_embed)


		# fwd_state = GRU(150, return_state=True)(char_embed)[-2]
		# bwd_state = GRU(150, return_state=True, go_backwards=True)(char_embed)[-2]
		# biLSTM_char_embed = Concatenate(axis=-1)([fwd_state, bwd_state])
		char_embed = Lambda(lambda x: K.reshape(x, shape=[-1, dataset.abs_len, char_embed_shape[1], 2 * embeddings.char_embed_dim]))(biLSTM_char_embed)		

		# char_embed = Dropout(0.5, name='char_embed_dropout')(char_embed)
		# pdb.set_trace()

		combined_embed = Concatenate( name='Sum')([embed, char_embed])

		combined_embed = Lambda(lambda x: K.reshape(x, shape=(-1, dataset.maxlen, (2*embeddings.char_embed_dim+ embeddings.embed_dim) )))(combined_embed)
		
		
		# biLSTM_embed= Bidirectional(LSTM(64, return_sequences=True))(combined_embed)

		# with_atention = AttentionWithContext()(biLSTM_embed)

		# with_atention = Lambda(lambda x: K.reshape(x, shape=(-1, dataset.abs_len, 2*64 )))(with_atention)

		biLSTM = Bidirectional(CuDNNLSTM(64, return_sequences=False))(combined_embed)
		biLSTM = Dropout(0.5)(biLSTM)

		biLSTM_r = Lambda(lambda x: K.reshape(x, shape=(-1, dataset.abs_len, 2*64)))(biLSTM)


		norm = BatchNormalization()(biLSTM_r)
		feedforward = Dense(dataset.nclasses, name='feed_forword')(norm)

		final_output = CRF(dataset.nclasses, learn_mode='marginal', sparse_target=True)(feedforward)
		# final_output = Activation('softmax')(feedforward) # (None, 36, 5) # (None, 36, 5)

		
		# print 'passed checkpoint 7: Final_classifier\n\n'

		model = Model(inputs=[main_input, char_input], outputs=final_output, name='output')
		model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

		# model.summary()
		# plot_model(model, to_file='model.png', show_shapes=True)

		
		# except Exception as e:
		# 	# print 'passed checkpoint E1\n\n'
		# 	# model.summary()
		# 	# plot_model(model, to_file='model.png', show_shapes=True)

		# 	traceback.print_exc()
		# 	pdb.set_trace() 

	
		return model


## Jin et al.
	
def dot_product(x, kernel):
	"""
	Wrapper for dot product operation, in order to be compatible with both
	Theano and Tensorflow
	Args:
		x (): input
		kernel (): weights
	Returns:
	"""
	if K.backend() == 'tensorflow':
		# todo: check that this is correct
		return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)
	else:
		return K.dot(x, kernel)

class Attention(Layer):
	def __init__(self,
				 W_regularizer=None, b_regularizer=None,
				 W_constraint=None, b_constraint=None,
				 bias=True,
				 return_attention=False,
				 **kwargs):
		"""
		Keras Layer that implements an Attention mechanism for temporal data.
		Supports Masking.
		Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]
		# Input shape
			3D tensor with shape: `(samples, steps, features)`.
		# Output shape
			2D tensor with shape: `(samples, features)`.
		:param kwargs:
		Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.
		The dimensions are inferred based on the output shape of the RNN.
		Note: The layer has been tested with Keras 1.x
		Example:
		
			# 1
			model.add(LSTM(64, return_sequences=True))
			model.add(Attention())
			# next add a Dense layer (for classification/regression) or whatever...
			# 2 - Get the attention scores
			hidden = LSTM(64, return_sequences=True)(words)
			sentence, word_scores = Attention(return_attention=True)(hidden)
		"""
		self.supports_masking = True
		self.return_attention = return_attention
		self.init = initializers.get('glorot_uniform')
		self.W_regularizer = regularizers.get(W_regularizer)
		self.b_regularizer = regularizers.get(b_regularizer)
		self.W_constraint = constraints.get(W_constraint)
		self.b_constraint = constraints.get(b_constraint)
		self.bias = bias
		super(Attention, self).__init__(**kwargs)
	
	def build(self, input_shape):
		assert len(input_shape) == 3
		self.W = self.add_weight((input_shape[-1],),
								 initializer=self.init,
								 name='{}_W'.format(self.name),
								 regularizer=self.W_regularizer,
								 constraint=self.W_constraint)
		if self.bias:
			self.b = self.add_weight((input_shape[1],),
									 initializer='zero',
									 name='{}_b'.format(self.name),
									 regularizer=self.b_regularizer,
									 constraint=self.b_constraint)
		else:
			self.b = None
		
		self.built = True
	
	def compute_mask(self, input, input_mask=None):
		# do not pass the mask to the next layers
		return None
	
	def call(self, x, mask=None):
		eij = dot_product(x, self.W)
		if self.bias:
			eij += self.b
		
		eij = K.tanh(eij)
		a = K.exp(eij)
		# apply mask after the exp. will be re-normalized next
		if mask is not None:
			# Cast the mask to floatX to avoid float64 upcasting in theano
			a *= K.cast(mask, K.floatx())
		# in some cases especially in the early stages of training the sum may be almost zero
		# and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.
		# a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())
		a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())
		weighted_input = x * K.expand_dims(a)
		result = K.sum(weighted_input, axis=1)
		if self.return_attention:
			return [result, a]
		return result
	
	def compute_output_shape(self, input_shape):
		if self.return_attention:
			return [(input_shape[0], input_shape[-1]),
					(input_shape[0], input_shape[1])]
		else:
			return input_shape[0], input_shape[-1]






class model_Jin():

	def get_model_defination(self, dataset, embeddings):
		try:
						
			# Build the model
			print('Building the model...')
			lstm_dim = 64

		### token embedding layer
			
			main_input = Input(shape=[dataset.abs_len, dataset.maxlen], dtype='int32', name='input') # (None, 36)

			# unused
			char_input = Input(shape=[dataset.abs_len, dataset.maxlen,  dataset.maxlen_word], dtype='int32', name='char_input') # (None, 36, 25)
			
			main_input_r = Lambda(lambda x: K.reshape(x, shape=(-1, dataset.maxlen)))(main_input)	

			embeds, _, _  = embeddings.init_weights(dataset.idx2word)		
			embed = Embedding(input_dim= dataset.vocsize, output_dim= embeddings.embed_dim, input_length= dataset.maxlen, weights=[embeds], mask_zero=False, name='embedding', trainable=True)(main_input_r)


			# embed = Lambda(lambda x: K.reshape(x, shape=[-1, dataset.abs_len, dataset.maxlen,  embeddings.embed_dim]))(embed)

			# embed = Lambda(lambda x: K.reshape(x, shape=[-1,  dataset.maxlen,  embeddings.embed_dim]))(embed)

		### sentence encoding layer		

			# pdb.set_trace();
			blstm_layer = Bidirectional(CuDNNLSTM(lstm_dim, return_sequences=True))(embed)
			
			# attention_layer = Attention()(blstm_layer)
			attention_layer  = blstm_layer
			
			sentence_encoding_layer = attention_layer# Lambda(lambda x: K.reshape(x, shape=(-1, dataset.abs_len, 2*lstm_dim )))(attention_layer)


			# biLSTM = Dropout(0.5)(blstm_layer)

			# sentence_encoding_layer = Lambda(lambda x: K.reshape(x, shape=(-1, dataset.abs_len, 2*lstm_dim)))(biLSTM)



			# biLSTM_embed= Bidirectional(LSTM(64, return_sequences=True))(combined_embed)

			# pdb.set_trace();

		### context enriching layer

			biLSTM = Bidirectional(CuDNNLSTM(lstm_dim, return_sequences=False))(sentence_encoding_layer)
			biLSTM = Dropout(0.5)(biLSTM)
			
			biLSTM_r = Lambda(lambda x: K.reshape(x, shape=(-1, dataset.abs_len, 2*lstm_dim)))(biLSTM)


			norm = BatchNormalization()(biLSTM_r)
			feedforward = Dense(dataset.nclasses, name='feed_forword')(norm)


		### label sequence optimazion layer

			final_output = CRF(dataset.nclasses, learn_mode='marginal', sparse_target=True)(feedforward)
			# final_output = Activation('softmax')(feedforward) # (None, 36, 5) # (None, 36, 5)

			model = Model(inputs=[main_input, char_input], outputs=final_output, name='output')
			model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

			# model.summary()
			# plot_model(model, to_file='model.png', show_shapes=True)

			
		except Exception as e:
			# print 'passed checkpoint E1\n\n'
			# model.summary()
			# plot_model(model, to_file='model.png', show_shapes=True)

			traceback.print_exc()
			pdb.set_trace() 

	
		return model