### 1
##### 10.1109/TPAMI.2019.2952095
#### Learning to Localize Sound Sources in Visual Scenes: Analysis and Applications


0	  Visual events are usually accompanied by sounds in our daily lives.
0	  However, can the machines learn to correlate the visual scene and sound, as well as localize the sound source only by observing them like humans?
1	  To investigate its empirical learnability, in this work we first present a novel unsupervised algorithm to address the problem of localizing sound sources in visual scenes.
1	  In order to achieve this goal, a two-stream network structure which handles each modality, with attention mechanism is developed for sound source localization.
1	  The network naturally reveals the localized response in the scene without human annotation.
1	  In addition, a new sound source dataset is developed for performance evaluation.
2	  Nevertheless, our empirical evaluation shows that the unsupervised method generates false conclusions in some cases.
2	  Thereby, we show that this false conclusion cannot be fixed without human prior knowledge due to the well-known correlation and causality mismatch misconception.
2	  We show that the false conclusion can be effectively corrected even with a small amount of supervision, i.e., semi-supervised setup.
2	  We present the versatility of the learned audio and visual embeddings on the cross-modal content alignment and we incorporate this proposed algorithm into sound saliency based automatic camera view panning in 360 degree videos.


### 2
##### 10.1109/TPAMI.2019.2952114
#### InLoc: Indoor Visual Localization with Dense Matching and View Synthesis


0	  We seek to predict the 6 degree-of-freedom (6DoF) pose of a query photograph with respect to a large indoor 3D map.
0	  The contributions of this work are three-fold.
1	  First, we develop a new large-scale visual localization method targeted for indoor spaces.
1	  The method proceeds along three steps: (i) efficient retrieval of candidate poses that scales to large-scale environments, (ii) pose estimation using dense matching rather than sparse local features to deal with weakly textured indoor scenes, and (iii) pose verification by virtual view synthesis that is robust to significant changes in viewpoint, scene layout, and occlusion.
1	  Second, we release a new dataset with reference 6DoF poses for large-scale indoor localization.
1	  Query photographs are captured by mobile phones at a different time than the reference 3D map, thus presenting a realistic indoor localization scenario.
2	  Third, we demonstrate that our method significantly outperforms current state-of-the-art indoor localization approaches on this new challenging data.
#	  Code and data are publicly available.


### 3
##### 10.1109/TPAMI.2019.2952353
#### Assessing Transferability from Simulation to Reality for Reinforcement Learning


0	  Learning robot control policies from physics simulations is of great interest to the robotics community as it may render the learning process faster, cheaper, and safer by alleviating the need for expensive real-world experiments.
0	  However, the direct transfer of learned behavior from simulation to reality is a major challenge.
0	  Optimizing a policy on a slightly faulty simulator can easily lead to the maximization of the ‘Simulation Optimization Bias’ (SOB).
0	  In this case, the optimizer exploits modeling errors of the simulator such that the resulting behavior can potentially damage the robot.
1	  We tackle this challenge by applying domain randomization, i.e., randomizing the parameters of the physics simulations during learning.
1	  We propose an algorithm called Simulation-based Policy Optimization with Transferability Assessment (SPOTA) which uses an estimator of the SOB to formulate a stopping criterion for training.
1	  The introduced estimator quantifies the over-fitting to the set of domains experienced while training.
2	  Our experimental results on two different second order nonlinear systems show that the new simulation-based policy search algorithm is able to learn a control policy exclusively from a randomized simulator, which can be applied directly to real systems without any additional training.


### 4
##### 10.1109/TPAMI.2019.2952096
#### Scalar Quantization as Sparse Least Square Optimization


0	  Quantization aims to form new vectors or matrices with shared values close to the original.
0	  In recent years, the popularity of scalar quantization has been soaring as it has been found huge utilities in reducing the resource cost of neural networks.
0	  Popular clustering-based techniques suffers substantially from the problems of dependency on the seed, empty or out-of-the-range clusters, and high time complexity.
1	  To overcome the problems, scalar quantization is examined from a new perspective, namely sparse least square optimization.
1	  Specifically, several quantization algorithms based on l1 least square are proposed and implemented.
1	  In addition, similar schemes with l1+ l2 and l0 regularization are proposed.
1	  Furthermore, to compute quantization results with given amount of values/clusters, this paper proposes an iterative method and a clustering-based method, and both of them are built on sparse least square.
1	  The algorithms proposed are tested under three data scenarios and their computational performance, including information loss, time consumption and distribution of values of sparse vectors are compared.
2	  The paper offers a new perspective to probe the area of quantization, and the algorithms proposed are superior especially under bit-width reducing scenarios, when the required post-quantization resolution is not significantly lower than the original scalar.


### 5
##### 10.1109/TPAMI.2019.2951664
#### Affine Invariants of Vector Fields


0	  Vector fields are a special kind of multidimensional data, which are in a certain sense similar to digital color images, but are distinct from them in several aspects.
0	  In each pixel, the field is assigned to a vector that shows the direction and the magnitude of the quantity, which has been measured.
0	  To detect the patterns of interest in the field, special matching methods must be developed.
1	  In this paper, we propose a method for the description and matching of vector field patterns under an unknown affine transformation of the field.
1	  Unlike digital images, transformations of vector fields act not only on the spatial coordinates but also on the field values, which makes the detection different from the image case.
1	  To measure the similarity between the template and the field patch, we propose original invariants with respect to total affine transformation.
1	  They are designed from the vector field moments.
2	  It is demonstrated by experiments on real data from fluid mechanics that they perform significantly better than potential competitors.


### 6
##### 10.1109/TPAMI.2019.2951667
#### A Temporally-Aware Interpolation Network for Video Frame Inpainting


0	  In this work, we explore video frame inpainting, a task that lies at the intersection of general video inpainting, frame interpolation, and video prediction.
0	  Although our problem can be addressed by applying methods from other video interpolation or extrapolation tasks, doing so fails to leverage the additional context information that our problem provides.
1	  To this end, we devise a method specifically designed for video frame inpainting that is composed of two modules: a bidirectional video prediction module and a temporally-aware frame interpolation module.
1	  The prediction module makes two intermediate predictions of the missing frames, each conditioned on the preceding and following frames respectively, using a shared convolutional LSTM-based encoder-decoder.
1	  The interpolation module blends the intermediate predictions by using time information and hidden activations from the video prediction module to resolve disagreements between the predictions.
2	  Our experiments demonstrate that our approach produces smoother and more accurate results than state-of-the-art methods for general video inpainting, frame interpolation, and video prediction.


### 7
##### 10.1109/TPAMI.2019.2946796
#### SpaRTA Tracking across occlusions via partitioning of 3D clouds of points


0	  Any 3D tracking algorithm has to deal with occlusions: multiple targets get so close to each other that the loss of their identities becomes likely, hence potentially affecting the very quality of the data with interrupted trajectories and identity switches.
1	  Here, we present a novel tracking method that addresses the problem of occlusions within large groups of featureless objects by means of three steps: i) it represents each target as a cloud of points in 3D; ii) once a 3D cluster corresponding to an occlusion occurs, it defines a partitioning problem by introducing a cost function that uses both attractive and repulsive spatio-temporal proximity links; iii) it minimizes the cost function through a semi-definite optimization technique specifically designed to cope with the presence of multi-minima landscapes.
1	  The algorithm is designed to work on 3D data regardless of the experimental method used: multi--camera systems, lidars, radars and RGB-D systems.
2	  By performing tests on public data-sets, we show that the new algorithm produces a significant improvement over the state-of-the-art tracking methods, both by reducing the number of identity switches and by increasing the accuracy of the estimated positions of the targets in real space.


### 8
##### 10.1109/TPAMI.2019.2950923
#### Effects of Image Degradation and Degradation Removal to CNN-based Image Classification


0	  Just like many other topics in computer vision, image classification has achieved significant progress recently by using deep learning neural networks, especially the Convolutional Neural Networks (CNNs).
0	  Most of the existing works focused on classifying very clear natural images, evidenced by the widely used image databases such as Caltech-256, PASCAL VOCs and ImageNet.
0	  However, in many real applications, the acquired images may contain certain degradations that lead to various kinds of blurring, noise, and distortions.
1	  One important and interesting problem is the effect of such degradations to the performance of CNN-based image classification and whether degradation removal helps CNN-based image classification.
1	  More specifically, we wonder whether image classification performance drops with each kind of degradation, whether this drop can be avoided by including degraded images into training, and whether existing computer vision algorithms that attempt to remove such degradations can help improve the image classification performance.
1	  In this paper, we empirically study those problems for nine kinds of degraded images - hazy images, motion-blurred images, fish-eye images, underwater images, low resolution images, salt-and-peppered images, images with white Gaussian noise, Gaussian-blurred images and out-of-focus images.
1	  We expect this work can draw more interests from the community to study the classification of degraded images.


### 9
##### 10.1109/TPAMI.2019.2950631
#### Ideals of the Multiview Variety


0	  The multiview variety of an arrangement of cameras is the Zariski closure of the images of world points in the cameras.
0	  The prime vanishing ideal of this complex projective variety is called the multiview ideal.
2	  We show that the bifocal and trifocal polynomials from the cameras generate the multiview ideal when the foci are distinct.
0	  In the computer vision literature, many sets of (determinantal) polynomials have been proposed to describe the multiview variety.
1	  We establish precise algebraic relationships between the multiview ideal and these various ideals.
2	  When the camera foci are noncoplanar, we prove that the ideal of bifocal polynomials saturate to give the multiview ideal.
2	  Finally, we prove that all the ideals we consider coincide when dehomogenized, to cut out the space of finite images.


### 10
##### 10.1109/TPAMI.2019.2948619
#### Joint Embedding of Graphs


0	  Feature extraction and dimension reduction for networks is critical in a wide variety of domains.
0	  Efficiently and accurately learning features for multiple graphs has important applications in statistical inference on graphs.
1	  We propose a method to jointly embed multiple undirected graphs.
1	  Given a set of graphs, the joint embedding method identifies a linear subspace spanned by rank one symmetric matrices and projects adjacency matrices of graphs into this subspace.
1	  The projection coefficients can be treated as features of the graphs, while the embedding components can represent vertex features.
1	  We also propose a random graph model for multiple graphs that generalizes other classical models for graphs.
2	  We show through theory and numerical experiments that under the model, the joint embedding method produces estimates of parameters with small errors.
2	  Via simulation experiments, we demonstrate that the joint embedding method produces features which lead to state of the art performance in classifying graphs.
2	  Applying the joint embedding method to human brain graphs, we find it extracts interpretable features with good prediction accuracy in different tasks.


### 11
##### 10.1109/TPAMI.2019.2950025
#### Visual Semantic Information Pursuit: A Survey


0	  Visual semantic information comprises two important parts: the meaning of each visual semantic unit and the coherent visual semantic relation conveyed by these visual semantic units.
0	  Essentially, the former one is a visual perception task while the latter one corresponds to visual context reasoning.
0	  Remarkable advances in visual perception have been achieved due to the success of deep learning.
0	  In contrast, visual semantic information pursuit, a visual scene semantic interpretation task combining visual perception and visual context reasoning, is still in its early stage.
0	  It is the core task of many different computer vision applications, such as object detection, visual semantic segmentation, visual relationship detection or scene graph generation.
0	  Since it helps to enhance the accuracy and the consistency of the resulting interpretation, visual context reasoning is often incorporated with visual perception in current deep end-to-end visual semantic information pursuit methods.
0	  Surprisingly, a comprehensive review for this exciting area is still lacking.
1	  In this survey, we present a unified theoretical paradigm for all these methods, followed by an overview of the major developments and the future trends in each potential direction.
1	  The common benchmark datasets, the evaluation metrics and the comparisons of the corresponding methods are also introduced.


### 12
##### 10.1109/TPAMI.2019.2949299
#### 3D Fingerprint Recognition based on Ridge-valley-guided 3D Reconstruction and 3D Topology Polymer Feature Extraction


0	  Automated fingerprint recognition system (AFRS) for 3D fingerprints is essential and highly promising in biometric security.
0	  Despite the progress in 3D AFRSs, high-quality real-time 3D fingerprint reconstruction and high-accuracy 3D fingerprint recognition remain two challenging issues.
1	  To address these issues, we propose a robust 3D AFRS based on ridge-valley-guided 3D fingerprint reconstruction and 3D topology feature extraction.
1	  The proposed 3D fingerprint reconstruction considers the unique fingerprint characteristic of ridge-valley (RV) and achieves real-time reconstruction.
1	  Different from traditional triangulation-based methods that establish correspondence between points by cross-correlation-based searching, we propose to establish RV correspondence (RVC) between ridges/valleys by defining and calculating a RVC matrix based on the topology of RV curves.
1	  To enhance depth reconstruction, curve-based smoothing is proposed to refine our novel RV disparity map.
1	  The proposed 3D fingerprint recognition is based on three-dimensional topology polymer (TTP) feature extraction.
1	  The TTP codes 3D topology by projecting the 3D minutiae onto multiple planes and extracting their corresponding 2D topologies, which has proven to be effective and efficient.
2	  Comprehensive experimental results demonstrate that our method outperforms the state-of-the-art methods in terms of both reconstruction and recognition accuracy.
2	  Thanks to the significantly short running time, our method is applicable to practical applications.


### 13
##### 10.1109/TPAMI.2019.2950198
#### Exploring Explicit Domain Supervision for Latent Space Disentanglement in Unpaired Image-to-Image Translation


0	  Image-to-image translation tasks have been widely investigated with Generative Adversarial Networks (GANs).
0	  However, existing approaches are mostly designed in an unsupervised manner while little attention has been paid to domain information within unpaired data.
1	  In this paper, we treat domain information as explicit supervision and design an unpaired image-to-image translation framework, Domain-supervised GAN (DosGAN), which takes the first step towards the exploration of explicit domain supervision.
1	  In contrast to representing domain characteristics using different generators or domain codes, we pre-train a classification network to explicitly classify the domain of an image.
1	  After pre-training, this network is used to extract the domain-specific features of each image.
1	  Such features, together with the domain-independent features extracted by another encoder (shared across different domains), are used to generate image in target domain.
2	  Extensive experiments on multiple facial attribute translation, multiple identity translation, multiple season translation and conditional edges-to-shoes/handbags demonstrate the effectiveness of our method.
1	  In addition, we can transfer the domain-specific feature extractor obtained on the Facescrub dataset with domain supervision information to unseen domains, such as faces in the CelebA dataset.
2	  We also succeed in achieving conditional translation with any two images in CelebA, while previous models like StarGAN cannot handle this task.


### 14
##### 10.1109/TPAMI.2019.2950317
#### Bilinear image translation for temporal analysis of photo collections


1	  We propose an approach for analyzing unpaired visual data annotated with time stamps by generating how images would have looked like if they were from different times.
1	  To isolate and transfer time dependent appearance variations, we introduce a new trainable bilinear factor separation module.
1	  We analyze its relation to classical factored representations and concatenation-based auto-encoders.
1	  We demonstrate this new module has clear advantages compared to standard concatenation when used in a bottleneck encoder-decoder convolutional neural network architecture.
2	  We also show that it can be inserted in a recent adversarial image translation architecture, enabling transfer to multiple different target time periods using a single network.
1	  We apply our model to a challenging collection of more than 13,000 cars manufactured between 1920 and 2000 and a dataset of high school yearbook portraits from 1930 to 2009.
1	  This allows us, for a given new input image, to generate a "history-lapse video" revealing changes over time by simply varying the latent variable corresponding to time.
2	  We show that by analyzing the generated history-lapse videos we can identify object deformations across time, extracting interesting changes in visual style over decades.


### 15
##### 10.1109/TPAMI.2019.2949414
#### Forecasting People Trajectories and Head Poses by Jointly Reasoning on Tracklets and Vislets


0	  In this work, we explore the correlation between people trajectories and their head orientations.
1	  We argue that people trajectory and head pose forecasting can be modelled as a joint problem.
0	  Recent approaches on trajectory forecasting leverage short-term trajectories (aka tracklets) of pedestrians to predict their future paths.
0	  In addition, sociological cues, such as expected destination or pedestrian interaction, are often combined with tracklets.
1	  In this paper, we propose MiXing-LSTM (MX-LSTM) to capture the interplay between positions and head orientations (vislets) thanks to a joint unconstrained optimization of full covariance matrices during the LSTM backpropagation.
1	  We additionally exploit the head orientations as a proxy for the visual attention, when modeling social interactions.
2	  MX-LSTM predicts future pedestrians location and head pose, increasing the standard capabilities of the current approaches on long-term trajectory forecasting.
2	  Compared to the state-of-the-art, our approach shows better performances on an extensive set of public benchmarks.
2	  MX-LSTM is particularly effective when people move slowly, i.e. the most challenging scenario for all other models.
2	  The proposed approach also allows for accurate predictions on a longer time horizon.


### 16
##### 10.1109/TPAMI.2019.2949302
#### Corner detection using second-order generalized Gaussian directional derivative representations


0	  Corner detection is a critical component of many image analysis and image understanding tasks such as object recognition and image matching.
0	  Our research indicates that existing corner detection algorithms cannot properly depict the difference between edges and corners and this results in wrong corner detections.
1	  In this paper, the capability of second-order generalized (isotropic and anisotropic) Gaussian directional derivative filters to suppress Gaussian noise is evaluated.
1	  The second-order generalized Gaussian directional derivative representations of step edge, L-type corner, Y- or T-type corner, X-type corner, and star-type corner are investigated and obtained.
1	  A number of properties for edges and corners are discovered which enable us to propose a new image corner detection method.
1	  Finally, the criteria on detection accuracy and average repeatability under affine image transformation, JPEG compression, and noise degradation, and the criteria on region repeatability are used to evaluate the proposed detector against nine state-of-the-art methods.
2	  The experimental results show that our proposed detector outperforms all the other tested detectors.


### 17
##### 10.1109/TPAMI.2019.2949562
#### Weakly-Supervised Learning of Category-specific 3D Object Shapes


0	  Category-specific 3D object shape models have greatly boosted the recent advances in object detection, recognition and segmentation.
0	  However, even the most advanced approach for learning 3D object shapes still requires heavy manual annotations on large-scale 2D images.
0	  In particular, annotating figure-ground segmentation is unbearably labor-intensive and time-consuming.
1	  To alleviate the costs of such manual annotations, we make an effort to learn category-specific 3D shape models by using weakly-labeled 2D images, where only object categories and keypoints are annotated.
1	  By exploring the underlying relationship between two tasks: object segmentation and category-specific 3D shape reconstruction, we propose a novel weakly-supervised learning framework to jointly address these two tasks and collaborate them to boost the final performance of the learned 3D shape models.
2	  Moreover, learning without using figure-ground segmentation leads to ambiguous solutions.
1	  To this end, we develop the confidence weighting schemes in the viewpoint estimation and 3D shape learning procedure.
2	  These schemes effectively reduce the confusion caused by the noisy data and thus increase the chances for obtaining more reliable 3D object shapes.
2	  Comprehensive experiments on the challenging PASCAL VOC benchmark show that our framework achieves comparable performance of the state-of-the-art methods that use expensive manual segmentation annotations


### 18
##### 10.1109/TPAMI.2019.2948348
#### Adversarial Margin Maximization Networks


0	  The tremendous recent success of deep neural networks (DNNs) has sparked a surge of interest in understanding their predictive ability.
0	  Unlike the human visual system which is able to generalize robustly and learn with little supervision, DNNs normally require a massive amount of data to learn new concepts.
0	  In addition, research works also show that DNNs are vulnerable to adversarial examples---maliciously generated images which seem perceptually similar to the natural ones but are actually formed to fool learning models, which means the models have problem generalizing to unseen data with certain type of distortions.
1	  In this paper, we analyze the generalization ability of DNNs comprehensively and attempt to improve it from a geometric point of view.
1	  We propose adversarial margin maximization (AMM), a learning-based regularization which exploits an adversarial perturbation as a proxy.
2	  It encourages a large margin in the input space, just like the support vector machines.
1	  With a differentiable formulation of the perturbation, we train the regularized DNNs simply through back-propagation in an end-to-end manner.
2	  Experimental results on various datasets (including MNIST, CIFAR-10/100, SVHN and ImageNet) and different DNN architectures demonstrate the superiority of our method over previous state-of-the-arts.
#	  Code and models for reproducing our results will be made publicly available.


### 19
##### 10.1109/TPAMI.2019.2948352
#### Orthogonal Deep Neural Networks


0	  In this paper, we introduce the algorithms of Orthogonal Deep Neural Networks (OrthDNNs) to connect with recent interest of spectrally regularized deep learning methods.
0	  OrthDNNs are theoretically motivated by generalization analysis of modern DNNs, with the aim to find solution properties of network weights that guarantee better generalization.
1	  To this end, we first prove that DNNs are of local isometry on data distributions of practical interest; by using a new covering of the sample space and introducing the local isometry property of DNNs into generalization analysis, we establish a new generalization error bound that is characterized by singular value spectrum of each of networks' weight matrices.
2	  We prove that the optimal bound w.r.t. the degree of isometry is attained when each weight matrix has a spectrum of equal singular values, among which that with orthonormal rows/columns is the most straightforward choice, suggesting the algorithms of OrthDNNs.
1	  We present both algorithms of strict and approximate OrthDNNs, and for the later ones we propose Singular Value Bounding, which performs as well as strict OrthDNNs but at a much lower computational cost.
1	  We also propose algorithms to make compatible use of batch normalization with OrthDNNs.
2	  Extensive comparative studies show the efficacy of OrthDNNs.


### 20
##### 10.1109/TPAMI.2019.2948011
#### Large scale shadow annotation and detection using lazy annotation and stacked CNNs


0	  Recent shadow detection algorithms have shown initial success on small datasets of images from specific domains.
0	  However, shadow detection on broader image domains is still challenging due to the lack of representative annotated training data.
1	  In this paper we propose "lazy annotation", an efficient annotation method where an annotator only needs to mark the important shadow areas and some non-shadow areas.
1	  This yields data with noisy labels that are not yet useful for training a shadow detector.
1	  We address the problem of label noise by jointly learning a shadow region classifier and recovering the labels in the training set.
2	  Experimental results show that a classifier trained with recovered labels achieves comparable performance to a classifier trained on the properly annotated data.
1	  These results motivated us to collect a new dataset that is 20 times larger than existing datasets and contains a large variety of scenes and image types.
1	  In addition, we propose a stacked Convolutional Neural Network architecture that efficiently trains on patch level shadow examples while incorporating image level semantic information.
2	  Our proposed pipeline, trained on recovered labels, performs at state-of-the art level.


### 21
##### 10.1109/TPAMI.2019.2947440
#### Interpreting the Rhetoric of Visual Advertisements


0	  Visual media have important persuasive power, but prior computer vision approaches have predominantly ignored the persuasive aspects of images.
1	  In this work, we propose a suite of data and techniques that enable progress on understanding the messages that visual advertisements convey.
1	  We make available a dataset of 64,832 image ads and 3,477 video ads, annotated with ten types of information: the topic and sentiment of the ad; whether it is funny, exciting, or effective; what action it prompts the viewer to do, and what arguments it provides for why this action should be taken; symbolic associations that the ad relies on; the metaphorical object transformations on which especially creative ads rely; and the climax in video ads.
1	  We develop methods that use multimodal cues, i.e. both visuals and slogans, for both the image and video domains.
1	  Our methods rely on finding poignant content spatially and temporally.
1	  We also examine the creative story construction in ads: for videos, we learn to predict when the climax occurs (if any), and how effective the story is; for images, we analyze how object transformations in ads metaphorically depict product properties.


### 22
##### 10.1109/TPAMI.2019.2947374
#### Learning Depth with Convolutional Spatial Propagation Network


0	  Depth prediction is one of the fundamental problems in computer vision.
1	  In this paper, we propose a simple yet effective convolutional spatial propagation network (CSPN) to learn the affinity matrix for various depth estimation tasks.
2	  We can append this module to any output from a state-of-the-art (SOTA) network to improve their performances.
1	  In practice, we further extend CSPN in two aspects: 1) take a sparse depth map as additional input, which is useful for the task of sparse to dense (a.k.a depth completion); 2) we propose 3D CSPN to handle features with one additional dimension, which is effective in the task of stereo matching using 3D cost volume.
2	  For the tasks of depth completion, we experimented the proposed CPSN conjunct algorithms over NYU v2 and KITTI datasets, where we show that our proposed algorithms not only produce high quality (e.g., 30% more reduction in depth error), but also run faster (e.g., 2 to 5 × faster) than previous SOTA spatial propagation network.
2	  We also evaluated our stereo matching algorithm on the Scene Flow and KITTI Stereo datasets, and rank 1st on both the KITTI Stereo 2012 and 2015 benchmarks, which demonstrates the effectiveness of the proposed module again.


### 23
##### 10.1109/TPAMI.2019.2947427
#### Appearance and Pose-Conditioned Human Image Generation using Deformable GANs


0	  In this paper, we address the problem of generating person images conditioned on both pose and appearance information.
1	  Specifically, given an image xa of a person and a target pose P(xb), extracted from a different image xb, we synthesize a new image of that person in pose P(xb), while preserving the visual details in xa.
1	  In order to deal with pixel-to-pixel misalignments caused by the pose differences between P(xa) and P(xb), we introduce deformable skip connections in the generator of our Generative Adversarial Network.
1	  Moreover, a nearest-neighbour loss is proposed instead of the common L1 and L2 losses in order to match the details of the generated image with the target image.
2	  Quantitative and qualitative results, using common datasets and protocols recently proposed for this task, show that our approach is competitive with respect to the state of the art.
1	  Moreover, we conduct an extensive evaluation using off-the-shell person re-identification (Re-ID) systems trained with person-generation based augmented data, which is one of the main important applications for this task.
2	  Our experiments show that our Deformable GANs can significantly boost the Re-ID accuracy and are even better than data-augmentation methods specifically trained using Re-ID losses.


### 24
##### 10.1109/TPAMI.2019.2947048
#### SurfelMeshing: Online Surfel-Based Mesh Reconstruction


1	  We address the problem of mesh reconstruction from live RGB-D video, assuming a calibrated camera and poses provided externally (e.g., by a SLAM system).
1	  In contrast to most existing approaches, we do not fuse depth measurements in a volume but in a dense surfel cloud.
1	  We asynchronously (re)triangulate the smoothed surfels to reconstruct a surface mesh.
1	  This novel approach enables to maintain a dense surface representation of the scene during SLAM which can quickly adapt to loop closures.
2	  This is possible by deforming the surfel cloud and asynchronously remeshing the surface where necessary.
2	  The surfel-based representation also naturally supports strongly varying scan resolution.
2	  In particular, it reconstructs colors at the input camera's resolution.
2	  Moreover, in contrast to many volumetric approaches, ours can reconstruct thin objects since objects do not need to enclose a volume.
2	  We demonstrate our approach in a number of experiments, showing that it produces reconstructions that are competitive with the state-of-the-art, and we discuss its advantages and limitations.
#	  The algorithm (excluding loop closure functionality) is available as open source at https://github.com/puzzlepaint/surfelmeshing.


### 25
##### 10.1109/TPAMI.2019.2946806
#### Deep Depth from Uncalibrated Small Motion Clip


0	  We propose a novel approach to infer a high-quality depth map from a set of images with small viewpoint variations.
0	  In general, techniques for depth estimation from small motion consist of camera pose estimation and dense reconstruction.
1	  In contrast to prior approaches that recover scene geometry and camera motions using pre-calibrated cameras, we introduce a self-calibrating bundle adjustment method tailored for small motion which enables computation of camera poses without the need for camera calibration.
1	  For dense depth reconstruction, we present a convolutional neural network called DPSNet (Deep Plane Sweep Network) whose design is inspired by best practices of traditional geometry-based approaches.
1	  Rather than directly estimating depth or optical flow correspondence from image pairs as done in many previous deep learning methods, DPSNet takes a plane sweep approach that involves building a cost volume from deep features using the plane sweep algorithm, regularizing the cost volume, and regressing the depth map from the cost volume.
1	  The cost volume is constructed using a differentiable warping process that allows for end-to-end training of the network.
2	  Through the effective incorporation of conventional multiview stereo concepts within a deep learning framework, the proposed method achieves state-of-the-art results on a variety of challenging datasets.


### 26
##### 10.1109/TPAMI.2019.2946823
#### Fine-grained Video Captioning via Graph-based Multi-granularity Interaction Learning


0	  Team sports auto-narrative requires simultaneous modeling of fine-grained individual actions and uncovering of spatio-temporal dependency structures of frequent group interactions, and then accurate mapping of these complex interaction details into long and detailed commentary.
1	  We propose a novel framework - Graph-based Learning for Multi-Granularity Interaction Representation (GLMGIR) for fine-grained team sports auto-narrative task.
1	  A multi-granular interaction module is proposed to extract among-subjects' interactive actions in a progressive way for encoding both intra- and inter-team interactions.
1	  Based on the above multi-granular representations, a multi-granular attention module is developed to consider action/event descriptions of multiple spatio-temporal resolutions.
1	  Both modules are integrated seamlessly and work in a collaborative way to generate the final narrative.
1	  In the meantime, we collect a new video dataset called Sports Video Narrative dataset (SVN).
1	  It is a novel direction as it contains 6K team sports videos with 10K ground-truth narratives.
1	  Furthermore, as previous metrics, DO NOT cope with fine-grained sports narrative task well, we hence develop a novel evaluation metric named Fine-grained Captioning Evaluation (FCE), which measures how accurate the generated linguistic description reflects fine-grained action details as well as the overall spatio-temporal interactional structure.
2	  Extensive experiments on our SVN dataset have demonstrated the effectiveness of the proposed framework for fine-grained team sports video auto-narrative.


### 27
##### 10.1109/TPAMI.2019.2946370
#### Bayesian Joint Matrix Decomposition for Data Integration with Heterogeneous Noise


0	  Matrix decomposition is a popular and fundamental approach in machine learning and data mining.
0	  It has been successfully applied into various fields.
0	  Most matrix decomposition methods focus on decomposing a data matrix from one single source.
0	  However, it is common that data are from different sources with heterogeneous noise.
0	  A few of matrix decomposition methods have been extended for such multi-view data integration and pattern discovery.
0	  While only few methods were designed to consider the heterogeneity of noise in such multi-view data for data integration explicitly.
1	  To this end, we propose a joint matrix decomposition framework (BJMD), which models the heterogeneity of noise by Gaussian distribution in a Bayesian framework.
1	  We develop two algorithms to solve this model: one is a variational Bayesian inference algorithm, which makes full use of the posterior distribution; and another is a maximum a posterior algorithm, which is more scalable and can be easily paralleled.
2	  Extensive experiments on synthetic and real-world datasets demonstrate that BJMD is superior or competitive to the state-of-the-art methods.


### 28
##### 10.1109/TPAMI.2019.2946567
#### Sinusoidal Sampling Enhanced Compressive Camera for High Speed Imaging


0	  Compressive sensing technique allows capturing fast phenomena at much higher frame rate than the camera sensor, by recovering a frame sequence from their encoded combination.
0	  However, most conventional compressive video sensing methods limit the achieved frame rate improvement to tenfold and only support low resolution recovery.
1	  Making use of camera's redundant spatial resolution for further frame rate improve, here we report a novel compressive video acquisition technique termed Sinusoidal Sampling Enhanced Compressive Camera (S2EC2) to encode denser frames within a snapshot.
1	  Specifically, we decompose the dense frames into groups and apply combinational coding: random codes within each group for compressive acquisition; group specific sinusoidal codes to multiplex different groups onto the high resolution sensor.
1	  The sinusoidal codes designed for these groups would shift their frequency components by different offsets in the Fourier domain and staggered the dominant frequencies of the coded measurements of these groups.
1	  Correspondingly, the reconstruction successfully separate coded measurements of different groups and recovers frames within each group.
1	  Besides, we also solve the implementation problem of insufficient gray scale spatial light modulation speed, and build a prototype achieving 2000 fps reconstruction with a 15.6 fps camera (the actual compression ratio is 0.009).
2	  The extensive experiments validate the proposed approach.


### 29
##### 10.1109/TPAMI.2019.2946159
#### Complex-Valued Disparity: Unified Depth Model of Depth from Stereo, Depth from Focus, and Depth from Defocus Based on the Light Field Gradient


1	  This paper proposes a unified depth model based on the light field gradient, in which estimated disparity is represented by the complex number.
1	  The complex-valued disparity by the proposed depth model can be represented in both the Cartesian and polar coordinates.
1	  In the Cartesian representation, the proposed depth model is represented by real and imaginary parts of the disparity.
1	  The real part can be used for disparity estimation with respect to the in-focus plane, whereas the imaginary part represents the non-Lambertian-ness.
1	  In the polar representation, the proposed depth model is expressed by the disparity magnitude and disparity angle.
2	  The disparity magnitude shows the relationship among depth from stereo, depth from focus, and depth from defocus, whereas the disparity angle shows whether or not the bundles of rays are flipped with respect to the in-focus plane.
2	  For disparity analysis, we present the real response, imaginary response, magnitude response, and angle response, which are represented by the three-dimensional volume.
2	  Experimental results on synthetic and real light field images show that the real and magnitude responses of the proposed depth model are valid for local disparity estimation.


### 30
##### 10.1109/TPAMI.2019.2945942
#### A review of domain adaptation without target labels


0	  Domain adaptation has become a prominent problem setting in machine learning and related fields.
0	  This review asks the question: how can a classifier learn from a source domain and generalize to a target domain?
1	  We present a categorization of approaches, divided into, what we refer to as, sample-based, feature-based and inference-based methods.
1	  Sample-based methods focus on weighting individual observations during training based on their importance to the target domain.
1	  Feature-based methods revolve around on mapping, projecting and representing features such that a source classifier performs well on the target domain and inference-based methods incorporate adaptation into the parameter estimation procedure, for instance through constraints on the optimization procedure.
1	  Additionally, we review a number of conditions that allow for formulating bounds on the cross-domain generalization error.
2	  Our categorization highlights recurring ideas and raises questions important to further research.


### 31
##### 10.1109/TPAMI.2019.2945574
#### Approximate Graph Laplacians for Multimodal Data Clustering


0	  One of the important approaches of handling data heterogeneity in multimodal data clustering is modeling each modality using a separate similarity graph.
0	  Information from the multiple graphs is integrated by combining them into a unified graph.
0	  A major challenge here is how to preserve cluster information while removing noise from individual graphs.
1	  In this regard, a novel algorithm, termed as CoALa, is proposed that integrates noise-free approximations of multiple similarity graphs.
1	  The proposed method first approximates a graph using the most informative eigenpairs of its Laplacian which contain cluster information.
1	  The approximate Laplacians are then integrated for the construction of a low-rank subspace that best preserves overall cluster information of multiple graphs.
2	  However, this approximate subspace differs from the full-rank subspace which integrates information from all the eigenpairs of each Laplacian.
1	  Matrix perturbation theory is used to theoretically evaluate how far approximate subspace deviates from the full-rank one for a given value of approximation rank.
1	  Finally, spectral clustering is performed on the approximate subspace to identify the clusters.
2	  Experimental results on several real-life cancer and benchmark data sets demonstrate that the proposed algorithm significantly and consistently outperforms state-of-the-art integrative clustering approaches.


### 32
##### 10.1109/TPAMI.2019.2944806
#### MFQE 2.0: A New Approach for Multi-frame Quality Enhancement on Compressed Video


0	  The past few years have witnessed great success in applying deep learning to enhance the quality of compressed image/video.
0	  The existing approaches mainly focus on enhancing the quality of a single frame, not considering the similarity between consecutive frames.
0	  Since heavy fluctuation exists across compressed video frames as investigated in this paper, frame similarity can be utilized for quality enhancement of low-quality frames given their neighbouring high-quality frames.
0	  This task is Multi-Frame Quality Enhancement (MFQE).
1	  Accordingly, this paper proposes an MFQE approach for compressed video, as the first attempt in this direction.
1	  In our approach, we firstly develop a Bidirectional Long Short-Term Memory (BiLSTM) based detector to locate Peak Quality Frames (PQFs) in compressed video.
1	  Then, a novel Multi-Frame Convolutional Neural Network (MF-CNN) is designed to enhance the quality of compressed video, in which the non-PQF and its nearest two PQFs are the input.
1	  In MF-CNN, motion between the non-PQF and PQFs is compensated by a motion compensation subnet.
1	  Subsequently, a quality enhancement subnet fuses the non-PQF and compensated PQFs, and then reduces the compression artifacts of the non-PQF.
1	  Also, PQF quality is enhanced in the same way.
2	  Finally, experiments validate the effectiveness and generalization ability of our MFQE approach in advancing the state-of-the-art quality enhancement of compressed video.


### 33
##### 10.1109/TPAMI.2019.2944808
#### SEWA DB: A Rich Database for Audio-Visual Emotion and Sentiment Research in the Wild


0	  Natural human-computer interaction and audio-visual human behaviour sensing systems, which would achieve robust performance in-the-wild are more needed than ever as digital devices are becoming indispensable part of our life more and more.
0	  Accurately annotated real-world data are the crux in devising such systems.
0	  However, existing databases usually consider controlled settings, low demographic variability, and a single task.
1	  In this paper, we introduce the SEWA database of more than 2000 minutes of audio-visual data of 398 people coming from six cultures, 50% female, and uniformly spanning the age range of 18 to 65 years old.
1	  Subjects were recorded in two different contexts: while watching adverts and while discussing adverts in a video chat.
2	  The database includes rich annotations of the recordings in terms of facial landmarks, facial action units (FAU), various vocalisations, mirroring, and continuously valued valence, arousal, liking, agreement, and prototypic examples of (dis)liking.
2	  This database aims to be an extremely valuable resource for researchers in affective computing and automatic human sensing and is expected to push forward the research in human behaviour analysis, including cultural studies.
2	  Along with the database, we provide extensive baseline experiments for automatic FAU detection and automatic valence, arousal and (dis)liking intensity estimation.


### 34
##### 10.1109/TPAMI.2019.2945027
#### High-dimensional Dense Residual Convolutional Neural Network for Light Field Reconstruction


0	  We consider the problem of high-dimensional light field reconstruction and develop a learning-based framework for spatial and angular super-resolution.
0	  Many current approaches either require disparity clues or restore the spatial and angular details separately.
0	  Such methods have difficulties with non-Lambertian surfaces or occlusions.
1	  In contrast, we formulate light field super-resolution (LFSR) as tensor restoration and develop a learning framework based on a two-stage restoration with 4-dimensional (4D) convolution.
1	  This allows our model to learn the features capturing the geometry information encoded in multiple adjacent views.
2	  Such geometric features vary near the occlusion regions and indicate the foreground object border.
1	  To train a feasible network, we propose a novel normalization operation based on a group of views in the feature maps, design a stage-wise loss function, and develop the multi-range training strategy to further improve the performance.
1	  Evaluations are conducted on a number of light field datasets including real-world scenes, synthetic data, and microscope light fields.
2	  The proposed method achieves superior performance and less execution time comparing with other state-of-the-art schemes.


### 35
##### 10.1109/TPAMI.2019.2944597
#### A Graph-based Approach for Making Consensus-based Decisions in Image Search and Person Re-identification


0	  Image matching and retrieval is the underlying problem in various directions of computer vision research, such as image search, biometrics, and person re-identification.
1	  The problem involves searching for the closest match to a query image in a database of images.
1	  This work presents a method for generating a consensus amongst multiple algorithms for image matching and retrieval.
1	  The proposed algorithm, Shortest Hamiltonian Path Estimation (SHaPE), maps the process of ranking candidates based on a set of scores to a graph-theoretic problem.
1	  This mapping is extended to incorporate results from multiple sets of scores obtained from different matching algorithms.
1	  The problem of consensus-based decision-making is solved by searching for a suitable path in the graph under specified constraints using a two-step process.
1	  First, a greedy algorithm is employed to generate an approximate solution.
2	  In the second step, the graph is extended and the problem is solved by applying Ant Colony Optimization.
2	  Experiments are performed for image search and person re-identification to illustrate the efficiency of SHaPE in image matching and retrieval.
2	  Although SHaPE is presented in the context of image retrieval, it can be applied, in general, to any problem involving the ranking of candidates based on multiple sets of scores.


### 36
##### 10.1109/TPAMI.2019.2944377
#### Video Anomaly Detection With Sparse Coding Inspired Deep Neural Networks


1	  This paper presents an anomaly detection method that is based on a sparse coding inspired Deep Neural Networks (DNN).
1	  Specifically, we propose a Temporally-coherent Sparse Coding (TSC), where a temporally-coherent term is used to preserve the similarity between two neighboring frames.
1	  The optimization of sparse coefficients in TSC is equivalent to a special stacked Recurrent Neural Networks (sRNN) architecture.
1	  Further, to reduce the computational cost in alternatively updating the dictionary and sparse coefficients in TSC optimization and to alleviate hyperparameters selection in TSC, we stack one more layer on top of the TSC-inspired sRNN to reconstruct the inputs, and arrive at an sRNN-AE.
2	  We further improve sRNN-AE in the following aspects: i) we propose to learn a data-dependent similarity measurement between neighboring frames in sRNN-AE; ii) we reduce the depth of the sRNN in sRNN-AE; iii) we conduct temporal pooling over the appearance features of several consecutive frames for motion characterization.
1	  We also build a large-scale anomaly detection dataset for performance evaluation.
2	  Extensive experiments on both a toy dataset under controlled settings and real datasets demonstrate the effectiveness of our sRNN-AE method for anomaly detection.


### 37
##### 10.1109/TPAMI.2019.2943860
#### Nonlinear Regression via Deep Negative Correlation Learning


0	  Nonlinear regression has been extensively employed in many computer vision problems (e.g., crowd counting, age estimation, affective computing).
0	  Under the umbrella of deep learning, two common solutions exist i) transforming nonlinear regression to a robust loss function which is jointly optimizable with the deep convolutional network, and ii) utilizing ensemble of deep networks.
0	  Although some improved performance is achieved, the former may be lacking due to the intrinsic limitation of choosing a single hypothesis and the latter usually suffers from much larger computational complexity.
0	  To cope with those issues, we propose to regress via an efficient "divide and conquer" manner.
1	  The core of our approach is the generalization of negative correlation learning that has been shown, both theoretically and empirically, to work well for non-deep regression problems.
1	  Without extra parameters, the proposed method controls the bias-variance-covariance trade-off systematically and usually yields a deep regression ensemble where each base model is both "accurate" and "diversified."
2	  Moreover, we show that each sub-problem in the proposed method has less Rademacher Complexity and thus is easier to optimize.
2	  Extensive experiments on several diverse and challenging tasks including crowd counting, personality analysis, age estimation, and image super-resolution demonstrate the superiority over challenging baselines.


### 38
##### 10.1109/TPAMI.2019.2943456
#### Interpretable Visual Question Answering by Reasoning on Dependency Trees


0	  Collaborative reasoning for understanding each image-question pair is very critical but underexplored for an interpretable visual question answering system.
0	  Although very recent works also attempted to use explicit compositional processes to assemble multiple subtasks embedded in the questions, their models heavily rely on annotations or handcrafted rules to obtain valid reasoning processes, leading to either heavy workloads or poor performance on composition reasoning.
1	  In this paper, to better align image and language domains in diverse and unrestricted cases, we propose a novel neural network model that performs global reasoning on a dependency tree parsed from the question, and we thus phrase our model as parse-tree-guided reasoning network (PTGRN).
1	  This network consists of three collaborative modules: i) an attention module to exploit the local visual evidence for each word parsed from the question, ii) a gated residual composition module to compose the previously mined evidence, and iii) a parse-tree-guided propagation module to pass the mined evidence along the parse tree.
2	  Our PTGRN is thus capable of building an interpretable VQA system that gradually derives the image cues following a question-driven parse-tree reasoning route.
2	  Experiments on relational datasets demonstrate the superiority of our PTGRN over current state-of-the-art VQA methods.


### 39
##### 10.1109/TPAMI.2019.2942928
#### Progressive Fusion for Unsupervised Binocular Depth Estimation using Cycled Networks


0	  Recent deep monocular depth estimation approaches based on supervised regression have achieved remarkable performance.
0	  However, they require costly ground truth annotations during training.
0	  To cope with this issue, in this paper we present a novel unsupervised deep learning approach for predicting depth maps.
1	  We introduce a new network architecture, named Progressive Fusion Network (PFN), that is specifically designed for stereo depth estimation.
1	  This network is based on a multi-scale refinement strategy that combines the information provided by both images.
1	  In addition, we propose to stack twice this network in order to form a cycle.
1	  This cycle approach can be interpreted as a form of data-augmentation since, at training time, the network learns both from the training set images (in the forward half-cycle) but also from the synthesized images (in the backward half-cycle).
1	  The architecture is jointly trained with adversarial learning.
2	  Extensive experiments on the publicly available datasets KITTI, Cityscapes and ApolloScape demonstrate the effectiveness of the proposed model which outperforms previous unsupervised deep learning methods for depth prediction.


### 40
##### 10.1109/TPAMI.2019.2942592
#### Adversarial Distillation for Learning with Privileged Provisions


0	  Knowledge distillation aims to train a student (model) for accurate inference in a resource-constrained environment.
0	  Traditionally, the student is trained by a high-capacity teacher (model) whose training is resource-intensive.
0	  The student trained this way is suboptimal because it is difficult to learn the real data distribution from the teacher.
1	  To address this issue, we propose to train the student against a discriminator in a minimax game.
2	  Such a minimax game has an issue that it can take an excessively long time for the training to converge.
1	  To address this issue, we propose adversarial distillation consisting of a student, a teacher, and a discriminator.
2	  The discriminator is now a multi-class classifier that distinguishes among the real data, the student, and the teacher.
1	  The student and the teacher aim to fool the discriminator via adversarial losses, while they learn from each other via distillation losses.
2	  By optimizing the adversarial and the distillation losses simultaneously, the student and the teacher can learn the real data distribution.
1	  To accelerate the training, we propose to obtain low-variance gradient updates from the discriminator using a Gumbel-Softmax trick.
2	  We conduct extensive experiments to demonstrate the superiority of the proposed adversarial distillation under both accuracy and training speed.


### 41
##### 10.1109/TPAMI.2019.2941941
#### MEMC-Net: Motion Estimation and Motion Compensation Driven Neural Network for Video Interpolation and Enhancement


0	  Motion estimation (ME) and motion compensation (MC) have been widely used for classical video frame interpolation systems over the past decades.
0	  Recently, a number of data-driven frame interpolation methods based on convolutional neural networks have been proposed.
0	  However, existing learning based methods typically estimate either flow or compensation kernels, thereby limiting performance on both computational efficiency and interpolation accuracy.
1	  In this work, we propose a motion estimation and compensation driven neural network for video frame interpolation.
1	  A novel adaptive warping layer is developed to integrate both optical flow and interpolation kernels to synthesize target frame pixels.
1	  This layer is fully differentiable such that both the flow and kernel estimation networks can be optimized jointly.
1	  The proposed model benefits from the advantages of motion estimation and compensation methods without using hand-crafted features.
2	  Compared to existing methods, our approach is computationally efficient and able to generate more visually appealing results.
2	  Furthermore, the proposed MEMC-Net can be seamlessly adapted to several video enhancement tasks, e.g., super-resolution, denoising, and deblocking.
2	  Extensive quantitative and qualitative evaluations demonstrate that the proposed method performs favorably against the state-of-the-art video frame interpolation and enhancement algorithms on a wide range of datasets.


### 42
##### 10.1109/TPAMI.2019.2942028
#### Harmonized Multimodal Learning with Gaussian Process Latent Variable Models


0	  Multimodal learning aims to discover the relationship between multiple modalities.
0	  It has become an important research topic due to extensive multimodal applications such as cross-modal retrieval.
1	  This paper attempts to address the modality heterogeneity problem based on Gaussian process latent variable models (GPLVMs) to represent multimodal data in a common space.
0	  Previous multimodal GPLVM extensions generally adopt individual learning schemes on latent representations and kernel hyperparameters, which ignore their intrinsic relationship.
1	  To exploit strong complementarity among different modalities and GPLVM components, we develop a novel learning scheme called Harmonization, where latent model parameters are jointly learned from each other.
1	  Beyond the correlation fitting or intra-modal structure preservation paradigms widely used in existing studies, the harmonization is derived in a model-driven manner to encourage the agreement between modality-specific GP kernels and the similarity of latent representations.
1	  We present a range of multimodal learning models by incorporating the harmonization mechanism into several representative GPLVM-based approaches.
2	  Experimental results on four benchmark datasets show that the proposed models outperform the strong baselines for cross-modal retrieval tasks, and that the harmonized multimodal learning method is superior in discovering semantically consistent latent representation.


### 43
##### 10.1109/TPAMI.2019.2942030
#### Hierarchical Long Short-Term Concurrent Memory for Human Interaction Recognition


0	  In this work, we aim to address the problem of human interaction recognition in videos by exploring the long-term inter-related dynamics among multiple persons.
0	  Recently, Long Short-Term Memory (LSTM) has become a popular choice to model individual dynamic for single-person action recognition due to its ability to capture the temporal motion information in a range.
0	  However, most existing LSTM-based methods focus only on capturing the dynamics of human interaction by simply combining all dynamics of individuals or modeling them as a whole.
0	  Such methods neglect the inter-related dynamics of how human interactions change over time.
1	  To this end, we propose a novel Hierarchical Long Short-Term Concurrent Memory (H-LSTCM) to model the long-term inter-related dynamics among a group of persons for recognizing human interactions.
1	  Specifically, we first feed each person's static features into a Single-Person LSTM to model the single-person dynamic.
1	  Subsequently, at one time step, the outputs of all Single-Person LSTM units are fed into a novel Concurrent LSTM (Co-LSTM) unit, which mainly consists of multiple sub-memory units, a new cell gate, and a new co-memory cell.
1	  In the Co-LSTM unit, each sub-memory unit stores individual motion information, while this Co-LSTM unit selectively integrates and stores inter-related motion information between multiple interacting persons from multiple sub-memory units via the cell gate and co-memory cell, respectively.
2	  Extensive experiments on several public datasets validate the effectiveness of the proposed H-LSTCM by comparing against baseline and state-of-the-art methods.


### 44
##### 10.1109/TPAMI.2019.2941876
#### Are Large-Scale 3D Models Really Necessary for Accurate Visual Localization?


0	  Accurate visual localization is a key technology for autonomous navigation.
0	  3D structure-based methods employ 3D models of the scene to estimate the full 6 degree-of-freedom (DOF) pose of a camera very accurately.
0	  However, constructing (and extending) large-scale 3D models is still a significant challenge.
0	  In contrast, 2D image retrieval-based methods only require a database of geo-tagged images, which is trivial to construct and to maintain.
0	  They are often considered inaccurate since they only approximate the positions of the cameras.
0	  Yet, the exact camera pose can theoretically be recovered when enough relevant database images are retrieved.
1	  In this paper, we demonstrate experimentally that large-scale 3D models are not strictly necessary for accurate visual localization.
1	  We create reference poses for a large and challenging urban dataset.
2	  Using these poses, we show that combining image-based methods with local reconstructions results in a pose accuracy similar to the state-of-the-art structure-based methods.
2	  Our results suggest that we might want to reconsider the current approach for accurate large-scale localization.


### 45
##### 10.1109/TPAMI.2019.2941472
#### Surface-aware Blind Image Deblurring


0	  Blind image deblurring is a conundrum because there are infinitely many pairs of latent images and blur kernels.
0	  To get a stable and reasonable deblurred image, proper prior knowledge of the latent image and the blur kernel is required.
1	  Different from recent works on the statistical observations of the difference between the blurred image and the clean one, our method is based on the surface-aware strategy from the intrinsic geometrical consideration.
1	  This approach facilitates the blur kernel estimation due to the preserved sharp edges in the intermediate latent images.
2	  Extensive experiments demonstrate that our method outperforms state-of-the-art methods on deblurring the text and natural image.
2	  Moreover, our method can achieve attractive results in some challenging cases, such as low-illumination images with large saturated regions and impulse noise.
2	  A direct extension of our method to the non-uniform deblurring problem also validates the effectiveness of the surface-aware prior.


### 46
##### 10.1109/TPAMI.2019.2941684
#### Loss Decomposition and Centroid Estimation for Positive and Unlabeled Learning


1	  This paper studies Positive and Unlabeled learning (PU learning), of which the target is to build a binary classifier where only positive data and unlabeled data are available for classifier training.
1	  To deal with the absence of negative training data, we first regard all unlabeled data as negative examples with false negative labels, and then convert PU learning into the risk minimization problem in the presence of such one-side label noise.
1	  Specifically, we propose a novel PU learning algorithm dubbed "Loss Decomposition and Centroid Estimation" (LDCE).
1	  By decomposing the loss function of corrupted negative examples into two parts, we show that only the second part is affected by the noisy labels.
1	  Thereby, we may estimate the centroid of corrupted negative set via an unbiased way to reduce the adverse impact of such label noise.
1	  Furthermore, we propose the "Kernelized LDCE" (KLDCE) by introducing the kernel trick, and show that KLDCE can be easily solved by combining Alternative Convex Search (ACS) and Sequential Minimal Optimization (SMO).
2	  Theoretically, we derive the generalization error bound which suggests that the generalization risk of our model converges to the empirical risk with the order of $\mathcal{O}(1/ \sqrt k + 1 /{\sqrt {n-k}} + 1/ \sqrt n)$ ($n$ and $k$ are the amounts of training data and positive data correspondingly).
2	  Experimentally, we conduct intensive experiments on synthetic dataset, UCI benchmark datasets and real-world datasets, and the results demonstrate that our approaches (LDCE and KLDCE) achieve the top-level performance when compared with both classic and state-of-the-art PU learning methods.


### 47
##### 10.1109/TPAMI.2019.2939530
#### Bound and Conquer: Improving Triangulation by Enforcing Consistency


0	  We study the accuracy of triangulation in multi-camera systems with respect to the number of cameras.
2	  We show that, under certain conditions, the optimal achievable reconstruction error decays quadratically as more cameras are added to the system.
1	  Furthermore, we analyse the error decay-rate of major state-of-the-art algorithms with respect to the number of cameras.
2	  To this end, we introduce the notion of consistency for triangulation, and show that consistent reconstruction algorithms achieve the optimal quadratic decay, which is asymptotically faster than some other methods.
2	  Finally, we present simulations results supporting our findings.
#	  Our simulations have been implemented in MATLAB and the resulting code is available in the supplementary material.


### 48
##### 10.1109/TPAMI.2019.2940948
#### Unsupervised Domain Adaptation for Depth Prediction from Images


0	  State-of-the-art methods to infer dense and accurate depth measurements from images rely on deep CNN models trained in an end-to-end fashion on a significant amount of data.
0	  However, despite the outstanding performance achieved, these frameworks suffer a drastic drop in accuracy when dealing with unseen environments much different, concerning appearance (e.g., synthetic vs. real) or context (e.g., indoor vs. outdoor), from those observed during the training phase.
0	  Such domain shift issue is usually softened by fine-tuning on smaller sets of images with depth labels acquired in the target domain with active sensors (e.g., LiDAR).
0	  However, relying on such supervised labeled data is seldom feasible in practical applications.
1	  Therefore, we propose an effective unsupervised domain adaptation technique enabling to overcome the domain shift problem without requiring any groundtruth label.
2	  Our method, deploying much more accessible to obtain stereo pairs, leverages traditional and not learning-based stereo algorithms to produce disparity/depth labels and on confidence measures to assess their degree of reliability.
2	  With these cues, we can fine-tune deep models through a novel confidence-guided loss function, neglecting the effect of outliers gathered from the output of conventional stereo algorithms.


### 49
##### 10.1109/TPAMI.2019.2940225
#### Sequence-to-Segments Networks for Detecting Segments in Videos


0	  Detecting segments of interest from videos is a common problem for many applications.
0	  And yet it is a challenging problem as it often requires not only knowledge of individual target segments, but also contextual understanding of the entire video and the relationships between the target segments.
1	  To address this problem, we propose the Sequence-to-Segments Network (S2N), a novel and general end-to-end sequential encoder-decoder architecture.
1	  S2N first encodes the input video into a sequence of hidden states that capture information progressively, as it appears in the video.
1	  It then employs the Segment Detection Unit (SDU), a novel decoding architecture, that sequentially detects segments.
1	  At each decoding step, the SDU integrates the decoder state and encoder hidden states to detect a target segment.
1	  During training, we address the problem of finding the best assignment of predicted segments to ground truth using the Hungarian Matching Algorithm with Lexicographic Cost.
1	  Additionally, we propose to use the squared Earth Mover's Distance to optimize the localization errors of the segments.
2	  We show the state-of-the-art performance of S2N across numerous tasks, including video highlighting, video summarization, and human action proposal generation.


### 50
##### 10.1109/TPAMI.2019.2940655
#### Topology-Aware Non-Rigid Point Cloud Registration


1	  In this paper, we introduce a non-rigid registration pipeline for unorganized point clouds that may be topologically different.
1	  Standard warp field estimation algorithms, even under robust, discontinuity-preserving regularization, produce erratic motion estimates on boundaries associated with ‘close-to-open’ topology changes.
1	  We overcome this limitation by exploiting backward motion: in the opposite direction, a ‘close-to-open’ event becomes ‘open-to-close’, which is by default handled correctly.
1	  Our approach relies on a general, topology-agnostic warp field estimation algorithm, similar to those employed in recent dynamic reconstruction systems from RGB-D input.
1	  We improve motion estimation on boundaries associated with topology changes in an efficient post-processing phase.
1	  Based on both forward and (inverted) backward warp hypotheses, we explicitly detect regions of the deformed geometry that undergo topological changes by means of local deformation criteria and broadly classify them as ‘contacts’ or ‘separations’.
1	  Subsequently, the two motion hypotheses are seamlessly blended on a local basis, according to the type and proximity of detected events.
2	  Our method achieves state-of-the-art motion estimation accuracy on the MPI Sintel dataset.
2	  Experiments on a custom dataset with topological event annotations demonstrate the effectiveness of our pipeline in estimating motion on event boundaries, as well as promising performance in explicit topological event detection.


### 51
##### 10.1109/TPAMI.2019.2940446
#### MTFH: A Matrix Tri-Factorization Hashing Framework for Efficient Cross-Modal Retrieval


0	  Hashing has recently sparked a great revolution in cross-modal retrieval because of its low storage cost and high query speed.
0	  Recent cross-modal hashing methods often learn unified or equal-length hash codes to represent the multi-modal data and make them intuitively comparable.
0	  However, such representations could inherently sacrifice their representation scalability because the data from different modalities may not have one-to-one correspondence and could be encoded more efficiently by different hash codes of unequal lengths.
1	  To mitigate these problems, this paper exploits a related and relatively unexplored problem: encode the heterogeneous data with varying hash lengths and generalize the cross-modal retrieval in various challenging scenarios.
1	  To this end, a generalized and flexible cross-modal hashing framework, termed Matrix Tri-Factorization Hashing (MTFH), is proposed to work seamlessly in various settings including paired or unpaired multi-modal data, and equal or varying hash length encoding scenarios.
1	  MTFH exploits an efficient objective function to flexibly learn modality-specific hash codes with different length settings, while synchronously learning two semantic correlation matrices for heterogeneous data comparable.
2	  As a result, the derived hash codes are more semantically meaningful for various challenging cross-modal retrieval tasks.
2	  Extensive experiments evaluated on public benchmark datasets highlight the superiority of MTFH under various retrieval scenarios and show its competitive performance with the state-of-the-arts.


### 52
##### 10.1109/TPAMI.2019.2938523
#### Learning Part-based Convolutional Features for Person Re-identification


0	  Part-level features offers fine granularity for pedestrian image description.
0	  In this article, we generally aim to learn discriminative part-informed features for person re-identification.
1	  First, we introduce a general part-level feature learning method, named Part-based Convolutional Baseline (PCB).
1	  Given an image, it outputs a convolutional descriptor consisting of several part-level features.
1	  PCB is general in that it is able to accommodate several part partitioning strategies.
2	  In experiment, we show that the learned descriptor maintains a significantly higher discriminative ability than the global descriptor.
1	  Second, Based on PCB, we propose refined part pooling (RPP) to improve the original partition.
1	  Our idea is that pixels within a well-located part should be similar to each other while being dissimilar with pixels from other parts.
1	  We call it within-part consistency.
1	  When a pixel-wise feature vector in a part is more similar to some other part, it is then an outlier, indicating inappropriate partitioning.
1	  RPP re-assigns these outliers to the parts they are closest to, resulting in refined parts with enhanced within-part consistency.
2	  Experiment confirms that RPP gains another round of performance boost over PCB.
2	  For instance, on the Market-1501 dataset, we achieve (77.4+4.2)% mAP and (92.3+1.5)% rank-1 accuracy, a competitive performance with the state of the art.


### 53
##### 10.1109/TPAMI.2019.2939237
#### On the Global Geometry of Sphere-Constrained Sparse Blind Deconvolution


0	  Blind deconvolution is the problem of recovering a convolutional kernel $a_0$ and an activation signal $x_0$ from their convolution $y = a_{0} ❂ x_{0}$.
0	  This problem is ill-posed without further constraints or priors.
1	  This paper studies the situation where the nonzero entries in the activation signal are sparsely and randomly populated.
1	  We normalize the convolution kernel to have unit Frobenius norm and cast the sparse blind deconvolution problem as a nonconvex optimization problem over the sphere.
1	  With this spherical constraint, every spurious local minimum turns out to be close to some signed shift truncation of the ground truth, under certain hypotheses.
1	  This benign property motivates an effective two stage algorithm that recovers the ground truth from the partial information offered by a suboptimal local minimum.
2	  This geometry-inspired algorithm recovers the ground truth for certain microscopy problems, also exhibits promising performance in the more challenging image deblurring problem.
2	  Our insights into the global geometry and the two stage algorithm extend to the convolutional dictionary learning problem, where a superposition of multiple convolution signals is observed.


### 54
##### 10.1109/TPAMI.2019.2939307
#### Deterministic Approximate Methods for Maximum Consensus Robust Fitting


0	  Maximum consensus estimation plays a critically important role in robust fitting problems in computer vision.
0	  Currently, the most prevalent algorithms for consensus maximization draw from the class of randomized hypothesize-and-verify algorithms, which are cheap but can usually deliver only rough approximate solutions.
0	  On the other extreme, there are exact algorithms which are exhaustive search in nature and can be costly for practical-sized inputs.
1	  This paper fills the gap between the two extremes by proposing deterministic algorithms to approximately optimize the maximum consensus criterion.
1	  Our work begins by reformulating consensus maximization with linear complementarity constraints.
1	  Then, we develop two novel algorithms: one based on non-smooth penalty method with a Frank-Wolfe style optimization scheme, the other based on the Alternating Direction Method of Multipliers (ADMM).
1	  Both algorithms solve convex subproblems to efficiently perform the optimization.
1	  We demonstrate the capability of our algorithms to greatly improve a rough initial estimate, such as those obtained using least squares or a randomized algorithm.
2	  Compared to the exact algorithms, our approach is much more practical on realistic input sizes.
2	  Further, our approach is naturally applicable to estimation problems with geometric residuals.
#	  Matlab code and demo program for our methods can be downloaded from https://goo.gl/FQcxpi.


### 55
##### 10.1109/TPAMI.2019.2938758
#### Res2Net: A New Multi-scale Backbone Architecture


0	  Representing features at multiple scales is of great importance for numerous vision tasks.
0	  Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications.
0	  However, most existing methods represent the multi-scale features in a layer-wise manner.
1	  In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block.
1	  The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer.
1	  The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA.
2	  We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet.
2	  Further ablation studies and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods.
#	  The source code and trained models are available on https://mmcheng.net/res2net/.


### 56
##### 10.1109/TPAMI.2019.2937869
#### Matrix Completion with Deterministic Sampling: Theories and Methods


0	  In some significant applications such as data forecasting, the locations of missing entries cannot obey any non-degenerate distributions, questioning the validity of the prevalent assumption that the missing data is randomly chosen according to some probabilistic model.
1	  To break through the limits of random sampling, we explore in this paper the problem of real-valued matrix completion under the setup of deterministic sampling.
1	  We propose two conditions, isomeric condition and relative well-conditionedness, for guaranteeing an arbitrary matrix to be recoverable from a sampling of the matrix entries.
2	  It is provable that the proposed conditions are weaker than the assumption of uniform sampling and, most importantly, it is also provable that the isomeric condition is necessary for the completions of any partial matrices to be identifiable.
2	  Equipped with these new tools, we prove a collection of theorems for missing data recovery as well as convex/nonconvex matrix completion.
2	  Among other things, we study in detail a Schatten quasi-norm induced method termed isomeric dictionary pursuit (IsoDP), and we show that IsoDP exhibits some distinct behaviors absent in the traditional bilinear programs.


### 57
##### 10.1109/TPAMI.2019.2937294
#### Deep Differentiable Random Forests for Age Estimation


0	  Age estimation from facial images is typically cast as a label distribution learning or regression problem, since aging is a gradual progress.
0	  Its main challenge is the facial feature space w.r.t. ages is inhomogeneous, due to the large variation in facial appearance across different persons of the same age and the non-stationary property of aging.
1	  In this paper, we propose two Deep Differentiable Random Forests methods, Deep Label Distribution Learning Forest (DLDLF) and Deep Regression Forest (DRF), for age estimation.
1	  Both of them connect split nodes to the top layer of convolutional neural networks (CNNs) and deal with inhomogeneous data by jointly learning input-dependent data partitions at the split nodes and age distributions at the leaf nodes.
1	  This joint learning follows an alternating strategy: (1) Fixing the leaf nodes and optimizing the split nodes and the CNN parameters by Back-propagation; (2) Fixing the split nodes and optimizing the leaf nodes by Variational Bounding.
2	  Two Deterministic Annealing processes are introduced into the learning of the split and leaf nodes, respectively, to avoid poor local optima and obtain better estimates of tree parameters free of initial values.
2	  Experimental results show that DLDLF and DRF achieve state-of-the-art results on three age estimation datasets.


### 58
##### 10.1109/TPAMI.2019.2937515
#### Reconstruction Geometric and Optical Parameters of Non-Planar Objects with Thin Film


0	  Here, we propose a novel method to estimate the parameters of non-planar objects with thin film surfaces.
0	  Being able to estimate the optical parameters of objects with thin film surfaces has a wide range of applications from industrial inspections to biological and archaeology research.
0	  However, there are many challenging issues that need to be overcome to model such parameters.
0	  The appearance of thin film objects is highly dependent on the surface orientation and optical parameters such as the refractive index and film thickness.
1	  First, we therefore analyzed the optical parameters of non-planar objects with thin film surfaces.
1	  Next, we proposed and implemented an analysis procedure and demonstrated its effectiveness for studying planar objects with thin film surfaces.
1	  Finally, we developed a device to acquire the shapes and optical parameters of objects with thin film surfaces using a camera and demonstrated the effectiveness of our method experimentally.
1	  Then, we surveyed the errors caused by the light source.
1	  We discussed the difference between the theoretically obtained parameters and experimental data obtained using a hyper spectral camera.


### 59
##### 10.1109/TPAMI.2019.2937086
#### Mask TextSpotter: An End-to-End Trainable Neural Network for Spotting Text with Arbitrary Shapes


0	  Unifying text detection and text recognition in an end-to-end training fashion has become a new trend for reading text in the wild, as these two tasks are highly relevant and complementary.
0	  In this paper, we investigate the problem of scene text spotting, which aims at simultaneous text detection and recognition in natural images.
1	  An end-to-end trainable neural network named as Mask TextSpotter is presented.
1	  Different from the previous text spotters that follow the pipeline consisting of a proposal generation network and a sequence-to-sequence recognition network, Mask TextSpotter enjoys a simple and smooth end-to-end learning procedure, in which both detection and recognition can be achieved directly from two-dimensional space via semantic segmentation.
1	  Further, a spatial attention module is proposed to enhance the performance and universality.
2	  Benefiting from the proposed two-dimensional representation on both detection and recognition, it easily handles text instances of irregular shapes, for instance, curved text.
2	  We evaluate it on four English datasets and one multi-language dataset, achieving consistently superior performance over state-of-the-art methods in both detection and end-to-end text recognition tasks.
2	  Moreover, we further investigate the recognition module of our method separately, which significantly outperforms state-of-the-art methods on both regular and irregular text datasets for scene text recognition.


### 60
##### 10.1109/TPAMI.2019.2937292
#### Discriminative Video Representation Learning Using Support Vector Classifiers


0	  Most popular deep models for action recognition in videos generate independent predictions for short clips, which are then pooled heuristically to assign an action label to the full video segment.
0	  As not all frames may characterize the underlying action, pooling schemes that impose equal importance on all frames might be unfavorable.
1	  To tackle this problem, we propose discriminative pooling, based on the notion that among the features generated on all short clips, there is at least one that characterizes the action.
1	  To identify useful features, we resort to a negative bag consisting of features that are known to be irrelevant.
1	  With the features from the video as a positive bag and the irrelevant features as the negative bag, we cast an objective to learn a (nonlinear) hyperplane that separates the unknown useful feature from the rest in a multiple instance learning formulation within a support vector machine setup.
1	  We use the parameters of this separating hyperplane as descriptors for the video.
1	  Since these parameters are directly related to the support vectors in a max-margin framework, they can be treated as weighted-average-pooling of the features from the bags.
2	  We report results from experiments on eight computer vision benchmarks demonstrating state-of-the-art performance across these tasks.


### 61
##### 10.1109/TPAMI.2019.2936841
#### Neural Image Compression for Gigapixel Histopathology Image Analysis


1	  We propose Neural Image Compression (NIC), a two-step method to build convolutional neural networks for gigapixel image analysis solely using weak image-level labels.
1	  First, gigapixel images are compressed using a neural network trained in an unsupervised fashion, retaining high-level information while suppressing pixel-level noise.
1	  Second, a convolutional neural network (CNN) is trained on these compressed image representations to predict image-level labels, avoiding the need for fine-grained manual annotations.
1	  We compared several encoding strategies, namely reconstruction error minimization, contrastive training and adversarial feature learning, and evaluated NIC on a synthetic task and two public histopathology datasets.
2	  We found that NIC can exploit visual cues associated with image-level labels successfully, integrating both global and local visual information.
2	  Furthermore, we visualized the regions of the input gigapixel images where the CNN attended to, and confirmed that they overlapped with annotations from human experts.


### 62
##### 10.1109/TPAMI.2019.2936378
#### Adversarial Attack Type I: Cheat Classifiers by Significant Changes


0	  Despite the great success of deep neural networks, the adversarial attack can cheat some well-trained classifiers by small permutations.
0	  In this paper, we propose another type of adversarial attack that can cheat classifiers by significant changes.
0	  For example, we can significantly change a face but well-trained neural networks still recognize the adversarial and the original example as the same person.
0	  Statistically, the existing adversarial attack increases Type II error and the proposed one aims at Type I error, which are hence named as Type II and Type I adversarial attack, respectively.
0	  The two types of attack are equally important but are essentially different, which are intuitively explained and numerically evaluated.
1	  To implement the proposed attack, a supervised variation autoencoder is designed and then the classifier is attacked by updating the latent variables using gradient information.
2	  Experimental results show that our method is practical and effective to generate Type I adversarial examples on large-scale image datasets.
2	  Most of these generated examples can pass detectors designed for defending Type II attack and the strengthening strategy is only efficient with a specific type attack, both implying that the underlying reasons for Type I and Type II attack are different.


### 63
##### 10.1109/TPAMI.2019.2936024
#### Semi-Supervised Adversarial Monocular Depth Estimation


0	  In this paper, we address the problem of monocular depth estimation when only a limited amount of training image-depth pairs are available.
0	  To achieve high regression accuracy, state-of-the-art estimation methods rely on CNNs trained with a vast amount of image-depth pairs, which are prohibitively costly or even infeasible to acquire.
1	  Aiming to break the bottleneck of such expensive data collections, in this paper, we propose a semi-supervised adversarial learning framework, which only utilizes a small amount of image-depth pairs with a large amount of cheaply-available monocular images to pursuit high accuracy.
1	  In particular, we use one generator to regress the depth and two discriminators to evaluate the predicted depth.
2	  These two discriminators provide their feedbacks to the generator as the loss to generate more realistic and accurate depth predictions.
2	  Experiments show that the proposed approach can (1) improve most state-of-the-art models on NYUD v2 dataset by effectively leveraging additional unlabeled data sources; (2) reach state-of-the-art accuracy when the training set is small, e.g., on Make3D dataset; (3) adapts well to an unseen new dataset (Make3D in our case) after training on an annotated dataset (KITTI in our case).


### 64
##### 10.1109/TPAMI.2019.2935715
#### Saliency Prediction in the Deep Learning Era: Successes and Limitations


0	  Visual saliency models have enjoyed a big leap in performance in recent years, thanks to advances in deep learning and large scale annotated data.
0	  Despite enormous effort and huge breakthroughs, however, models still fall short in reaching human-level accuracy.
0	  In this work, I explore the landscape of the field emphasizing on new deep saliency models, benchmarks, and datasets.
1	  A large number of image and video saliency models are reviewed and compared over two image benchmarks and two large scale video datasets.
2	  Further, I identify factors that contribute to the gap between models and humans and discuss remaining issues that need to be addressed to build the next generation of more powerful saliency models.
2	  Some specific questions that are addressed include: in what ways current models fail, how to remedy them, what can be learned from cognitive studies of attention, how explicit saliency judgments relate to fixations, how to conduct fair model comparison, and what are the emerging applications of saliency models.


### 65
##### 10.1109/TPAMI.2019.2934852
#### Learning Energy-based Spatial-Temporal Generative ConvNets for Dynamic Patterns


0	  Video sequences contain rich dynamic patterns, such as dynamic texture patterns that exhibit stationarity in the temporal domain, and action patterns that are non-stationary in either spatial or temporal domain.
2	  We show that an energy-based spatial-temporal generative ConvNet can be used to model and synthesize dynamic patterns.
1	  The model defines a probability distribution on the video sequence, and the log probability is defined by a spatial-temporal ConvNet that consists of multiple layers of spatial-temporal filters to capture spatial-temporal patterns of different scales.
1	  The model can be learned from the training video sequences by an "analysis by synthesis" learning algorithm that iterates the following two steps.
1	  Step 1 synthesizes video sequences from the currently learned model.
1	  Step 2 then updates the model parameters based on the difference between the synthesized video sequences and the observed training sequences.
2	  We show that the learning algorithm can synthesize realistic dynamic patterns.
2	  We also show that it is possible to learn the model from incomplete training sequences with either occluded pixels or missing frames, so that model learning and pattern completion can be accomplished simultaneously.


### 66
##### 10.1109/TPAMI.2019.2934455
#### Polyhedral Conic Classifiers for Computer Vision Applications and Open Set Recognition


0	  This paper introduces a family of quasi-linear discriminants that outperform current large-margin methods in sliding window visual object detection and open set recognition tasks.
0	  In these applications, the classification problems are both numerically imbalanced -- positive training and test windows are much rarer than negative ones -- and geometrically asymmetric -- the positive samples typically form compact, visually-coherent groups while negatives are much more diverse, including anything at all that is not a well-centered sample from the target class.
0	  For such tasks, there is a need for discriminants whose decision regions focus on tightly circumscribing the positive class, while still taking account of negatives in zones where the two classes overlap.
1	  To this end, we propose a family of quasi-linear polyhedral conic discriminants whose positive regions are distorted L1 or L2 balls.
1	  In addition, we also integrated the proposed classification loss into deep neural networks so that both the features and classifier can be learned simultaneously end-to-end fashion to improve the classification accuracies.
1	  The methods can be trained from either binary or positive-only samples using constrained quadratic programs related to SVMs.
2	  Our experiments show that they significantly outperform linear SVMs, deep neural networks using softmax loss function and existing one-class discriminants.


### 67
##### 10.1109/TPAMI.2019.2934052
#### Matching Seqlets: An Unsupervised Approach for Locality Preserving Sequence Matching


1	  In this paper, we propose a novel unsupervised approach for sequence matching by explicitly accounting for the locality properties in the sequences.
0	  In contrast to conventional approaches that rely on frame-to-frame matching, we conduct matching using sequencelet or seqlet, a sub-sequence wherein the frames share strong similarities and are thus grouped together.
1	  The optimal seqlets and matching between them are learned jointly, without any supervision from users.
1	  The learned seqlets preserve the locality information at the scale of interest and resolve the ambiguities during matching, which are omitted by frame-based matching methods.
2	  We show that our proposed approach outperforms the state-of-the-art ones on datasets of different domains including human actions, facial expressions, speech, and character strokes.


### 68
##### 10.1109/TPAMI.2019.2933829
#### Inferring Latent Domains for Unsupervised Deep Domain Adaptation


0	  Unsupervised Domain Adaptation (UDA) refers to the problem of learning a model in a target domain where labeled data are not available by leveraging information from annotated data in a source domain.
0	  Most deep UDA approaches operate in a single-source, single-target scenario, i.e.
0	  they assume that the source and the target samples arise from a single distribution.
0	  However, in practice most datasets can be regarded as mixtures of multiple domains.
0	  In these cases, exploiting traditional single-source, single-target methods for learning classification models may lead to poor results.
0	  Furthermore, it is often difficult to provide the domain labels for all data points, i.e. latent domains should be automatically discovered.
1	  This paper introduces a novel deep architecture which addresses the problem of UDA by automatically discovering latent domains in visual datasets and exploiting this information to learn robust target classifiers.
1	  Specifically, our architecture is based on two main components, i.e. a side branch that automatically computes the assignment of each sample to its latent domain and novel layers that exploit domain membership information to appropriately align the distribution of the CNN internal feature representations to a reference distribution.
2	  We evaluate our approach on publicly available benchmarks, showing that it outperforms state-of-the-art domain adaptation methods.


### 69
##### 10.1109/TPAMI.2019.2933841
#### Faster First-Order Methods for Stochastic Non-Convex Optimization on Riemannian Manifolds


0	  First-order non-convex Riemannian optimization algorithms have gained recent popularity in structured machine learning problems including principal component analysis and low-rank matrix completion.
1	  The current paper presents an efficient Riemannian Stochastic Path Integrated Differential EstimatoR (R-SPIDER) algorithm to solve the finite-sum and online Riemannian non-convex minimization problems.
1	  At the core of R-SPIDER is a recursive semi-stochastic gradient estimator that can accurately estimate Riemannian gradient under not only exponential mapping and parallel transport, but also general retraction and vector transport operations.
2	  Compared with prior Riemannian algorithms, such a recursive gradient estimation mechanism endows R-SPIDER with lower computational cost in first-order oracle complexity.
2	  Specifically, for finite-sum problems with $n$ components, R-SPIDER is proved to converge to an $\epsilon$-approximate stationary point within $\mathcal{O}\big(\min\big(n+\frac{\sqrt{n}}{\epsilon^2},\frac{1}{\epsilon^3}\big)\big)$ stochastic gradient evaluations, beating the best-known complexity $\mathcal{O}\big(n+\frac{1}{\epsilon^4}\big)$; for online optimization, R-SPIDER is shown to converge with $\mathcal{O}\big(\frac{1}{\epsilon^3}\big)$ complexity which is, to the best of our knowledge, the first non-asymptotic result for online Riemannian optimization.
1	  For the special case of gradient dominated functions, we further develop a variant of R-SPIDER with improved linear rate of convergence.
2	  Extensive experimental results demonstrate the advantage of the proposed algorithms over the state-of-the-art Riemannian non-convex optimization methods.


### 70
##### 10.1109/TPAMI.2019.2933818
#### Virtual Point Removal for Large-Scale 3D Point Clouds With Multiple Glass Planes


0	  Large-scale 3D point clouds (LS3DPCs) captured by terrestrial LiDAR scanners often include virtual points which are generated by glass reflection.
0	  The virtual points may degrade the performance of various computer vision techniques when applied to LS3DPCs.
1	  In this paper, we propose a virtual point removal algorithm for LS3DPCs with multiple glass planes.
1	  We first estimate multiple glass regions by modeling the reliability with respect to each glass plane, respectively, such that the regions are assigned high reliability when they have multiple echo pulses for each emitted laser pulse.
1	  Then we detect each point whether it is a virtual point or not.
1	  For a given point, we recursively traverse all the possible trajectories of reflection, and select the optimal trajectory which provides a point with a similar geometric feature to a given point at the symmetric location.
1	  We evaluate the performance of the proposed algorithm on various LS3DPC models with diverse numbers of glass planes.
2	  Experimental results show that the proposed algorithm estimates multiple glass regions faithfully and detects the virtual points successfully.
2	  Moreover, we also show that the proposed algorithm yields a much better performance of reflection artifact removal compared with the existing method qualitatively and quantitatively.



### 71
##### 10.1109/TPAMI.2019.2933510
#### P-CNN: Part-Based Convolutional Neural Networks for Fine-Grained Visual Categorization


1 	  This paper proposes an end-to-end fine-grained visual categorization system, termed Part-based Convolutional Neural Network (P-CNN), which consists of three modules.
1	  The first module is a Squeeze-and-Excitation (SE) block, which learns to recalibrate channel-wise feature responses by emphasizing informative channels and suppressing less useful ones.
1	  The second module is a Part Localization Network (PLN) used to locate distinctive object parts, through which a bank of convolutional filters are learned as discriminative part detectors.
1	  Thus, a group of informative parts can be discovered by convolving the feature maps with each part detector.
1	  The third module is a Part Classification Network (PCN) that has two streams.
1	  The first stream classifies each individual object part into image-level categories.
1	  The second stream concatenates part features and global feature into a joint feature for the final classification.
1	  In order to learn powerful part features and boost the joint feature capability, we propose a Duplex Focal Loss used for metric learning and part classification, which focuses on training hard examples.
1	  We further merge PLN and PCN into a unified network for an end-to-end training process via a simple training technique.
2	  Comprehensive experiments and comparisons with state-of-the-art methods on three benchmark datasets demonstrate the effectiveness of our proposed method.


### 72
##### 10.1109/TPAMI.2019.2933209
#### Parallel and Scalable Heat Methods for Geodesic Distance Computation


0	  In this paper, we propose a parallel and scalable approach for geodesic distance computation on triangle meshes.
2	  Our key observation is that the recovery of geodesic distance in the heat method can be reformulated as an optimization of its gradients subject to integrability, which can be solved using an efficient fixed-order method that requires no linear system solving and converges quickly.
1	  Afterwards, the geodesic distance is efficiently recovered by parallel integration of the optimized gradients in breadth-first order.
1	  Moreover, we employ a similar breadth-first strategy to derive a parallel Gauss-Seidel solver for the diffusion step in the heat method.
1	  To further lower the memory consumption from gradient optimization on faces, we also propose a formulation that optimizes the projected gradients on edges, further reducing the memory footprint by about 50%.
1	  Our approach is trivially parallelizable, with a low memory footprint that grows linearly with respect to the model size.
2	  This makes it particularly suitable for handling large models.
2	  Experimental results show that it can efficiently compute geodesic distance on meshes with more than 200 million vertices on a desktop PC with 128GB RAM, outperforming the original heat method and other state-of-the-art geodesic distance solvers.


### 73
##### 10.1109/TPAMI.2019.2932979
#### Sparse Coding of Shape Trajectories for Facial Expression and Action Recognition


0	  The detection and tracking of human landmarks in video streams has gained in reliability partly due to the availability of affordable RGB-D sensors.
0	  The analysis of such time-varying geometric data is playing an important role in automatic human behavior understanding.
0	  However, suitable shape representations as well as their temporal evolution, termed trajectories, often lie to nonlinear manifolds.
0	  This puts an additional constraint (i.e., nonlinearity) in using conventional Machine Learning techniques.
1	  As a solution, this paper accommodates the well-known Sparse Coding and Dictionary Learning approach to study time-varying shapes on the Kendall shape spaces of 2D and 3D landmarks.
1	  We illustrate effective coding of 3D skeletal sequences for action recognition and 2D facial landmark sequences for macro- and micro-expression recognition.
1	  To overcome the inherent nonlinearity of the shape spaces, intrinsic and extrinsic solutions were explored.
2	  As main results, shape trajectories give rise to more discriminative time-series with suitable computational properties, including sparsity and vector space structure.
2	  Extensive experiments conducted on commonly-used datasets demonstrate the competitiveness of the proposed approaches with respect to state-of-the-art.


### 74
##### 10.1109/TPAMI.2019.2932429
#### DAC-SDC Low Power Object Detection Challenge for UAV Applications


0	  The 55th Design Automation Conference (DAC) held its first System Design Contest (SDC) in 2018.
1	  SDC'18 features a lower power object detection challenge (LPODC) on designing and implementing novel algorithms based object detection in images taken from unmanned aerial vehicles (UAV).
1	  The dataset includes 95 categories and 150k images, and the hardware platforms include Nvidia's TX2 and Xilinx's PYNQ Z1.
1	  DAC-SDC'18 attracted more than 110 entries from 12 countries.
1	  This paper presents in detail the dataset and evaluation procedure.
2	  It further discusses the methods developed by some of the entries as well as representative results.
2	  The paper concludes with directions for future improvements.


### 75
##### 10.1109/TPAMI.2019.2932976
#### Multivariate Extension of Matrix-based Renyi's α-order Entropy Functional


0	  The matrix-based Renyi's α-order entropy functional was recently introduced using the normalized eigenspectrum of a Hermitian matrix of the projected data in a reproducing kernel Hilbert space (RKHS).
0	  However, the current theory in the matrix-based Renyi's α-order entropy functional only defines the entropy of a single variable or mutual information between two random variables.
0	  In information theory and machine learning communities, one is also frequently interested in multivariate information quantities, such as the multivariate joint entropy and different interactive quantities among multiple variables.
1	  In this paper, we first define the matrix-based Renyi's α-order joint entropy among multiple variables.
1	  We then show how this definition can ease the estimation of various information quantities that measure the interactions among multiple variables, such as interactive information and total correlation.
1	  We finally present an application to feature selection to show how our definition provides a simple yet powerful way to estimate a widely-acknowledged intractable quantity from data.
2	  A real example on hyperspectral image (HSI) band selection is also provided.


### 76
##### 10.1109/TPAMI.2019.2932415
#### SafePredict: A Meta-Algorithm for Machine Learning That Uses Refusals to Guarantee Correctness


0	  SafePredict is a novel meta-algorithm that works with any base prediction algorithm for online data to guarantee an arbitrarily chosen correctness rate, $1-\in$, by allowing refusals.
0	  Allowing refusals means that the meta-algorithm may refuse to emit a prediction produced by the base algorithm on occasion so that the error rate on non-refused predictions does not exceed $\in$.
1	  The SafePredict error bound does not rely on any assumptions on the data distribution or the base predictor.
1	  When the base predictor happens not to exceed the target error rate $\in$, SafePredict refuses only a finite number of times.
1	  When the error rate of the base predictor changes through time SafePredict makes use of a weight-shifting heuristic that adapts to these changes without knowing when the changes occur yet still maintains the correctness guarantee.
2	  Empirical results show that (i) SafePredict compares favorably with state-of-the art confidence based refusal mechanisms which fail to offer robust error guarantees; and (ii) combining SafePredict with such refusal mechanisms can in many cases further reduce the number of refusals.
#	  Our software is included in the supplementary material.


### 77
##### 10.1109/TPAMI.2019.2930192
#### Pattern of Local Gravitational Force(PLGF): A novel Local Image Descriptor


1	  This paper presents a novel local image descriptor called Pattern of Local Gravitational Force (PLGF).
1	  It is inspired by Law of Universal Gravitation.
1	  PLGF is a hybrid descriptor, which is a combination of two feature components: one is the Pattern of Local Gravitational Force Magnitude (PLGFM), and another is Pattern of Local Gravitational Force Angle (PLGFA).
1	  PLGFM encodes the local gravitational force magnitude, and PLGFA encodes the local gravitational force angle that the center pixel exerts on all other pixels within a local neighborhood.
1	  We propose a novel noise resistance and the edge-preserving binary pattern called neighbors to center difference binary pattern (NCDBP) for gravitational force magnitude encoding.
2	  Finally, the histograms of the two components are concatenated to construct the PLGF descriptor.
2	  Experimental results on the existing face recognition databases, texture database, and biomedical image database show that PLGF is an effective image descriptor, and it outperforms other widely used existing descriptors.
2	  Even if in complicated variations like noise, and illumination with smaller databases, a combination of PLGF and convolutional neural network (CNN) performs consistently better than other state-of-the-art techniques.


### 78
##### 10.1109/TPAMI.2019.2932058
#### Hierarchical Deep Click Feature Prediction for Fine-grained Image Recognition


0	  The click feature of an image, defined as the user-click-frequency vector of the image on a pre-defined word vocabulary, is known to effectively reduce the semantic gap for fine-grained image recognition.
0	  Unfortunately, user-click-frequency data are usually absent in practice.
0	  It remains challenging to predict the click feature from the visual feature, because the user-click-frequency vector of an image is always noisy and sparse.
1	  In this paper, we devise a Hierarchical Deep Word Embedding (HDWE) model by integrating sparse constraints and an improved RELU operator to address click feature prediction from visual features.
1	  HDWE is a coarse-to-fine click feature predictor that is learned with the help of an auxiliary image dataset containing click information.
1	  It can therefore discover the hierarchy of word semantics.
2	  We evaluate HDWE on three dog and one bird image datasets, in which Clickture-Dog and Clickture-Bird are respectively utilized as auxiliary datasets to provide click data.
2	  Our empirical studies show that HDWE has 1) higher recognition accuracy, 2) a larger compression ratio, and 3) good one-shot learning ability and scalability to unseen categories.


### 79
##### 10.1109/TPAMI.2019.2931897
#### Selfie Video Stabilization


0	  We propose a novel algorithm for stabilizing selfie videos.
0	  Our goal is to automatically generate stabilized video that has optimal smooth motion in the sense of both foreground and background.
0	  The key insight is that non-rigid foreground motion in selfie videos can be analyzed using a 3D face model, and background motion can be analyzed using optical flow.
1	  We use second derivative of temporal trajectory of selected pixels as the measure of smoothness.
2	  Our algorithm stabilizes selfie videos by minimizing the smoothness measure of the background, regularized by the motion of the foreground.
2	  Experiments show that our method outperforms state-of-the-art general video stabilization techniques in selfie videos.


### 80
##### 10.1109/TPAMI.2019.2932062
#### Switchable Normalization for Learning-to-Normalize Deep Representation


1	  We address a learning-to-normalize problem by proposing Switchable Normalization (SN), which learns to select different normalizers for different normalization layers of a deep neural network.
1	  SN employs three distinct scopes to compute statistics (means and variances) including a channel, a layer, and a minibatch.
1	  SN switches between them by learning their importance weights in an end-to-end manner.
1	  It has several good properties.
1	  First, it adapts to various network architectures and tasks.
1	  Second, it is robust to a wide range of batch sizes, maintaining high performance even when small minibatch is presented (eg 2 images/GPU).
1	  Third, SN does not have sensitive hyper-parameter, unlike group normalization that searches the number of groups as a hyper-parameter.
1	  Without bells and whistles, SN outperforms its counterparts on various challenging benchmarks, such as ImageNet, COCO, CityScapes, ADE20K, MegaFace and Kinetics.
2	  Analyses of SN are also presented to answer the following three questions: 
2     (a) Is it useful to allow each normalization layer to select its own normalizer?
2	  (b) What impacts the choices of normalizers?
2	  (c) Do different tasks and datasets prefer different normalizers?
2	  We hope SN will help ease the usage and understand the normalization techniques in deep learning.
#	  The code of SN has been released at https://github.com/switchablenorms.


### 81
##### 10.1109/TPAMI.2019.2931577
#### Absolute Pose Estimation of Central Cameras Using Planar Regions


1	  A novel method is proposed for the absolute pose estimation of a central 2D camera with respect to 3D depth data without the use of any dedicated calibration pattern or explicit point correspondences.
1	  The proposed method has no specific assumption about the data source: plain depth information is expected from the 3D sensing device and a central camera is used to capture the 2D images.
1	  Both the perspective and omnidirectional central cameras are handled within a single generic camera model.
1	  Pose estimation is formulated as a 2D-3D nonlinear shape registration task which is solved without point correspondences or complex similarity metrics.
1	  It relies on a set of corresponding planar regions, and the pose parameters are obtained by solving an overdetermined system of nonlinear equations.
2	  The efficiency and robustness of the proposed method were confirmed on both large scale synthetic data and on real data acquired from various types of sensors.


### 82
##### 10.1109/TPAMI.2019.2931569
#### Dual Adversarial Transfer for Sequence Labeling


0	  We propose a new architecture for addressing sequence labeling, termed Dual Adversarial Transfer Network (DATNet).
1	  Specifically, the proposed DATNet includes two variants, i.e., DATNet-F and DATNet-P, which are proposed to explore effective feature fusion between high and low resource.
1	  To address the noisy and imbalanced training data, we propose a novel Generalized Resource-Adversarial Discriminator (GRAD) and adopt adversarial training to boost model generalization.
2	  We investigate the effects of different components of DATNet across different domains and languages, and show that significant improvement can be obtained especially for low-resource data.
2	  Without augmenting any additional hand-crafted features, we achieve state-of-the-art performances on CoNLL, Twitter, PTB-WSJ, OntoNotes and Universal Dependencies with three popular sequence labeling tasks, i.e. Named entity recognition (NER), Part-of-Speech (POS) Tagging and Chunking.


### 83
##### 10.1109/TPAMI.2019.2929519
#### Graph Embedding Using Frequency Filtering


0	  The target of graph embedding is to embed graphs in vector space such that the embedded feature vectors follow the differences and similarities of the source graphs.
1	  In this paper, a novel method named Frequency Filtering Embedding (FFE) is proposed which uses graph Fourier transform and Frequency filtering as a graph Fourier domain operator for graph feature extraction.
0	  Frequency filtering amplifies or attenuates selected frequencies using appropriate filter functions.
1	  Here, heat, anti-heat, part-sine and identity filter sets are proposed as the filter functions.
1	  A generalized version of FFE named GeFFE is also proposed by defining pseudo-Fourier operators.
1	  This method can be considered as a general framework for formulating some previously defined invariants in other works by choosing a suitable filter bank and defining suitable pseudo-Fourier operators.
1	  This flexibility empowers GeFFE to adapt itself to the properties of each graph dataset unlike the previous spectral embedding methods and leads to superior classification accuracy relative to the others.
2	  Utilizing the proposed part-sine filter set which its members filter different parts of the spectrum in turn improves the classification accuracy of GeFFE method.
2	  Additionally GeFFE resolves the cospectrality problem entirely in tested datasets.


### 84
##### 10.1109/TPAMI.2019.2931317
#### Reconstruct as Far as You Can: Consensus of Non-Rigid Reconstruction from Feasible Regions


0	  Much progress has been made for non-rigid structure from motion during the last two decades, which made it possible to provide reasonable solutions for benchmark data.
0	  In order to utilize NRSfM techniques in realistic situations, however, we are facing two problems that must be solved: First, general scenes contain complex deformations as well as multiple objects, which violates usual assumptions of previous proposals.
0	  Second, there are many unreconstructable regions in the video, either because of discontinued tracks of 2D trajectories or those regions static towards camera, which require careful manipulations.
1	  In this paper, we show that a consensus-based reconstruction framework can handle these issues effectively.
1	  Even though the entire scene is complex, its parts usually have simpler deformations, and even though there are some unreconstructable parts, they can be weeded out to reduce harmful effect on the entire reconstruction.
2	  The main difficulty lies in identifying appropriate parts, however, it can be effectively avoided by sampling parts stochastically and then aggregate their reconstructions afterwards.
2	  Experimental results show that the proposed method renews the state-of-the-art for popular benchmark data under much harsher environments, i.e, narrow camera view ranges, and it can reconstruct real-word videos effectively for as many areas as it can.


### 85
##### 10.1109/TPAMI.2019.2930985
#### Learning Continuous Face Age Progression: A Pyramid of GANs


0	  The two underlying requirements of face age progression, i.e., aging accuracy and identity permanence, are not well studied in the literature.
1	  This paper presents a novel generative adversarial network based approach to address the issues in a coupled manner.
1	  It separately models the constraints for intrinsic subject-specific characteristics and age-specific facial changes w.r.t. the elapsed time, ensuring that the generated faces present desired aging effects while keeping personalized properties stable.
1	  To render photo-realistic facial details, high-level age-specific features conveyed by the synthesized face are estimated by a pyramidal adversarial discriminator at multiple scales, which simulates aging effects in a finer way.
1	  Further, an adversarial learning scheme is introduced to simultaneously train a single generator and multiple parallel discriminators, resulting in smooth continuous face aging sequences.
2	  Our method is applicable even in the presence of variations in pose, expression, makeup, etc., achieving remarkably vivid aging effects.
2	  Quantitative evaluations by a COTS face recognition system demonstrate that the target age distributions are accurately recovered, and 99.88% and 99.98% age progressed faces are correctly verified at 0.001% FAR after transformations of approximately 28 and 23 years on MORPH and CACD, respectively.
2	  Both visual and quantitative assessments show that the approach advances the state-of-the-art.


### 86
##### 10.1109/TPAMI.2019.2929034
#### Visual Tracking via Dynamic Memory Networks


0	  Template-matching methods for visual tracking have gained popularity recently due to their good performance and fast speed.
0	  However, they lack effective ways to adapt to changes in the target object's appearance, making their tracking accuracy still far from state-of-the-art.
1	  In this paper, we propose a dynamic memory network to adapt the template to the target's appearance variations during tracking.
1	  The reading and writing process of the external memory is controlled by an LSTM network with the search feature map as input.
1	  A spatial attention mechanism is applied to concentrate the LSTM input on the potential target as the location of the target is at first unknown.
1	  To prevent aggressive model adaptivity, we apply gated residual template learning to control the amount of retrieved memory that is used to combine with the initial template.
1	  In order to alleviate the drift problem, we also design a "negative" memory unit that stores templates for distractors, which are used to cancel out wrong responses from the object template.
1	  To further boost the tracking performance, an auxiliary classification loss is added after the feature extractor part.
1	  Unlike tracking-by-detection methods where the object's information is maintained by the weight parameters of neural networks, which requires expensive online fine-tuning to be adaptable, our tracker runs completely feed-forward and adapts to the target's appearance changes by updating the external memory.
2	  Moreover, the capacity of our model is not determined by the network size as with other trackers - the capacity can be easily enlarged as the memory requirements of a task increase, which is favorable for memorizing long-term object information.
2	  Extensive experiments on the OTB and VOT datasets demonstrate that our trackers perform favorably against state-of-the-art tracking methods while retaining real-time speed.


### 87
##### 10.1109/TPAMI.2019.2930051
#### Rotation Averaging with the Chordal Distance: Global Minimizers and Strong Duality


0	  In this paper we explore the role of duality principles within the problem of rotation averaging, a fundamental task in a wide range of applications.
0	  In its conventional form, rotation averaging is stated as a minimization over multiple rotation constraints.
0	  As these constraints are non-convex, this problem is generally considered challenging to solve globally.
1	  We show how to circumvent this difficulty through the use of Lagrangian duality.
1	  While such an approach is well-known it is normally not guaranteed to provide a tight relaxation.
1	  Based on spectral graph theory, we analytically prove that in many cases there is no duality gap unless the noise levels are severe.
1	  This allows us to obtain certifiably global solutions to a class of important non-convex problems in polynomial time.
2	  We also propose an efficient, scalable algorithm that outperforms general purpose numerical solvers by a large margin and compares favourably to current state-of-the-art.
2	  Further, our approach is able to handle the large problem instances commonly occurring in structure from motion settings and it is trivially parallelizable.
2	  Experiments are presented for a number of different instances of both synthetic and real-world data.


### 88
##### 10.1109/TPAMI.2019.2930258
#### Every Pixel Counts ++: Joint Learning of Geometry and Motion with 3D Holistic Understanding


0	  Learning to estimate 3D geometry in a single frame and optical flow from consecutive frames by watching unlabeled videos via deep convolutional network has made significant progress recently.
0	  Current state-of-the-art (SoTA) methods treat the two tasks independently.
0	  One important assumption of the existing depth estimation methods is that the scenes contain no moving object.
1	  In this paper, we propose to address the two tasks as a whole, i.e. to jointly understand per-pixel 3D geometry and motion.
1	  This eliminates the need of static scene assumption and enforces the inherent geometrical consistency during the learning process, yielding significantly improved results for both tasks.
1	  We call our method as "Every Pixel Counts++" or "EPC++".
1	  Various loss terms are formulated to jointly supervise the learning across geometrical cues and effective adaptive training strategy is proposed to achieve better performance.
2	  Comprehensive experiments were conducted on datasets with different scenes, including driving scenario (KITTI 2012 and KITTI 2015 datasets), mixed outdoor/indoor scenes (Make3D) and synthetic animation (MPI Sintel dataset).
2	  Performance on the five tasks of depth estimation, optical flow estimation, odometry, moving object segmentation and scene flow estimation shows that our approach outperforms other SoTA methods, demonstrating the effectiveness of each module of our proposed method.


### 89
##### 10.1109/TPAMI.2019.2930501
#### Fast Exact Evaluation of Univariate Kernel Sums


1	  This paper presents new methodology for computationally efficient evaluation of univariate kernel sums.
1	  It is shown that a rich class of kernels allows for exact evaluation of functions expressed as a sum of kernels using simple recursions.
1	  Given an ordered sample the computational complexity is linear in the sample size.
2	  Direct applications to the estimation of denisties and their derivatives shows that the proposed approach is competitive with the state-of-the-art.
2	  Extensions to multivariate problems including independent component analysis and spatial smoothing illustrate the versatility of univariate kernel estimators, and highlight the efficiency and accuracy of the proposed approach.
2	  Multiple applications in image processing, including image deconvolution; denoising; and reconstruction are considered, showing that the proposed approach offers very promising potential in these fields.


### 90
##### 10.1109/TPAMI.2019.2929520
#### Deep Affinity Network for Multiple Object Tracking


0	  Multiple Object Tracking (MOT) plays an important role in solving many fundamental problems in video analysis and computer vision.
0	  Most MOT methods employ two steps: Object Detection and Data Association.
0	  The first step detects objects of interest in every frame of a video, and the second establishes correspondence between the detected objects in different frames to obtain their tracks.
0	  Object detection has made tremendous progress in the last few years due to deep learning.
0	  However, data association for tracking still relies on hand crafted constraints such as appearance, motion, spatial proximity, grouping etc. to compute affinities between the objects in different frames.
1	  In this paper, we harness the power of deep learning for data association in tracking by jointly modeling object appearances and their affinities between different frames in an end-to-end fashion.
1	  The proposed Deep Affinity Network (DAN) learns compact, yet comprehensive features of pre-detected objects at several levels of abstraction, and performs exhaustive pairing permutations of those features in any two frames to infer object affinities.
1	  DAN also accounts for multiple objects appearing and disappearing between video frames.
1	  We exploit the resulting efficient affinity computations to associate objects in the current frame deep into the previous frames for reliable on-line tracking.
2	  Our technique is evaluated on popular multiple object tracking challenges MOT15, MOT17 and UA-DETRAC.
2	  Comprehensive benchmarking under twelve evaluation metrics demonstrates that our approach is among the best performing techniques on the leader board for these challenges.
#	  The open source implementation of our work is available at https://github.com/shijieS/SST.git.


### 91
##### 10.1109/TPAMI.2019.2928806
#### Distributed Variational Representation Learning


0	  The problem of distributed representation learning is one in which multiple observations $X_1,\ldots,X_K$ are processed separately to learn as much information as possible about some source $Y$.
1	  We investigate this problem from information-theoretic grounds, through a generalization of Tishby's centralized Information Bottleneck (IB) method to the distributed setting.
1	  Specifically, $K\geq 2$ encoders, compress their observations $X_1,\ldots,X_K$ separately such that, collectively, the produced representations preserve as much information as possible about $Y$.
1	  We study both discrete memoryless (DM) and vector Gaussian data models.
1	  For the discrete model, we establish a single-letter characterization of the optimal tradeoff for a class of memoryless sources.
1	  For the vector Gaussian model, we provide an explicit characterization of the optimal complexity-relevance tradeoff.
1	  Furthermore, we develop a variational bound on the complexity-relevance tradeoff which generalizes the evidence lower bound (ELBO) to the distributed setting.
1	  We provide two algorithms to compute this bound: i) a Blahut-Arimoto type iterative algorithm which computes optimal complexity-relevance mappings by iterating over a set of self-consistent equations, and ii) a variational inference type algorithm in which the encoding mappings are parametrized by neural networks, the bound approximated by Markov sampling and optimized with stochastic gradient descent.
2	  Numerical results on synthetic and real datasets are provided to support the efficiency of the approaches and algorithms developed in this paper.


### 92
##### 10.1109/TPAMI.2019.2928550
#### Stereo Matching Using Multi-level Cost Volume and Multi-scale Feature Constancy


0	  For CNNs based stereo matching methods, cost volumes play an important role in achieving good matching accuracy.
1	  In this paper, we present an end-to-end trainable convolution neural network to fully use cost volumes for stereo matching.
1	  Our network consists of three sub-modules, i.e., shared feature extraction, initial disparity estimation, and disparity refinement.
1	  These sub-modules of our network are tightly-coupled, making it compact and easy to train.
1	  Moreover, we investigate the problem of developing a robust model to perform well across multiple datasets with different characteristics.
1	  We achieve this by introducing a two-stage finetuning scheme to gently transfer the model to target datasets.
1	  Specifically, in the first stage, the model is finetuned using both a large synthetic dataset and the target datasets with a relatively large learning rate, while in the second stage the model is trained using only the target datasets with a small learning rate.
2	  The proposed method is tested on several benchmarks including the Middlebury 2014, KITTI 2015, ETH3D 2017, and SceneFlow datasets.
2	  Experimental results show that our method achieves the state-of-the-art performance on all the datasets.
2	  The proposed method also won the 1st prize on the Stereo task of Robust Vision Challenge 2018.


### 93
##### 10.1109/TPAMI.2019.2929170
#### Confidence Propagation through CNNs for Guided Sparse Depth Regression


0	  Generally, convolutional neural networks (CNNs) process data on a regular grid, e.g. data generated by ordinary cameras.
0	  Designing CNNs for sparse and irregularly spaced input data is still an open research problem with numerous applications in autonomous driving, robotics, and surveillance.
1	  In this paper, we propose an algebraically-constrained normalized convolution layer for CNNs with highly sparse input that has a smaller number of network parameters compared to related work.
1	  We propose novel strategies for determining the confidence from the convolution operation and propagating it to consecutive layers.
1	  We also propose an objective function that simultaneously minimizes the data error while maximizing the output confidence.
1	  To integrate structural information, we also investigate fusion strategies to combine depth and RGB information in our normalized convolution network framework.
1	  In addition, we introduce the use of output confidence as an auxiliary information to improve the results.
2	  The capabilities of our normalized convolution network framework are demonstrated for the problem of scene depth completion.
2	  Comprehensive experiments are performed on the KITTI-Depth and the NYU-Depth-v2 datasets.
2	  The results clearly demonstrate that the proposed approach achieves superior performance while requiring only about 1-5% of the number of parameters compared to the state-of-the-art methods.


### 94
##### 10.1109/TPAMI.2019.2929166
#### Multiset Feature Learning for Highly Imbalanced Data Classification


0	  With the expansion of data, increasing imbalanced data has emerged.
0	  When the imbalance ratio (IR) of data is high, most existing imbalanced learning methods decline seriously in classification performance.
1	  In this paper, we systematically investigate the highly imbalanced data classification problem, and propose an uncorrelated cost-sensitive multiset learning (UCML) approach for it.
1	  Specifically, UCML first constructs multiple balanced subsets through random partition, and then employs the multiset feature learning (MFL) to learn discriminant features from the constructed multiset.
1	  To enhance the usability of each subset and deal with the non-linearity issue existed in each subset, we further propose a deep metric based UCML (DM-UCML) approach.
1	  DM-UCML introduces the generative adversarial network technique into the multiset constructing process, such that each subset can own similar distribution with the original dataset.
1	  To cope with the non-linearity issue, DM-UCML integrates deep metric learning with MFL, such that more favorable performance can be achieved.
1	  In addition, DM-UCML designs a new discriminant term to enhance the discriminability of learned metrics.
2	  Experiments on eight traditional highly class-imbalanced datasets and two large-scale datasets indicate that: the proposed approaches outperform state-of-the-art highly imbalanced learning methods and are more robust to high IR.


### 95
##### 10.1109/TPAMI.2019.2929257
#### OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields


0	  Realtime multi-person 2D pose estimation is a key component in enabling machines to have an understanding of people in images and videos.
1	  In this work, we present a realtime approach to detect the 2D pose of multiple people in an image.
1	  The proposed method uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image.
2	  This bottom-up system achieves high accuracy and realtime performance, regardless of the number of people in the image.
0	  In previous work, PAFs and body part location estimation were refined simultaneously across training stages.
2	  We demonstrate that using a PAF-only refinement is able to achieve a substantial increase in both runtime performance and accuracy.
2	  We also present the first combined body and foot keypoint detector, based on an annotated foot dataset that we have publicly released.
2	  We show that the combined detector not only reduces the inference time compared to running them sequentially, but also maintains the accuracy of each component individually.
2	  This work has culminated in the release of OpenPose, the first open-source realtime system for multi-person 2D pose detection, including body, foot, hand, and facial keypoints.


### 96
##### 10.1109/TPAMI.2019.2929146
#### 3D Rigid Motion Segmentation with Mixed and Unknown Number of Models


0	  Many real-world video sequences cannot be conveniently categorized as general or degenerate; in such cases, imposing a false dichotomy in using the fundamental matrix or homography model for motion segmentation on video sequences would lead to difficulty.
0	  Even when we are confronted with a general scene-motion, the fundamental matrix approach as a model for motion segmentation still suffers from several defects, which we discuss in this paper.
0	  The full potential of the fundamental matrix approach could only be realized if we judiciously harness information from the simpler homography model.
1	  From these considerations, we propose a multi-model spectral clustering framework that synergistically combines multiple models (homography and fundamental matrix) together.
1	  We show that the performance can be substantially improved in this way.
0	  For general motion segmentation tasks, the number of independently moving objects is often unknown a priori and needs to be estimated from the observations.
0	  This is referred to as model selection and it is essentially still an open research problem.
1	  In this work, we propose a set of model selection criteria balancing data fidelity and model complexity.
2	  We perform extensive testing on existing motion segmentation datasets with both segmentation and model selection tasks, achieving state-of-the-art performance on all of them; we also put forth a more realistic and challenging dataset adapted from the KITTI benchmark, containing real-world effects such as strong perspectives and strong forward translations not seen in the traditional datasets.


### 97
##### 10.1109/TPAMI.2019.2929038
#### Learning with privileged information via adversarial discriminative modality distillation


0	  Heterogeneous data modalities can provide complementary cues for several tasks, usually leading to more robust algorithms and better performance.
0	  However, while training data can be accurately collected to include a variety of sensory modalities, it is often the case that not all of them are available in real life (testing) scenarios, where a model has to be deployed.
0	  This raises the challenge of how to extract information from multimodal data in the training stage, in a form that can be exploited at test time, considering limitations such as noisy or missing modalities.
1	  This paper presents a new approach in this direction for RGB-D vision tasks, developed within the adversarial learning and privileged information frameworks.
1	  We consider the practical case of learning representations from depth and RGB videos, while relying only on RGB data at test time.
1	  We propose a new approach to train a hallucination network that learns to distill depth information via adversarial learning, resulting in a clean approach without several losses to balance or hyperparameters.
2	  We report state-of-the-art results for object classification on the NYUD dataset, and video action recognition on the largest multimodal dataset available for this task, the NTU RGB+D, as well as on the Northwestern-UCLA.


### 98
##### 10.1109/TPAMI.2019.2929036
#### Pose-Guided Representation Learning for Person Re-Identification


0	  The large pose variations and misalignment errors exhibited by person images significantly increase the difficulty of person Re-Identification (ReID).
0	  Existing works commonly apply extra operations like pose estimation, part segmentation, etc., to alleviate those issues and improve the robustness of pedestrian representations.
0	  While boosting the ReID accuracy, those operations introduce considerable computational overheads and make the deep models complex and hard to tune.
1	  To chase a more efficient solution, we propose a Part-Guided Representation (PGR) composed of Pose Invariant Feature (PIF) and Local Descriptive Feature (LDF), respectively.
1	  We call PGR "Part-Guided" because it is trained and supervised by local part cues.
1	  Specifically, PIF approximates a pose invariant representation inferred by pose estimation and pose normalization.
1	  LDF focuses on discriminative body parts by approximating a representation learned with body region segmentation.
1	  In this way, extra pose extraction is only introduced during the training stage to supervise the learning of PGR, but is not required during the testing stage for feature extraction.
2	  Extensive comparisons with recent works on five widely used datasets demonstrate the competitive accuracy and efficiency of PGR.


### 99
##### 10.1109/TPAMI.2019.2929043
#### Robust Low-Rank Tensor Recovery with Rectification and Alignment


0	  Low-rank tensor recovery in the presence of sparse but arbitrary errors is an important problem with many practical applications.
1	  In this work, we propose a general framework that recovers low-rank tensors, in which the data can be deformed by some unknown transformations and corrupted by arbitrary sparse errors.
1	  We give a unified presentation of the surrogate-based formulations that incorporate the features of rectification and alignment simultaneously, and establish worst-case error bounds of the recovered tensor.
0	  In this context, the state-of-the-art methods ‘RASL’ and ‘TILT’ can be viewed as two special cases of our work, and yet each only performs part of the function of our method.
1	  Subsequently, we study the optimization aspects of the problem in detail by deriving two algorithms, one based on the alternating direction method of multipliers (ADMM) and the other based on proximal gradient.
1	  We provide convergence guarantees for the latter algorithm, and demonstrate the performance of the former through in-depth simulations.
2	  Finally, we present extensive experimental results on public datasets to demonstrate the effectiveness and efficiency of the proposed framework and algorithms.


### 100
##### 10.1109/TPAMI.2019.2927975
#### On Learning 3D Face Morphable Model from In-the-wild Images


0	  As a classic statistical model of 3D facial shape and albedo, 3D Morphable Model (3DMM) is widely used in facial analysis, e.g., model fitting, image synthesis.
0	  Conventional 3DMM is learned from a set of well-controlled 2D face images with associated 3D face scans, and represented by two sets of PCA basis functions.
0	  Due to the type and amount of training data, as well as, the linear bases, the representation power of 3DMM can be limited.
1	  To address these problems, this paper proposes an innovative framework to learn a nonlinear 3DMM model from a large set of unconstrained face images, without collecting 3D face scans.
1	  Specifically, given a face image as input, a network encoder estimates the projection, illumination, shape and albedo parameters.
1	  Two decoders serve as the nonlinear 3DMM to map from the shape and albedo parameters to the 3D shape and albedo, respectively.
1	  With the projection parameter, lighting, 3D shape, and albedo, a novel analytically-differentiable rendering layer is designed to reconstruct the original input face.
1	  The entire network is end-to-end trainable with only weak supervision.
2	  We demonstrate the superior representation power of our nonlinear 3DMM over its linear counterpart, and its contribution to face alignment and 3D reconstruction.


### 101
##### 10.1109/TPAMI.2019.2928294
#### Leader-based Multi-Scale Attention Deep Architecture for Person Re-identification


0	  Person Re-identification (re-id) aims to match people across non-overlapping camera views in a public space.
0	  It is a challenging problem because many people captured in surveillance videos often wear similar clothes.
0	  Consequently, the differences in their appearance are typically subtle and only detectable at the right locations and scales.
1	  In this paper, a deep re-id network is proposed consisting of two novel components: a multi-scale deep learning layer and a leader-based attention learning layer.
1	  With these components, our model is able to learn deep discriminative feature representations at different scales and automatically determine the optimal weightings for each scale when combining them.
1	  The importance of different spatial locations for extracting discriminative features is also learned explicitly via our leader-based attention module.
2	  Extensive experiments are carried out to demonstrate that the proposed model outperforms the state-of-the-art on a number of benchmarks, and has a better generalization ability under a domain generalization setting.


### 102
##### 10.1109/TPAMI.2019.2928296
#### DoubleFusion: Real-time Capture of Human Performances with Inner Body Shapes from a Single Depth Sensor


1	  We propose DoubleFusion, a new real-time system that combines volumetric non-rigid reconstruction with data-driven template fitting to simultaneously reconstruct detailed surface geometry, large non-rigid motion, and the optimized human body shape from a single depth camera.
1	  One of the key contributions of this method is a double-layer representation consisting of a complete parametric body model inside, and a gradually fused detailed surface outside.
2	  A pre-defined node graph on the body parameterizes the non-rigid deformations near the body, and a free-form dynamically changing graph parameterizes the outer surface layer far from the body, which allows more general reconstruction.
2	  We further propose a joint motion tracking method based on the double-layer representation to enable robust and fast motion tracking performance.
1	  Moreover, the inner parametric body is optimized online and forced to fit inside the outer surface layer as well as the live depth input.
2	  Overall, our method enables increasingly denoised, detailed and complete surface reconstructions, fast motion tracking performance and plausible inner body shape reconstruction in real-time.
2	  Experiments and comparisons show improved fast motion tracking and loop closure performance on more challenging scenarios.
2	  Two extended applications including body measurement and shape retargeting show the potential of our system in terms of practical use.


### 103
##### 10.1109/TPAMI.2019.2928540
#### Coherence Constrained Graph LSTM for Group Activity Recognition


0	  This work aims to address the group activity recognition problem by exploring human motion characteristics.
0	  Traditional methods hold that the motions of all persons contribute equally to the group activity, which suppresses the contributions of some relevant motions to the whole activity while overstates some irrelevant motions.
1	  To handle this problem, we present a Spatio-Temporal Context Coherence (STCC) constraint and a Global Context Coherence (GCC) constraint to capture the relevant motions and quantify their contributions to the group activity, respectively.
1	  Based on this, we propose a novel Coherence Constrained Graph LSTM (CCG-LSTM) with STCC and GCC to effectively recognize group activity, by modeling the relevant motions of individuals while suppressing the irrelevant motions.
1	  Specifically, to capture the relevant motions, we build the CCG-LSTM with a temporal confidence gate and a spatial confidence gate to control the memory state updating in terms of the temporally previous state and the spatially neighboring states, respectively.
1	  Besides, an attention mechanism is employed to quantify the contribution of a certain motion by measuring the consistency between itself and the whole activity at each time step.
2	  Finally, we conduct experiments on two widely-used datasets to illustrate the effectiveness of the proposed CCG-LSTM compared with the state-of-the-arts methods.


### 104
##### 10.1109/TPAMI.2019.2926728
#### Joint Task-Recursive Learning for RGB-D Scene Understanding


0	  RGB-D scene understanding under monocular camera is an emerging and challenging topic with many potential applications.
1	  In this paper, we propose a novel Task-Recursive Learning (TRL) framework to jointly and recurrently conduct three representative tasks therein containing depth estimation, surface normal prediction and semantic segmentation.
1	  TRL recursively refines the prediction results through a series of task-level interactions, where one-time cross-task interaction is abstracted as one network block of one time stage.
1	  In each stage, we serialize multiple tasks into a sequence and then recursively perform their interactions.
1	  To adaptively enhance counterpart patterns, we encapsulate interactions into a specific Task-Attentional Module (TAM) to mutually-boost the tasks from each other.
1	  Across stages, the historical experiences of previous states of tasks are selectively propagated into the next stages by using Feature-Selection unit (FS-Unit), which takes advantage of complementary information across tasks.
2	  The sequence of task-level interactions are also evolved along a coarse-to-fine scale space such that the required details may be refined progressively.
2	  Finally the task-abstracted sequence problem of multi-task prediction is framed into a recursive network.
2	  Extensive experiments on NYU-Depth v2 and SUN RGB-D datasets demonstrate that our method can recursively refines the results of the triple tasks and achieves state-of-the-art performance.


### 105
##### 10.1109/TPAMI.2019.2927909
#### A Microfacet-based Model for Photometric Stereo with General Isotropic Reflectance


1	  This paper presents a precise, stable, and invertible reflectance model for photometric stereo.
1	  This microfacet-based model is applicable to all types of isotropic surface reflectance, covering cases from diffusion to specular reflections.
1	  We introduce a single variable to physically quantify the surface smoothness, and by monotonically sliding this variable between 0 and 1, our model enables a versatile representation that can smoothly transform between an ellipsoid of revolution and the equation for Lambertian reflectance.
1	  In the inverse domain, this model offers a compact and physically interpretable formulation, for which we introduce a fast and lightweight solver that allows accurate estimations for both surface smoothness and surface shape.
2	  Finally, extensive experiments on the appearances of synthesized and real objects evidence that this model is state-of-the-art in our off-the-shelf solution.


### 106
##### 10.1109/TPAMI.2019.2927476
#### Recipe1M+: A Dataset for Learning Cross-Modal Embeddings for Cooking Recipes and Food Images


1	  In this paper, we introduce Recipe1M+, a new large-scale, structured corpus of over one million cooking recipes and 13 million food images.
1	  As the largest publicly available collection of recipe data, Recipe1M+ affords the ability to train high-capacity models on aligned, multi-modal data.
1	  Using these data, we train a neural network to learn a joint embedding of recipes and images that yields impressive results on an image-recipe retrieval task.
2	  Moreover, we demonstrate that regularization via the addition of a high-level classification objective both improves retrieval performance to rival that of humans and enables semantic vector arithmetic.
2	  We postulate that these embeddings will provide a basis for further exploration of the Recipe1M+ dataset and food and cooking in general.
#	  Code, data and models are publicly available.


### 107
##### 10.1109/TPAMI.2019.2927311
#### Blind Deblurring of Barcodes via Kullback-Leibler Divergence


0	  Barcode encoding schemes impose symbolic constraints which fix certain segments of the image.
1	  We present, implement, and assess a method for blind deblurring and denoising based entirely on Kullback-Leibler divergence.
1	  The method is designed to incorporate and exploit the full strength of barcode symbologies.
1	  Via both standard barcode reading software and smartphone apps, we demonstrate the remarkable ability of our method to blindly recover simulated images of highly blurred and noisy barcodes.
2	  As proof of concept, we present one application on a real-life out of focus camera image.


### 108
##### 10.1109/TPAMI.2019.2927203
#### Relative Saliency and Ranking: Models, Metrics, Data and Benchmarks


0	  Salient object detection is a problem that has been considered in detail and many solutions have been proposed.
0	  In this paper, we argue that work to date has addressed a problem that is relatively ill-posed.
0	  Specifically, there is not universal agreement about what constitutes a salient object when multiple observers are queried.
0	  This implies that some objects are more likely to be judged salient than others, and implies a relative rank exists on salient objects.
1	  Initially, we present a novel deep learning solution based on a hierarchical representation of relative saliency and stage-wise refinement.
1	  Further to this, we present data, analysis and baseline benchmark results towards addressing the problem of salient object ranking.
1	  Methods for deriving suitable ranked salient object instances are presented, along with metrics suitable to measuring algorithm performance.
1	  In addition, we show how a derived dataset can be successively refined to provide cleaned results that correlate well with pristine ground truth in its characteristics and value for training and testing models.
2	  Finally, we provide a comparison among prevailing algorithms that address salient object ranking or detection to establish initial baselines providing a basis for comparison with future efforts addressing this problem.
2	  The source code and data are publicly available via our project page: ryersonvisionlab.github.io/cocosalrank


### 109
##### 10.1109/TPAMI.2019.2926266
#### Variational Context: Exploiting Visual and Textual Context for Grounding Referring Expressions


0	  We focus on grounding (i.e., localizing or linking) referring expressions in images, e.g., "largest elephant standing behind baby elephant".
0	  This is a general yet challenging vision-language task since it does not only require the localization of objects, but also the multimodal comprehension of context - visual attributes (e.g., "largest", "baby") and relationships (e.g., "behind") that help to distinguish the referent from other objects, especially those of the same category.
0	  Due to the exponential complexity involved in modeling the context associated with multiple image regions, existing work oversimplifies this task to pairwise region modeling by multiple instance learning.
1	  In this paper, we propose a variational Bayesian method, called Variational Context, to solve the problem of complex context modeling in referring expression grounding.
2	  Our model exploits the reciprocal relation between the referent and context, i.e., either of them influences estimation of the posterior distribution of the other, and thereby the search space of context can be greatly reduced.
2	  In addition, our proposed Variational Context framework can automatically unify both referring expression comprehension and generation.Context-aware referring exrepssion generation helps to evaluate the estimated context and punish false gounding results in the unified framework.


### 110
##### 10.1109/TPAMI.2019.2926459
#### Local LDA: Open-Ended Learning of Latent Topics for 3D Object Recognition


0	  Service robots are expected to be more autonomous and work effectively in human-centric environments.
0	  This implies that robots should have special capabilities, such as learning from past experiences and real-time object category recognition.
1	  This paper proposes an open-ended 3D object recognition system which concurrently learns both the object categories and the statistical features for encoding objects.
1	  In particular, we propose an extension of Latent Dirichlet Allocation to learn structural semantic features (i.e. visual topics), from low-level feature co-occurrences, for each category independently.
1	  Moreover, topics in each category are discovered in an unsupervised fashion and are updated incrementally using new object views.
1	  In this way, the advantages of both the (hand-crafted) local features and the (learned) structural semantic features have been considered and combined in an efficient way.
2	  An extensive set of experiments has been performed to assess the performance of the proposed Local-LDA in terms of descriptiveness, scalability, and computation time.
2	  Experimental results show that the overall classification performance obtained with Local-LDA is clearly better than the best performances obtained with the state-of-the-art approaches.
2	  Moreover, the best scalability, in terms of number of learned categories, was obtained with the proposed Local-LDA approach, closely followed by a Bag-of-Words (BoW) approach.


### 111
##### 10.1109/TPAMI.2019.2926463
#### The ApolloScape Open Dataset for Autonomous Driving and its Application


0	  Autonomous driving has attracted tremendous attention especially in the past few years.
0	  The key techniques for a self-driving car include solving tasks like 3D map construction, self-localization, parsing the driving road and understanding objects, which enable vehicles to reason and act.
0	  However, large scale data set for training and system evaluation is still a bottleneck for developing robust perception models.
1	  In this paper, we present the ApolloScape dataset [1] and its applications for autonomous driving.
1	  Compared with existing public datasets from real scenes, e.g., KITTI [2] or Cityscapes [3], ApolloScape contains much large and richer labelling including holistic semantic dense point cloud for each site, stereo, per-pixel semantic labelling, lanemark labelling, instance segmentation, 3D car instance, high accurate location for every frame in various driving videos from multiple sites, cities and daytimes.
1	  For each task, it contains at lease 15x larger amount of images than SOTA datasets.
1	  To label such a complete dataset, we develop various tools and algorithms specified for each task to accelerate the labelling process, such as joint 3D-2D segment labeling, active labelling in videos etc.
1	  Depend on ApolloScape, we are able to develop algorithms jointly consider the learning and inference of multiple tasks.
1	  In this paper, we provide a sensor fusion scheme integrating camera videos, consumer-grade motion sensors (GPS/IMU), and a 3D semantic map in order to achieve robust self-localization and semantic segmentation for autonomous driving.
2	  We show that practically, sensor fusion and joint learning of multiple tasks are beneficial to achieve a more robust and accurate system.
2	  We expect our dataset and proposed relevant algorithms can support and motivate researchers for further development of multi-sensor fusion and multi-task learning in the field of computer vision.


### 112
##### 10.1109/TPAMI.2019.2926357
#### Simultaneous Fidelity and Regularization Learning for Image Restoration


0	  Most existing non-blind restoration methods are based on the assumption that a precise degradation model is known.
0	  As the degradation process can only partially known or inaccurately modeled, images may not be well restored.
0	  For rain streak removal, although an input image can be decomposed into a scene layer and a rain streak layer, there exists no explicit formulation for modeling rain streaks and the composition with scene layer.
0	  For blind deconvolution, as estimation error of blur kernel is usually introduced, the subsequent non-blind deconvolution process does not restore the latent image well.
1	  We propose a principled algorithm within the maximum a posterior framework to tackle image restoration with a partially known or inaccurate degradation model.
1	  The residual caused by a partially known or inaccurate degradation model is spatially dependent and complexly distributed.
1	  With a training set of degraded and ground-truth image pairs, we parameterize and learn the fidelity term for a degradation model in a task-driven manner.
1	  The regularization term can also be learned along with the fidelity term, thereby forming a simultaneous fidelity and regularization learning model.
2	  Extensive experimental results demonstrate the effectiveness of the proposed model for image deconvolution with inaccurate blur kernels and rain streak removal.


### 113
##### 10.1109/TPAMI.2019.2926033
#### Community Detection Using Restrained Random-walk Similarity


1	  In this paper, we propose a restrained random-walk similarity method for detecting the community structures of graphs.
1	  The basic premise of our method is that the starting vertices of finite-length random walks are judged to be in the same community if the walkers pass similar sets of vertices.
1	  This idea is based on our consideration that a random walker tends to move in the community including the walker's starting vertex for some time after starting the walk.
1	  Therefore, the sets of vertices passed by random walkers starting from vertices in the same community must be similar.
1	  The idea is reinforced with two conditions.
1	  First, we exclude abnormal random walks.
1	  Random walks that depart from each vertex are executed many times, and vertices that are rarely passed by the walkers are excluded from the set of vertices that the walkers may pass.
1	  Second, we forcibly restrain random walks to an appropriate length.
1	  In our method, a random walk is terminated when the walker repeatedly visits vertices that they have already passed.
2	  Experiments on real-world networks demonstrate that our method outperforms previous techniques in terms of accuracy.


### 114
##### 10.1109/TPAMI.2019.2919824
#### Providing a Single Ground-truth for Illuminant Estimation for the ColorChecker Dataset


0	  The ColorChecker dataset is one of the most widely used image sets for evaluating and ranking illuminant estimation algorithms.
0	  However, this single set of images has at least 3 different sets of ground-truth (i.e.  correct answers) associated with it.
0	  In the literature it is often asserted that one algorithm is better than another when the algorithms in question have been tuned and tested with the different ground-truths.
1	  In this short correspondence we present some of the background as to why the 3 existing ground-truths are different and go on to make a new single and recommended set of correct answers.
2	  Experiments reinforce the importance of this work in that we show that the total ordering of a set of algorithms may be reversed depending on whether we use the new or legacy ground-truth data.


### 115
##### 10.1109/TPAMI.2019.2925793
#### A General Decoupled Learning Framework for Parameterized Image Operators


0	  Many different deep networks have been used to approximate, accelerate or improve traditional image operators.
0	  Among these traditional operators, many contain parameters which need to be tweaked to obtain the satisfactory results, which we refer to as "parameterized image operators".
0	  However, most existing deep networks trained for these operators are only designed for one specific parameter configuration, which does not meet the needs of real scenarios that usually require flexible parameters settings.
1	  To overcome this limitation, we propose a new decoupled learning algorithm to learn from the operator parameters to dynamically adjust the weights of a deep network for image operators, denoted as the base network.
1	  The learned algorithm is formed as another network, namely the weight learning network, which can be end-to-end jointly trained with the base network.
2	  Experiments demonstrate that the proposed framework can be successfully applied to many traditional parameterized image operators.
1	  To accelerate the parameter tuning for practical scenarios, the proposed framework can be further extended to dynamically change the weights of only one single layer of the base network while sharing most computation cost.
2	  We demonstrate that this cheap parameter-tuning extension of the proposed decoupled learning framework even outperforms the state-of-the-art alternative approaches.


### 116
##### 10.1109/TPAMI.2019.2924953
#### Border-Peeling Clustering


1	  In this paper, we present a novel non-parametric clustering technique.
1	  Our technique is based on the notion that each latent cluster is comprised of layers that surround its core, where the external layers, or border points, implicitly separate the clusters.
1	  Unlike previous techniques, such as DBSCAN, where the cores of the clusters are defined directly by their densities, here the latent cores are revealed by a progressive peeling of the border points.
1	  Analyzing the density of the local neighborhoods allows identifying the border points and associating them with points of inner layers.
2	  We show that the peeling process adapts to the local densities and characteristics to successfully separates adjacent clusters (of possibly different densities).
1	  We extensively tested our technique on large sets of labeled data, including high-dimensional datasets of deep features that were trained by a convolutional neural network.
2	  We show that our technique is competitive to other state-of-the-art non-parametric methods, using a fixed set of parameters throughout the experiments.


### 117
##### 10.1109/TPAMI.2019.2925347
#### Asymmetric Mapping Quantization for Nearest Neighbor Search


0	  Nearest neighbor search is a fundamental problem in computer vision and machine learning.
0	  The straightforward solution, linear scan, is both computationally and memory intensive in large scale high-dimensional cases, hence is not preferable in practice.
0	  Therefore, there have been a lot of interests in algorithms that perform approximate nearest neighbor (ANN) search.
1	  In this paper, we propose a novel addition-based vector quantization algorithm, Asymmetric Mapping Quantization (AMQ), to efficiently conduct ANN search.
1	  Unlike existing addition-based quantization methods that suffer from handling the problem caused by the norm of database vector, we map the query vector and database vector using different mapping functions to transform the computation of L-2 distance to inner product similarity, thus do not need to evaluate the norm of database vector.
2	  Moreover, we further propose Distributed Asymmetric Mapping Quantization (DAMQ) to enable AMQ to work on very large dataset by distributed learning.
2	  Extensive experiments on approximate nearest neighbor search and image retrieval validate the merits of the proposed AMQ and DAMQ.


### 118
##### 10.1109/TPAMI.2019.2924417
#### Revisiting Video Saliency Prediction in the Deep Learning Era


0	  Predicting where people look in static scenes, a.k.a visual saliency, has received significant research interests recently.
0	  However, relatively less effort has been spent in understanding visual attention over dynamic scenes.
0	  This work makes three contributions to video saliency research.
1	  First, we introduce a new benchmark, called DHF1K, for predicting fixations during dynamic scene free-viewing, which is a long-time need in this field.
1	  DHF1K consists of 1K high-quality, elaborately selected videos annotated by 17 observers using an eye tracker.
1	  The videos span a wide range of scenes, motions, object types and backgrounds.
1	  Second, we propose a novel video saliency model, called ACLNet, that augments the CNN-LSTM network with a supervised attention mechanism to enable fast end-to-end learning.
1	  The attention mechanism explicitly encodes static saliency information, thus allowing LSTM to focus on learning a more flexible temporal saliency representation.
1	  Such a design leverages existing large-scale static fixation datasets, avoids overfitting, and significantly improves training efficiency and testing performance.
1	  Third, we perform an extensive evaluation of state-of-the-art saliency models on three current datasets (i.e., DHF1K, Hollywood2, UCF sports).
2	  Experimental results over more than 1.2K testing videos containing 400K frames demonstrate that ACLNet outperforms other contenders and has a fast processing speed (40fps).


### 119
##### 10.1109/TPAMI.2019.2924428
#### A Framework of Composite Functional Gradient Methods for Generative Adversarial Models


0	  Generative adversarial networks (GAN) are trained through a minimax game between a generator and a discriminator to generate data that mimics observations.
0	  While being widely used, GAN training is known to be empirically unstable.
1	  This paper presents a new theory for generative adversarial methods that does not rely on the traditional minimax formulation.
1	  Our theory shows that with a strong discriminator, a good generator can be obtained by composite functional gradient learning, so that several distance measures (including the KL divergence and the JS divergence) between the probability distributions of real data and generated data are simultaneously improved after each functional gradient step until converging to zero.
2	  This new point of view leads to stable procedures for training generative models.
2	  It also gives a new theoretical insight into the original GAN.
2	  Empirical results on image generation show the effectiveness of our new method.


### 120
##### 10.1109/TPAMI.2019.2923998
#### Ranking-Preserving Cross-Source Learning for Image Retargeting Quality Assessment


0	  Image retargeting techniques adjust images into different sizes and have attracted much attention recently.
0	  Objective quality assessment (OQA) of image retargeting results is often desired to automatically select the best results.
0	  Existing OQA methods train a model using some benchmarks (e.g., RetargetMe), in which subjective scores evaluated by users are provided.
1	  Observing that it is challenging even for human subjects to give consistent scores for retargeting results of different source images (diff-source-results), in this paper we propose a learning-based OQA method that trains a General Regression Neural Network (GRNN) model based on relative scores --- which preserve the ranking --- of retargeting results of the same source image (same-source-results).
1	  In particular, we develop a novel training scheme with provable convergence that learns a common base scalar for same-source-results.
1	  With this source specific offset, our computed scores not only preserve the ranking of subjective scores for same-source-results, but also provide a reference to compare the diff-source-results.
1	  We train and evaluate our GRNN model using human preference data collected in RetargetMe.
2	  Moreover, we introduce a further subjective benchmark to evaluate the generalizability of different OQA methods.
2	  Experimental results demonstrate that our method outperforms ten representative OQA methods in ranking prediction.


### 121
##### 10.1109/TPAMI.2019.2923240
#### Bayesian Low-Tubal-Rank Robust Tensor Factorization with Multi-Rank Determination


0	  Robust tensor factorization is a fundamental problem in machine learning and computer vision, which aims at recovering tensors corrupted with outliers as a sum of the low-rank and sparse components.
0	  However, existing methods either suffer from limited modeling power in preserving low-rank structures, or have difficulties in determining the target tensor rank and the trade-off between the low-rank and sparse components.
1	  To address these problems, we propose a fully Bayesian treatment of robust tensor factorization along with a generalized sparsity-inducing prior.
1	  By adapting the recently proposed low-tubal-rank model in a generative manner, our method is effective in preserving low-rank structures.
1	  Moreover, benefiting from the proposed prior and the Bayesian framework, the proposed method can automatically determine the tensor rank while inferring the trade-off between the low-rank and sparse components.
1	  For model estimation, we develop a variational inference algorithm, and further improve its efficiency by reformulating the variational updates in the frequency domain.
2	  Experimental results on both synthetic and real-world datasets demonstrate the effectiveness of the proposed method in multi-rank determination as well as its superiority in image denoising and background modeling over state-of-the-art approaches.


### 122
##### 10.1109/TPAMI.2019.2923513
#### Zig-Zag Network for Semantic Segmentation of RGB-D Images


0	  Semantic segmentation of images requires an understanding of appearances of objects and their spatial relationships in scenes.
0	  The fully convolutional network (FCN) has been successfully applied to recognize objects' appearances, which are represented with RGB channels.
0	  Images augmented with depth channels provide more understanding of the geometric information of the scene in an image.
1	  In this paper, we present a multiple-branch neural network to utilize depth information to assist in the semantic segmentation of images.
1	  Our approach splits the image into layers according to the "scene-scale".
1	  We introduce the context-aware receptive field (CARF), which provides better control of the relevant context information of learned features.
1	  Each branch of the network is equipped with CARF to adaptively aggregate the context information of image regions, leading to a more focused domain that is easier to learn.
1	  Furthermore, we propose a new zig-zag architecture to exchange information between the feature maps at different levels, augmented by the CARFs of the backbone network and decoder network.
1	  With the flexible information propagation allowed by our zig-zag network, we enrich the context information of feature maps for the segmentation.
2	  We show that the zig-zag network achieves state-of-the-art performances on several public datasets.


### 123
##### 10.1109/TPAMI.2019.2923621
#### Photometric Depth Super-Resolution


1	  This study explores the use of photometric techniques (shape-from-shading and uncalibrated photometric stereo) for upsampling the low-resolution depth map from an RGB-D sensor to the higher resolution of the companion RGB image.
1	  A single-shot variational approach is first put forward, which is effective as long as the target's reflectance is piecewise-constant.
1	  It is then shown that this dependency upon a specific reflectance model can be relaxed by focusing on a specific class of objects (e.g., faces), and delegate reflectance estimation to a deep neural network.
2	  A multi-shot strategy based on randomly varying lighting conditions is eventually discussed.
2	  It requires no training or prior on the reflectance, yet this comes at the price of a dedicated acquisition setup.
2	  Both quantitative and qualitative evaluations illustrate the effectiveness of the proposed methods on synthetic and real-world scenarios.


### 124
##### 10.1109/TPAMI.2019.2923201
#### Clouds of Oriented Gradients for 3D Detection of Objects, Surfaces, and Indoor Scene Layouts


0	  We develop new representations and algorithms for three-dimensional (3D) object detection and spatial layout prediction in cluttered indoor scenes.
1	  We first propose a clouds of oriented gradient (COG) descriptor that links the 2D appearance and 3D pose of object categories, and thus accurately models how perspective projection affects perceived image gradients.
1	  To better represent the 3D visual styles of large objects and provide contextual cues to improve the detection of small objects, we introduce latent support surfaces.
1	  We then propose a "Manhattan voxel" representation which better captures the 3D room layout geometry of common indoor environments.
1	  Effective classification rules are learned via a latent structured prediction framework.
2	  Contextual relationships among categories and layout are captured via a cascade of classifiers, leading to holistic scene hypotheses that exceed the state-of-the-art on the SUN RGB-D database.


### 125
##### 10.1109/TPAMI.2019.2922640
#### RotationNet for Joint Object Categorization and Unsupervised Pose Estimation from Multi-view Images


1	  We propose a Convolutional Neural Network (CNN)-based model "RotationNet," which takes multi-view images of an object as input and jointly estimates its pose and object category.
1	  Unlike previous approaches that use known viewpoint labels for training, our method treats the viewpoint labels as latent variables, which are learned in an unsupervised manner during the training using an unaligned object dataset.
1	  RotationNet uses only a partial set of multi-view images for inference, and this property makes it useful in practical scenarios where only partial views are available.
1	  Moreover, our pose alignment strategy enables one to obtain view-specific feature representations shared across classes, which is important to maintain high accuracy in both object categorization and pose estimation.
2	  Effectiveness of RotationNet is demonstrated by its superior performance to the state-of-the-art methods of 3D object classification on 10- and 40-class ModelNet datasets.
2	  We also show that RotationNet, even trained without known poses, achieves comparable performance to the state-of-the-art methods on an object pose estimation dataset.
2	  Furthermore, our object ranking method based on classification by RotationNet achieved the first prize in two tracks of the 3D Shape Retrieval Contest (SHREC) 2017.
2	  Finally, we demonstrate the performance of real-world applications of RotationNet trained with our newly created multi-view image dataset using a moving USB camera.


### 126
##### 10.1109/TPAMI.2019.2922959
#### One shot segmentation: unifying rigid detection and non-rigid segmentation using elastic regularization


0	  This paper proposes a novel approach for the non-rigid segmentation of deformable objects in image sequences, which is based on one-shot segmentation that unifies rigid detection and non-rigid segmentation using elastic regularization.
0	  The domain of application is the segmentation of a visual object that temporally undergoes a rigid transformation (e.g., affine transformation) and a non-rigid transformation (i.e., contour deformation).
0	  The majority of segmentation approaches to solve this problem are generally based on two steps that run in sequence: a rigid detection, followed by a non-rigid segmentation.
1	  In this paper, we propose a new approach, where both the rigid and non-rigid segmentation are performed in a single shot using a sparse low-dimensional manifold that represents the visual object deformations.
1	  Given the multi-modality of these deformations, the manifold partitions the training data into several patches, where each patch provides a segmentation proposal during the inference process.
1	  These multiple segmentation proposals are merged using the classification results produced by deep belief networks (DBN) that compute the confidence on each segmentation proposal.
1	  Thus, an ensemble of DBN classifiers is used for estimating the final segmentation.
2	  Compared to current methods proposed in the field, our proposed approach is advantageous in four aspects: 
2	  (i) it is a unified framework to produce rigid and non-rigid segmentations; 
2	  (ii) it uses an ensemble classification process, which can help the segmentation robustness; 
2	  (iii) it provides a significant reduction in terms of the number of dimensions of the rigid and non-rigid segmentations search spaces, compared to current approaches that divide these two problems; and 
2	  (iv) this lower dimensionality of the search space can also reduce the need for large annotated training sets to be used for estimating the DBN models.
2	  Experiments on the problem of left ventricle endocardial segmentation from ultrasound images, and lip segmentation from frontal facial images using the extended Cohn-Kanade (CK+) database, demonstrate the potential of the methodology through qualitative and quantitative evaluations, and the ability to reduce the search and training complexities without a significant impact on the segmentation accuracy.


### 127
##### 10.1109/TPAMI.2019.2922396
#### Towards Safe Weakly Supervised Learning


0	  In this paper, we study weakly supervised learning where a large amount of label information is not accessible.
0	  This includes incomplete supervision such as semi-supervised learning and domain adaptation; inexact supervision, such as multi-instance learning and inaccurate supervision, such as label noise learning.
0	  Unlike supervised learning, weakly supervised learning, however, may sometimes even degenerate performance.
0	  Such deficiency definitely hinders the deployment of weakly supervised learning to real applications.
0	  For this reason, it is desired to study safe weakly supervised learning.
1	  In this paper we present a generic ensemble learning scheme to derive the safe prediction.
1	  We consider optimizing the worst-case performance gain which leads to a maximin optimization.
2	  Our resultant formulation brings multiple advantages.
2	  Firstly, for many commonly used convex loss functions in classification and regression tasks, our formulation is guaranteed to derive a safe prediction under a mild condition.
2	  Secondly, prior knowledge related to the weight of the base weakly supervised learners can be flexibly embedded.
2	  Thirdly, our formulation can be globally and efficiently addressed.
2	  Finally, it is in an intuitive geometric interpretation.
2	  Extensive experiments on multiple weakly supervised learning tasks clearly demonstrate the effectiveness of our proposal algorithms.


### 128
##### 10.1109/TPAMI.2019.2922175
#### Vocabulary-informed Zero-shot and Open-set Learning


0	  Despite significant progress in object categorization, in recent years, a number of important challenges remain; mainly, ability to learn from limited labeled data and ability to recognize object classes within large, potentially open, set of labels.
0	  Zero-shot learning is one way of addressing these challenges, but it has only been shown to work with limited sized class vocabularies and typically requires separation between supervised and unsupervised classes, allowing former to inform the latter but not vice versa.
1	  We propose the notion of vocabulary-informed learning to alleviate the above mentioned challenges and address problems of supervised, zero-shot, generalized zero-shot and open set recognition using a unified framework.
1	  Specifically, we propose a weighted maximum margin framework for semantic manifold-based recognition that incorporates distance constraints from (both supervised and unsupervised) vocabulary atoms.
1	  Distance constraints ensure that labeled samples are projected closer to their correct prototypes, in the embedding space, than to others.
2	  We illustrate that resulting model shows improvements in supervised, zero-shot, generalized zero-shot, and large open set recognition, with up to 310K class vocabulary on AwA and ImageNet datasets.


### 129
##### 10.1109/TPAMI.2019.2922181
#### Object Detection from Scratch with Deep Supervision


0	  We propose Deeply Supervised Object Detectors (DSOD), an object detection framework that can be trained from scratch.
0	  Recent advances in object detection heavily depend on the off-the-shelf models pre-trained on large-scale classification datasets like ImageNet and OpenImage.
0	  However, one problem is that adopting pre-trained models from classification to detection task may incur learning bias due to the different objective function and diverse distributions of object categories.
0	  Techniques like fine-tuning on detection task could alleviate this issue to some extent but are still not fundamental.
0	  Furthermore, transferring these pre-trained models cross discrepant domains will be more difficult (e.g., from RGB to depth images).
0	  Thus, a better solution to handle these critical problems is to train object detectors from scratch, which motivates our proposed method.
1	  In DSOD, we contribute a set of design principles for learning object detectors from scratch.
1	  One of the key principles is the deep supervision, enabled by layer-wise dense connections in both backbone networks and prediction layers, plays a critical role in learning good detectors from scratch.
2	  We evaluate our method on PASCAL VOC 2007, 2012 and COCO datasets.
2	  DSOD achieves consistently better results than the state-of-the-art methods with much more compact models.


### 130
##### 10.1109/TPAMI.2019.2921960
#### Semantic Fisher Scores for Task Transfer: Using Objects to Classify Scenes


1	  The tranfer of a neural network (CNN) trained to recognize objects to the task of scene classification is considered.
1	  A Bag-of-Semantics (BoS) representation is first induced, by feeding scene image patches to the object CNN, and representing the scene image by the ensuing bag of posterior class probability vectors (semantic posteriors).
1	  The encoding of the BoS with a Fisher vector (FV) is then studied.
1	  A link is established between the FV of any probabilistic model and the Q-function of the expectation-maximization (EM) algorithm used to estimate its parameters by maximum likelihood.
1	  This enables 1) imediate derivation of FVs for any model for which an EM algorithm exists, and 2) leveraging efficient implementations from the EM literature for the computation of FVs.
2	  It is then shown that standard FVs, such as those derived from Gaussian or even Dirichelet mixtures, are unsuccessful for the transfer of semantic posteriors, due to the highly non-linear nature of the probability simplex.
2	  The analysis of these FVs shows that significant benefits can ensue by 1) designing FVs in the natural parameter space of the multinomial distribution, and 2) adopting sophisticated probabilistic models of semantic feature covariance.
2	  The combination of these two insights leads to the encoding of the BoS in the natural parameter space of the multinomial, using a vector of Fisher scores derived from a mixture of factor analyzers (MFA).
1	  A network implementation of the MFA Fisher Score (MFA-FS), denoted as the MFAFSNet, is finally proposed to enable end-to-end training.
2	  Experiments with various object CNNs and datasets show that the approach has state-of-the-art transfer performance.
2	  Somewhat surprisingly, the scene classification results are superior to those of a CNN explicitly trained for scene classification, using a large scene dataset (Places).
2	  This suggests that holistic analysis is insufficient for scene classification.
2	  The modeling of local object semantics appears to be at least equally important.
2	  The two approaches are also shown to be strongly complementary, leading to very large scene classification gains when combined, and outperforming all previous scene classification approaches by a sizeable margin.


### 131
##### 10.1109/TPAMI.2019.2921539
#### Two-Stream Region Convolutional 3D Network for Temporal Activity Detection


0	  We address the problem of temporal activity detection in continuous, untrimmed video streams.
0	  This is a difficult task that requires extracting meaningful spatio-temporal features to capture activities, accurately localizing the start and end times of each activity.
1	  We introduce a new model, Region Convolutional 3D Network (R-C3D), which encodes the video streams using a three-dimensional fully convolutional network, then generates candidate temporal regions containing activities and finally classifies selected regions into specific activities.
1	  Computation is saved due to the sharing of convolutional features between the proposal and the classification pipelines.
1	  We further improve the detection performance by efficiently integrating an optical flow based motion stream with the original RGB stream.
1	  The two-stream network is jointly optimized by fusing the flow and RGB feature maps at different levels.
1	  Additionally, the training stage incorporates an online hard example mining strategy to address the extreme foreground-background imbalance typically observed in any detection pipeline.
1	  Instead of heuristically sampling the candidate segments for the final activity classification stage, we rank them according to their performance and only select the worst performers to update the model.
1	  This improves the model without heavy hyper-parameter tuning.
2	  Extensive experiments on three benchmark datasets are carried out to show superior performance over existing temporal activity detection methods.
2	  Our model achieves state-of-the-art results on the THUMOS'14 and Charades datasets.
2	  We further demonstrate that our model is a general temporal activity detection framework that does not rely on assumptions about particular dataset properties by evaluating our approach on the ActivityNet dataset.


### 132
##### 10.1109/TPAMI.2019.2921548
#### Real-world Image Denoising with Deep Boosting


1	  We propose a Deep Boosting Framework (DBF) for real-world image denoising by integrating the deep learning technique into the boosting algorithm.
1	  The DBF replaces conventional handcrafted boosting units by elaborate convolutional neural networks, which brings notable advantages in terms of both performance and speed.
1	  We design a lightweight Dense Dilated Fusion Network (DDFN) as an embodiment of the boosting unit, which addresses the vanishing of gradients during training due to the cascading of networks while promoting the efficiency of limited parameters.
1	  The capabilities of the proposed method are first validated on several representative simulation tasks including non-blind and blind Gaussian denoising and JPEG image deblocking.
1	  We then focus on a practical scenario to tackle with the complex and challenging real-world noise.
1	  To facilitate leaning-based methods including ours, we build a new Real-world Image Denoising (RID) dataset, which contains 200 pairs of high-resolution images with diverse scene content under various shooting conditions.
1	  Moreover, we conduct comprehensive analysis on the domain shift issue for real-world denoising and propose an effective one-shot domain transfer scheme to address this issue.
2	  Comprehensive experiments on widely used benchmarks demonstrate that the proposed method significantly surpasses existing methods on the task of real-world image denoising.


### 133
##### 10.1109/TPAMI.2019.2921543
#### Guided Attention Inference Network


0	  With only coarse labels, weakly supervised learning typically uses top-down attention maps generated by back-propagating gradients as priors for tasks such as object localization and semantic segmentation.
0	  While these attention maps are intuitive and informative explanations of deep neural network, there is no effective mechanism to manipulate the network attention during learning process.
1	  In this paper, we address three shortcomings of previous approaches in modeling such attention maps in one common framework.
1	  First, we make attention maps a natural and explicit component in the training pipeline such that they are end-to-end trainable.
1	  Moreover, we provide self-guidance directly on these maps by exploring supervision from the network itself to improve them towards specific target tasks.
1	  Lastly, we proposed a design to seamlessly bridge the gap between using weak and extra supervision if available.
2	  Despite its simplicity, experiments on the semantic segmentation task demonstrate the effectiveness of our methods.
2	  Besides, the proposed framework provides a way not only explaining the focus of the learner but also feeding back with direct guidance towards specific tasks.
2	  Under mild assumptions our method can also be understood as a plug-in to existing convolutional neural networks to improve their generalization performance.


### 134
##### 10.1109/TPAMI.2019.2921574
#### CoRRN: Cooperative Reflection Removal Network


0	  Removing the undesired reflections from images taken through the glass is of broad application to various computer vision tasks.
0	  Non-learning based methods utilize different handcrafted priors such as the separable sparse gradients caused by different levels of blurs, which often fail due to their limited description capability to the properties of real-world reflections.
1	  In this paper, we propose a network with the feature-sharing strategy to tackle this problem in a cooperative and unified framework, by integrating image context information and the multi-scale gradient information.
1	  To remove the strong reflections existed in some local regions, we propose a statistic loss by considering the gradient level statistics between the background and reflections.
1	  Our network is trained on a new dataset with 3250 reflection images taken under diverse real-world scenes.
2	  Experiments on a public benchmark dataset show that the proposed method performs favorably against state-of-the-art methods.


### 135
##### 10.1109/TPAMI.2019.2921327
#### Joint Segmentation and Path Classification of Curvilinear Structures


0	  Detection of curvilinear structures in images has long been of interest.
0	  One of the most challenging aspects of this problem is inferring the graph representation of the curvilinear network.
0	  Most existing delineation approaches first perform binary segmentation of the image and then refine it using either a set of hand-designed heuristics or a separate classifier that assigns likelihood to paths extracted from the pixel-wise prediction.
1	  In our work, we bridge the gap between segmentation and path classification by training a deep network that performs those two tasks simultaneously.
2	  We show that this approach is beneficial because it enforces consistency across the whole processing pipeline.
1	  We apply our approach on roads and neurons datasets.


### 136
##### 10.1109/TPAMI.2019.2917908
#### Learning and Tracking the 3D Body Shape of Freely Moving Infants from RGB-D sequences


0	  Statistical models of the human body surface are generally learned from thousands of high-quality 3D scans in predefined poses to cover the wide variety of human body shapes and articulations.
0	  Acquisition of such data requires expensive equipment, calibration procedures, and is limited to cooperative subjects who can understand and follow instructions, such as adults.
1	  We present a method for learning a statistical 3D Skinned Multi-Infant Linear body model (SMIL) from incomplete, low-quality RGB-D sequences of freely moving infants.
2	  Quantitative experiments show that SMIL faithfully represents the RGB-D data and properly factorizes the shape and pose of the infants.
2	  To demonstrate the applicability of SMIL, we fit the model to RGB-D sequences of freely moving infants and show, with a case study, that our method captures enough motion detail for General Movements Assessment (GMA), a method used in clinical practice for early detection of neurodevelopmental disorders in infants.
2	  SMIL provides a new tool for analyzing infant shape and movement and is a step towards an automated system for GMA.


### 137
##### 10.1109/TPAMI.2019.2921031
#### Trace Quotient with Sparsity Priors for Learning Low Dimensional Image Representations


0	  This work studies the problem of learning appropriate low dimensional image representations.
1	  We propose a generic algorithmic framework, which leverages two classic representation learning paradigms, i.e., sparse representation and the trace quotient criterion, to disentangle underlying factors of variation in high dimensional images.
1	  Specifically, we aim to learn simple representations of low dimensional, discriminant factors by applying the trace quotient criterion to well-engineered sparse representations.
1	  We construct a unified cost function, coined as the SPARse LOW dimensional representation (SparLow) function, for jointly learning both a sparsifying dictionary and a dimensionality reduction transformation.
0	  The SparLow function is widely applicable for developing various algorithms in three classic machine learning scenarios, namely, unsupervised, supervised, and semi-supervised learning.
1	  In order to develop efficient joint learning algorithms for maximizing the SparLow function, we deploy a framework of sparse coding with appropriate convex priors to ensure the sparse representations to be locally differentiable.
2	  Moreover, we develop an efficient geometric conjugate gradient algorithm to maximize the SparLow function on its underlying Riemannian manifold.
2	  Performance of the proposed SparLow algorithmic framework is investigated on several image processing tasks, such as 3D data visualization, face/digit recognition, and object/scene categorization.


### 138
##### 10.1109/TPAMI.2019.2920899
#### Reconstruct and Represent Video Contents for Captioning via Reinforcement Learning


0	  In this paper, the problem of describing visual contents of a video sequence with natural language is addressed.
1	  Unlike previous video captioning work mainly exploiting the cues of video contents to make a description, we propose a reconstruction network (RecNet) in a novel encoder-decoder-reconstructor architecture, which leverages both forward (video to sentence) and backward (sentence to video) flows for video captioning.
1	  Specifically, the encoder-decoder component makes use of the forward flow to produce a sentence description based on the encoded video semantic features.
1	  Two types of reconstructors are subsequently proposed to employ backward flow and reproduce the video features from local and global perspectives, respectively, capitalizing on the hidden state sequence generated by the decoder.
1	  Moreover, in order to make a comprehensive reconstruction of the video features, we propose to fuse the two types of reconstructors together.
1	  The generation loss yielded by the encoder-decoder component and the reconstruction loss introduced by the reconstructor are jointly cast into training the proposed RecNet in an end-to-end fashion.
1	  Additionally, the RecNet is finetuned by CIDEr optimization via reinforcement learning, which significantly boosts the captioning performance.
2	  Experimental results on benchmark datasets demonstrate that the proposed reconstructor can boost the performance of video captioning consistently.


### 139
##### 10.1109/TPAMI.2019.2920821
#### Local Deformable 3D Reconstruction with Cartan's Connections


0	  3D reconstruction of deformable objects using inter-image visual motion from monocular images has been studied under Shape-from- Template (SfT) and Non-Rigid Structure-from-Motion (NRSfM).
0	  Most methods have been developed for simple deformation models, primarily isometry.
0	  They may treat a surface as a discrete set of points and draw constraints from the points only or they may use a non- parametric representation and use both points and differentials to express constraints.
1	  We propose a differential framework based on Cartan's theory of connections and moving frames.
1	  It is applicable to SfT and NRSfM, and to deformation models other than isometry.
1	  It utilises infinitesimal-level assumptions on the surface's geometry and mappings.
1	  It has the following properties.
1	  1) It allows one to derive existing solutions in a simpler way.
1	  2) It models SfT and NRSfM in a unified way.
1	  3) It allows us to introduce a new skewless deformation model and solve SfT and NRSfM for it.
1	  4) It facilitates a generic solution to SfT which does not require deformation modeling.
1	  Our framework is complete: it solves deformable 3D reconstruction for a whole class of algebraic deformation models including isometry.
2	  We compared our solutions with the state-of-the-art methods and show that ours outperform in terms of both accuracy and computation time.


### 140
##### 10.1109/TPAMI.2019.2920636
#### Gravitational Laws of Focus of Attention


0	  The understanding of the mechanisms behind focus of attention in a visual scene is a problem of great interest in visual perception and computer vision.
1	  In this paper, we describe a model of scanpath as a dynamic process which can be interpreted as a variational law somehow related to mechanics, where the focus of attention is subject to a gravitational field.
1	  The distributed virtual mass that drives eye movements is associated with the presence of details and motion in the video.
1	  Unlike most current models, the proposed approach does not estimate directly the saliency map, but the prediction of eye movements allows us to integrate over time the positions of interest.
1	  The process of inhibition-of-return is also supported in the same dynamic model with the purpose of simulating fixations and saccades.
1	  The differential equations of motion of the proposed model are numerically integrated to simulate scanpaths on both images and videos.
2	  Experimental results for the tasks of saliency and scanpath prediction on a wide collection of datasets are presented to support the theory.
2	  Top level performances are achieved especially in the prediction of scanpaths, which is the primary purpose of the proposed model.


### 141
##### 10.1109/TPAMI.2019.2920591
#### On the Convergence of Learning-based Iterative Methods for Nonconvex Inverse Problems


0	  Numerous tasks at the core of statistics, learning and vision areas are specific cases of ill-posed inverse problems.
0	  Recently, learning-based (e.g., deep) iterative methods have been empirically shown to be useful for these problems.
0	  Nevertheless, integrating learnable structures into iterations is still a laborious process, which can only be guided by intuitions or empirical insights.
0	  Moreover, there is a lack of rigorous analysis about the convergence behaviors of these reimplemented iterations, and thus the significance of such methods is a little bit vague.
1	  This paper moves beyond these limits and proposes Flexible Iterative Modularization Algorithm (FIMA), a generic and provable paradigm for nonconvex inverse problems.
2	  Our theoretical analysis reveals that FIMA allows us to generate globally convergent trajectories for learning-based iterative methods.
2	  Meanwhile, the devised scheduling policies on flexible modules should also be beneficial for classical numerical methods in the nonconvex scenario.
2	  Extensive experiments on real applications verify the superiority of FIMA.


### 142
##### 10.1109/TPAMI.2019.2919707
#### On the Robustness of Semantic Segmentation Models to Adversarial Attacks


0	  Deep Neural Networks (DNNs) have demonstrated exceptional performance on most recognition tasks such as image classification and segmentation.
0	  However, they have also been shown to be vulnerable to adversarial examples.
0	  This phenomenon has recently attracted a lot of attention but it has not been extensively studied on multiple, large-scale datasets and structured prediction tasks such as semantic segmentation which often require more specialised networks with additional components such as CRFs, dilated convolutions, skip-connections and multiscale processing.
1	  In this paper, we present what to our knowledge is the first rigorous evaluation of adversarial attacks on modern semantic segmentation models, using two large-scale datasets.
1	  We analyse the effect of different network architectures, model capacity and multiscale processing, and show that many observations made on the task of classification do not always transfer to this more complex task.
1	  Furthermore, we show how mean-field inference in deep structured models, multiscale processing (and more generally, input transformations) naturally implement recently proposed adversarial defenses.
2	  Our observations will aid future efforts in understanding and defending against adversarial examples.
2	  Moreover, in the shorter term, we show how to effectively benchmark robustness and show which segmentation models should currently be preferred in safety-critical applications due to their inherent robustness.


### 143
##### 10.1109/TPAMI.2019.2919597
#### MOSES: A Streaming Algorithm for Linear Dimensionality Reduction


1	  This paper introduces Memory-limited Online Sub- space Estimation Scheme (MOSES) for both estimating the principal components of streaming data and reducing its dimension.
1	  More specifically, in various applications such as sensor net- works, the data vectors are presented sequentially to a user who has limited storage and processing time available.
1	  Applied to such problems, MOSES can provide a running estimate of leading principal components of the data that has arrived so far and also reduce its dimension.
1	  MOSES generalises the popular incremental Singular Vale Decomposition (iSVD) to handle thin blocks of data, rather than just vectors.
1	  This minor generalisation in part allows us to complement MOSES with a comprehensive statistical analysis, thus providing the first theoretically-sound variant of iSVD, which has been lacking despite the empirical success of this method.
1	  This generalisation also enables us to concretely interpret MOSES as an approximate solver for the underlying non-convex optimisation program.
2	  We find that MOSES consistently surpasses the state of the art in our numerical experiments with both synthetic and real-world datasets, while being computationally inexpensive.


### 144
##### 10.1109/TPAMI.2019.2919616
#### Direction-aware Spatial Context Features for Shadow Detection and Removal


0	  Shadow detection and shadow removal are fundamental and challenging tasks, requiring an understanding of the global image semantics.
1	  This paper presents a novel deep neural network design for shadow detection and removal by analyzing the spatial image context in a direction-aware manner.
1	  To achieve this, we first formulate the direction-aware attention mechanism in a spatial recurrent neural network (RNN) by introducing attention weights when aggregating spatial context features in the RNN.
2	  By learning these weights through training, we can recover direction-aware spatial context (DSC) for detecting and removing shadows.
1	  This design is developed into the DSC module and embedded in a convolutional neural network to learn the DSC features at different levels.
1	  Moreover, we design a weighted cross entropy loss to make effective the training for shadow detection and further adopt the network for shadow removal by using a Euclidean loss function and formulating a color transfer function to address the color and luminosity inconsistencies in the training pairs.
1	  We employed two shadow detection benchmark datasets and two shadow removal benchmark datasets, and performed various experiments to evaluate our method.
2	  Experimental results show that our method performs favorably against the state-of-the-art methods for both shadow detection and shadow removal.


### 145
##### 10.1109/TPAMI.2019.2919301
#### DART: Distribution Aware Retinal Transform for Event-based Cameras


1	  We introduce a generic visual descriptor, termed as distribution aware retinal transform (DART), that encodes the structural context using log-polar grids for event cameras.
1	  The DART descriptor is applied to four different problems, namely object classification, tracking, detection and feature matching: 
1	  (1) The DART features are directly employed as local descriptors in a bag-of-words classification framework and testing is carried out on four standard event-based object datasets (N-MNIST, MNIST-DVS, CIFAR10-DVS, NCaltech-101); 
1	  (2) Extending the classification system, tracking is demonstrated using two key novelties: 
1	  (i) Statistical bootstrapping is leveraged with online learning for overcoming the low-sample problem during the one-shot learning of the tracker, 
1	  (ii) Cyclical shifts are induced in the log-polar domain of the DART descriptor to achieve robustness to object scale and rotation variations; 
1	  (3) To solve the long-term object tracking problem, an object detector is designed using the principle of cluster majority voting.
2	  The detection scheme is then combined with the tracker to result in a high intersection-over-union score with augmented ground truth annotations on the publicly available event camera dataset; (4) Finally, the event context encoded by DART greatly simplifies the feature correspondence problem, especially for spatio-temporal slices far apart in time, which has not been explicitly tackled in the event-based vision domain.


### 146
##### 10.1109/TPAMI.2019.2919303
#### Learning Low-Dimensional Temporal Representations with Latent Alignments


0	  Low-dimensional discriminative representations enhance machine learning methods in both performance and complexity.
0	  This has motivated supervised dimensionality reduction (DR), which transforms high-dimensional data into a discriminative subspace.
0	  Most DR methods require data to be i.i.d.
0	  However, in some domains, data naturally appear in sequences, where the observations are temporally correlated.
1	  We propose a DR method, namely, latent temporal linear discriminant analysis (LT-LDA), to learn low-dimensional temporal representations.
1	  We construct the separability among sequence classes by lifting the holistic temporal structures, which are established based on temporal alignments and may change in different subspaces.
1	  We jointly learn the subspace and the associated latent alignments by optimizing an objective that favors easily separable temporal structures.
1	  We show that this objective is connected to the inference of alignments and thus allows for an iterative solution.
2	  We provide both theoretical insight and empirical evaluations on several real-world sequence datasets to show the applicability of our method.


### 147
##### 10.1109/TPAMI.2019.2919308
#### A Functional Representation for Graph Matching


0	  Graph matching is an important and persistent problem in computer vision and pattern recognition for finding node-to-node correspondence between graphs.
0	  However, graph matching that incorporates pairwise constraints can be formulated as a quadratic assignment problem (QAP), which is NP-complete and results in intrinsic computational difficulties.
1	  This paper presents a functional representation for graph matching (FRGM) that aims to provide more geometric insights on the problem and reduce the space and time complexities.
1	  To achieve these goals, we represent each graph by a linear function space equipped with a functional such as inner product or metric, that has an explicit geometric meaning.
1	  Consequently, the correspondence matrix between graphs can be represented as a linear representation map.
1	  Furthermore, this map can be reformulated as a new parameterization for matching graphs in Euclidean space such that it is consistent with graphs under rigid or nonrigid deformations.
1	  This allows us to estimate the correspondence matrix and geometric deformations simultaneously.
2	  We use the representation of edge-attributes rather than the affinity matrix to reduce the space complexity and propose an efficient optimization strategy to reduce the time complexity.
2	  The experimental results on both synthetic and real-world datasets show that the FRGM can achieve state-of-the-art performance.


### 148
##### 10.1109/TPAMI.2019.2919284
#### Robust RGB-D Face Recognition Using Attribute-Aware Loss


0	  Existing convolutional neural network (CNN) based face recognition algorithms typically learn a discriminative feature mapping, using a loss function that enforces separation of features from different classes and/or aggregation of features within the same class.
0	  However, they may suffer from bias in the training data such as uneven sampling density, because they optimize the adjacency relationship of the learned features without considering the proximity of the underlying faces.
0	  Moreover, since they only use facial images for training, the learned feature mapping may not correctly indicate the relationship of other attributes such as gender and ethnicity, which can be important for some face recognition applications.
1	  In this paper, we propose a new CNN-based face recognition approach that incorporates such attributes into the training process.
2	  Using an attribute-aware loss function that regularizes the feature mapping using attribute proximity, our approach learns more discriminative features that are correlated with the attributes.
1	  We train our face recognition model on a large-scale RGB-D data set with over 100K identities captured under real application conditions.
2	  By comparing our approach with other methods on a variety of experiments, we demonstrate that depth channel and attribute-aware loss greatly improve the accuracy and robustness of face recognition.


### 149
##### 10.1109/TPAMI.2019.2918284
#### Convolutional Networks with Dense Connectivity


0	  Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output.
1	  In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion.
1	  Whereas traditional convolutional networks with L layers have L connections -- one between each layer and its subsequent layer -- our network has $\frac{L(L+1)}{2}$ direct connections.
1	  For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers.
0	  DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially improve parameter efficiency.
2	  We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet).
2	  DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less parameters and computation to achieve high performance.


### 150
##### 10.1109/TPAMI.2019.2918208
#### Learning Visual Instance Retrieval from Failure: Efficient Online Local Metric Adaptation from Negative Samples


0	  Existing visual instance retrieval (VIR) approaches attempt to learn a faithful global matching metric or discriminative feature embedding offline to cover enormous visual appearance variations, so as to directly use it online on various unseen probes for retrieval.
0	  However, their requirement for a huge set of positive training pairs is very demanding in practice and the performance is largely constrained for the unseen testing samples due to the severe data shifting issue.
1	  In contrast, this paper advocates a different paradigm: part of the learning can be performed online but with nominal costs, so as to achieve online metric adaptation for different query probes.
1	  By exploiting easily-available negative samples, we propose a novel solution to achieve the optimal local metric adaptation effectively and efficiently.
2	  The insight of our method is the local hard negative samples can actually provide tight constraints to fine tune the metric locally.
2	  Our local metric adaptation method is generally applicable to be used on top of any offline-learned baselines.
2	  In addition, this paper gives in-depth theoretical analyses of the proposed method to guarantee the reduction of the classification error both asymptotically and practically.
2	  Extensive experiments on various VIR tasks have confirmed our effectiveness and superiority.
