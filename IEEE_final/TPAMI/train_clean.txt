### 41
##### 10.1109/TPAMI.2019.2941941
#### MEMC-Net: Motion Estimation and Motion Compensation Driven Neural Network for Video Interpolation and Enhancement


0	  Motion estimation (ME) and motion compensation (MC) have been widely used for classical video frame interpolation systems over the past decades.
0	  Recently, a number of data-driven frame interpolation methods based on convolutional neural networks have been proposed.
0	  However, existing learning based methods typically estimate either flow or compensation kernels, thereby limiting performance on both computational efficiency and interpolation accuracy.
1	  In this work, we propose a motion estimation and compensation driven neural network for video frame interpolation.
1	  A novel adaptive warping layer is developed to integrate both optical flow and interpolation kernels to synthesize target frame pixels.
1	  This layer is fully differentiable such that both the flow and kernel estimation networks can be optimized jointly.
1	  The proposed model benefits from the advantages of motion estimation and compensation methods without using hand-crafted features.
2	  Compared to existing methods, our approach is computationally efficient and able to generate more visually appealing results.
2	  Furthermore, the proposed MEMC-Net can be seamlessly adapted to several video enhancement tasks, e.g., super-resolution, denoising, and deblocking.
2	  Extensive quantitative and qualitative evaluations demonstrate that the proposed method performs favorably against the state-of-the-art video frame interpolation and enhancement algorithms on a wide range of datasets.


### 42
##### 10.1109/TPAMI.2019.2942028
#### Harmonized Multimodal Learning with Gaussian Process Latent Variable Models


0	  Multimodal learning aims to discover the relationship between multiple modalities.
0	  It has become an important research topic due to extensive multimodal applications such as cross-modal retrieval.
1	  This paper attempts to address the modality heterogeneity problem based on Gaussian process latent variable models (GPLVMs) to represent multimodal data in a common space.
0	  Previous multimodal GPLVM extensions generally adopt individual learning schemes on latent representations and kernel hyperparameters, which ignore their intrinsic relationship.
1	  To exploit strong complementarity among different modalities and GPLVM components, we develop a novel learning scheme called Harmonization, where latent model parameters are jointly learned from each other.
1	  Beyond the correlation fitting or intra-modal structure preservation paradigms widely used in existing studies, the harmonization is derived in a model-driven manner to encourage the agreement between modality-specific GP kernels and the similarity of latent representations.
1	  We present a range of multimodal learning models by incorporating the harmonization mechanism into several representative GPLVM-based approaches.
2	  Experimental results on four benchmark datasets show that the proposed models outperform the strong baselines for cross-modal retrieval tasks, and that the harmonized multimodal learning method is superior in discovering semantically consistent latent representation.


### 43
##### 10.1109/TPAMI.2019.2942030
#### Hierarchical Long Short-Term Concurrent Memory for Human Interaction Recognition


0	  In this work, we aim to address the problem of human interaction recognition in videos by exploring the long-term inter-related dynamics among multiple persons.
0	  Recently, Long Short-Term Memory (LSTM) has become a popular choice to model individual dynamic for single-person action recognition due to its ability to capture the temporal motion information in a range.
0	  However, most existing LSTM-based methods focus only on capturing the dynamics of human interaction by simply combining all dynamics of individuals or modeling them as a whole.
0	  Such methods neglect the inter-related dynamics of how human interactions change over time.
1	  To this end, we propose a novel Hierarchical Long Short-Term Concurrent Memory (H-LSTCM) to model the long-term inter-related dynamics among a group of persons for recognizing human interactions.
1	  Specifically, we first feed each person's static features into a Single-Person LSTM to model the single-person dynamic.
1	  Subsequently, at one time step, the outputs of all Single-Person LSTM units are fed into a novel Concurrent LSTM (Co-LSTM) unit, which mainly consists of multiple sub-memory units, a new cell gate, and a new co-memory cell.
1	  In the Co-LSTM unit, each sub-memory unit stores individual motion information, while this Co-LSTM unit selectively integrates and stores inter-related motion information between multiple interacting persons from multiple sub-memory units via the cell gate and co-memory cell, respectively.
2	  Extensive experiments on several public datasets validate the effectiveness of the proposed H-LSTCM by comparing against baseline and state-of-the-art methods.


### 44
##### 10.1109/TPAMI.2019.2941876
#### Are Large-Scale 3D Models Really Necessary for Accurate Visual Localization?


0	  Accurate visual localization is a key technology for autonomous navigation.
0	  3D structure-based methods employ 3D models of the scene to estimate the full 6 degree-of-freedom (DOF) pose of a camera very accurately.
0	  However, constructing (and extending) large-scale 3D models is still a significant challenge.
0	  In contrast, 2D image retrieval-based methods only require a database of geo-tagged images, which is trivial to construct and to maintain.
0	  They are often considered inaccurate since they only approximate the positions of the cameras.
0	  Yet, the exact camera pose can theoretically be recovered when enough relevant database images are retrieved.
1	  In this paper, we demonstrate experimentally that large-scale 3D models are not strictly necessary for accurate visual localization.
1	  We create reference poses for a large and challenging urban dataset.
2	  Using these poses, we show that combining image-based methods with local reconstructions results in a pose accuracy similar to the state-of-the-art structure-based methods.
2	  Our results suggest that we might want to reconsider the current approach for accurate large-scale localization.


### 45
##### 10.1109/TPAMI.2019.2941472
#### Surface-aware Blind Image Deblurring


0	  Blind image deblurring is a conundrum because there are infinitely many pairs of latent images and blur kernels.
0	  To get a stable and reasonable deblurred image, proper prior knowledge of the latent image and the blur kernel is required.
1	  Different from recent works on the statistical observations of the difference between the blurred image and the clean one, our method is based on the surface-aware strategy from the intrinsic geometrical consideration.
1	  This approach facilitates the blur kernel estimation due to the preserved sharp edges in the intermediate latent images.
2	  Extensive experiments demonstrate that our method outperforms state-of-the-art methods on deblurring the text and natural image.
2	  Moreover, our method can achieve attractive results in some challenging cases, such as low-illumination images with large saturated regions and impulse noise.
2	  A direct extension of our method to the non-uniform deblurring problem also validates the effectiveness of the surface-aware prior.


### 46
##### 10.1109/TPAMI.2019.2941684
#### Loss Decomposition and Centroid Estimation for Positive and Unlabeled Learning


1	  This paper studies Positive and Unlabeled learning (PU learning), of which the target is to build a binary classifier where only positive data and unlabeled data are available for classifier training.
1	  To deal with the absence of negative training data, we first regard all unlabeled data as negative examples with false negative labels, and then convert PU learning into the risk minimization problem in the presence of such one-side label noise.
1	  Specifically, we propose a novel PU learning algorithm dubbed "Loss Decomposition and Centroid Estimation" (LDCE).
1	  By decomposing the loss function of corrupted negative examples into two parts, we show that only the second part is affected by the noisy labels.
1	  Thereby, we may estimate the centroid of corrupted negative set via an unbiased way to reduce the adverse impact of such label noise.
1	  Furthermore, we propose the "Kernelized LDCE" (KLDCE) by introducing the kernel trick, and show that KLDCE can be easily solved by combining Alternative Convex Search (ACS) and Sequential Minimal Optimization (SMO).
2	  Theoretically, we derive the generalization error bound which suggests that the generalization risk of our model converges to the empirical risk with the order of $\mathcal{O}(1/ \sqrt k + 1 /{\sqrt {n-k}} + 1/ \sqrt n)$ ($n$ and $k$ are the amounts of training data and positive data correspondingly).
2	  Experimentally, we conduct intensive experiments on synthetic dataset, UCI benchmark datasets and real-world datasets, and the results demonstrate that our approaches (LDCE and KLDCE) achieve the top-level performance when compared with both classic and state-of-the-art PU learning methods.


### 47
##### 10.1109/TPAMI.2019.2939530
#### Bound and Conquer: Improving Triangulation by Enforcing Consistency


0	  We study the accuracy of triangulation in multi-camera systems with respect to the number of cameras.
2	  We show that, under certain conditions, the optimal achievable reconstruction error decays quadratically as more cameras are added to the system.
1	  Furthermore, we analyse the error decay-rate of major state-of-the-art algorithms with respect to the number of cameras.
1	  To this end, we introduce the notion of consistency for triangulation, and show that consistent reconstruction algorithms achieve the optimal quadratic decay, which is asymptotically faster than some other methods.
2	  Finally, we present simulations results supporting our findings.
#	  Our simulations have been implemented in MATLAB and the resulting code is available in the supplementary material.


### 48
##### 10.1109/TPAMI.2019.2940948
#### Unsupervised Domain Adaptation for Depth Prediction from Images


0	  State-of-the-art methods to infer dense and accurate depth measurements from images rely on deep CNN models trained in an end-to-end fashion on a significant amount of data.
0	  However, despite the outstanding performance achieved, these frameworks suffer a drastic drop in accuracy when dealing with unseen environments much different, concerning appearance (e.g., synthetic vs. real) or context (e.g., indoor vs. outdoor), from those observed during the training phase.
0	  Such domain shift issue is usually softened by fine-tuning on smaller sets of images with depth labels acquired in the target domain with active sensors (e.g., LiDAR).
0	  However, relying on such supervised labeled data is seldom feasible in practical applications.
1	  Therefore, we propose an effective unsupervised domain adaptation technique enabling to overcome the domain shift problem without requiring any groundtruth label.
2	  Our method, deploying much more accessible to obtain stereo pairs, leverages traditional and not learning-based stereo algorithms to produce disparity/depth labels and on confidence measures to assess their degree of reliability.
2	  With these cues, we can fine-tune deep models through a novel confidence-guided loss function, neglecting the effect of outliers gathered from the output of conventional stereo algorithms.


### 49
##### 10.1109/TPAMI.2019.2940225
#### Sequence-to-Segments Networks for Detecting Segments in Videos


0	  Detecting segments of interest from videos is a common problem for many applications.
0	  And yet it is a challenging problem as it often requires not only knowledge of individual target segments, but also contextual understanding of the entire video and the relationships between the target segments.
1	  To address this problem, we propose the Sequence-to-Segments Network (S2N), a novel and general end-to-end sequential encoder-decoder architecture.
1	  S2N first encodes the input video into a sequence of hidden states that capture information progressively, as it appears in the video.
1	  It then employs the Segment Detection Unit (SDU), a novel decoding architecture, that sequentially detects segments.
1	  At each decoding step, the SDU integrates the decoder state and encoder hidden states to detect a target segment.
1	  During training, we address the problem of finding the best assignment of predicted segments to ground truth using the Hungarian Matching Algorithm with Lexicographic Cost.
1	  Additionally, we propose to use the squared Earth Mover's Distance to optimize the localization errors of the segments.
2	  We show the state-of-the-art performance of S2N across numerous tasks, including video highlighting, video summarization, and human action proposal generation.


### 50
##### 10.1109/TPAMI.2019.2940655
#### Topology-Aware Non-Rigid Point Cloud Registration


1	  In this paper, we introduce a non-rigid registration pipeline for unorganized point clouds that may be topologically different.
1	  Standard warp field estimation algorithms, even under robust, discontinuity-preserving regularization, produce erratic motion estimates on boundaries associated with ‘close-to-open’ topology changes.
1	  We overcome this limitation by exploiting backward motion: in the opposite direction, a ‘close-to-open’ event becomes ‘open-to-close’, which is by default handled correctly.
1	  Our approach relies on a general, topology-agnostic warp field estimation algorithm, similar to those employed in recent dynamic reconstruction systems from RGB-D input.
1	  We improve motion estimation on boundaries associated with topology changes in an efficient post-processing phase.
1	  Based on both forward and (inverted) backward warp hypotheses, we explicitly detect regions of the deformed geometry that undergo topological changes by means of local deformation criteria and broadly classify them as ‘contacts’ or ‘separations’.
1	  Subsequently, the two motion hypotheses are seamlessly blended on a local basis, according to the type and proximity of detected events.
2	  Our method achieves state-of-the-art motion estimation accuracy on the MPI Sintel dataset.
2	  Experiments on a custom dataset with topological event annotations demonstrate the effectiveness of our pipeline in estimating motion on event boundaries, as well as promising performance in explicit topological event detection.


### 51
##### 10.1109/TPAMI.2019.2940446
#### MTFH: A Matrix Tri-Factorization Hashing Framework for Efficient Cross-Modal Retrieval


0	  Hashing has recently sparked a great revolution in cross-modal retrieval because of its low storage cost and high query speed.
0	  Recent cross-modal hashing methods often learn unified or equal-length hash codes to represent the multi-modal data and make them intuitively comparable.
0	  However, such representations could inherently sacrifice their representation scalability because the data from different modalities may not have one-to-one correspondence and could be encoded more efficiently by different hash codes of unequal lengths.
1	  To mitigate these problems, this paper exploits a related and relatively unexplored problem: encode the heterogeneous data with varying hash lengths and generalize the cross-modal retrieval in various challenging scenarios.
1	  To this end, a generalized and flexible cross-modal hashing framework, termed Matrix Tri-Factorization Hashing (MTFH), is proposed to work seamlessly in various settings including paired or unpaired multi-modal data, and equal or varying hash length encoding scenarios.
1	  MTFH exploits an efficient objective function to flexibly learn modality-specific hash codes with different length settings, while synchronously learning two semantic correlation matrices for heterogeneous data comparable.
2	  As a result, the derived hash codes are more semantically meaningful for various challenging cross-modal retrieval tasks.
2	  Extensive experiments evaluated on public benchmark datasets highlight the superiority of MTFH under various retrieval scenarios and show its competitive performance with the state-of-the-arts.


### 52
##### 10.1109/TPAMI.2019.2938523
#### Learning Part-based Convolutional Features for Person Re-identification


0	  Part-level features offers fine granularity for pedestrian image description.
0	  In this article, we generally aim to learn discriminative part-informed features for person re-identification.
1	  First, we introduce a general part-level feature learning method, named Part-based Convolutional Baseline (PCB).
1	  Given an image, it outputs a convolutional descriptor consisting of several part-level features.
1	  PCB is general in that it is able to accommodate several part partitioning strategies.
2	  In experiment, we show that the learned descriptor maintains a significantly higher discriminative ability than the global descriptor.
1	  Second, Based on PCB, we propose refined part pooling (RPP) to improve the original partition.
1	  Our idea is that pixels within a well-located part should be similar to each other while being dissimilar with pixels from other parts.
1	  We call it within-part consistency.
1	  When a pixel-wise feature vector in a part is more similar to some other part, it is then an outlier, indicating inappropriate partitioning.
1	  RPP re-assigns these outliers to the parts they are closest to, resulting in refined parts with enhanced within-part consistency.
2	  Experiment confirms that RPP gains another round of performance boost over PCB.
2	  For instance, on the Market-1501 dataset, we achieve (77.4+4.2)% mAP and (92.3+1.5)% rank-1 accuracy, a competitive performance with the state of the art.


### 53
##### 10.1109/TPAMI.2019.2939237
#### On the Global Geometry of Sphere-Constrained Sparse Blind Deconvolution


0	  Blind deconvolution is the problem of recovering a convolutional kernel $a_0$ and an activation signal $x_0$ from their convolution $y = a_{0} ❂ x_{0}$.
0	  This problem is ill-posed without further constraints or priors.
1	  This paper studies the situation where the nonzero entries in the activation signal are sparsely and randomly populated.
1	  We normalize the convolution kernel to have unit Frobenius norm and cast the sparse blind deconvolution problem as a nonconvex optimization problem over the sphere.
1	  With this spherical constraint, every spurious local minimum turns out to be close to some signed shift truncation of the ground truth, under certain hypotheses.
1	  This benign property motivates an effective two stage algorithm that recovers the ground truth from the partial information offered by a suboptimal local minimum.
2	  This geometry-inspired algorithm recovers the ground truth for certain microscopy problems, also exhibits promising performance in the more challenging image deblurring problem.
2	  Our insights into the global geometry and the two stage algorithm extend to the convolutional dictionary learning problem, where a superposition of multiple convolution signals is observed.


### 54
##### 10.1109/TPAMI.2019.2939307
#### Deterministic Approximate Methods for Maximum Consensus Robust Fitting


0	  Maximum consensus estimation plays a critically important role in robust fitting problems in computer vision.
0	  Currently, the most prevalent algorithms for consensus maximization draw from the class of randomized hypothesize-and-verify algorithms, which are cheap but can usually deliver only rough approximate solutions.
0	  On the other extreme, there are exact algorithms which are exhaustive search in nature and can be costly for practical-sized inputs.
1	  This paper fills the gap between the two extremes by proposing deterministic algorithms to approximately optimize the maximum consensus criterion.
1	  Our work begins by reformulating consensus maximization with linear complementarity constraints.
1	  Then, we develop two novel algorithms: one based on non-smooth penalty method with a Frank-Wolfe style optimization scheme, the other based on the Alternating Direction Method of Multipliers (ADMM).
1	  Both algorithms solve convex subproblems to efficiently perform the optimization.
1	  We demonstrate the capability of our algorithms to greatly improve a rough initial estimate, such as those obtained using least squares or a randomized algorithm.
2	  Compared to the exact algorithms, our approach is much more practical on realistic input sizes.
2	  Further, our approach is naturally applicable to estimation problems with geometric residuals.
#	  Matlab code and demo program for our methods can be downloaded from https://goo.gl/FQcxpi.


### 55
##### 10.1109/TPAMI.2019.2938758
#### Res2Net: A New Multi-scale Backbone Architecture


0	  Representing features at multiple scales is of great importance for numerous vision tasks.
0	  Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications.
0	  However, most existing methods represent the multi-scale features in a layer-wise manner.
1	  In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block.
1	  The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer.
1	  The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA.
2	  We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet.
2	  Further ablation studies and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods.
#	  The source code and trained models are available on https://mmcheng.net/res2net/.


### 56
##### 10.1109/TPAMI.2019.2937869
#### Matrix Completion with Deterministic Sampling: Theories and Methods


0	  In some significant applications such as data forecasting, the locations of missing entries cannot obey any non-degenerate distributions, questioning the validity of the prevalent assumption that the missing data is randomly chosen according to some probabilistic model.
1	  To break through the limits of random sampling, we explore in this paper the problem of real-valued matrix completion under the setup of deterministic sampling.
1	  We propose two conditions, isomeric condition and relative well-conditionedness, for guaranteeing an arbitrary matrix to be recoverable from a sampling of the matrix entries.
2	  It is provable that the proposed conditions are weaker than the assumption of uniform sampling and, most importantly, it is also provable that the isomeric condition is necessary for the completions of any partial matrices to be identifiable.
2	  Equipped with these new tools, we prove a collection of theorems for missing data recovery as well as convex/nonconvex matrix completion.
2	  Among other things, we study in detail a Schatten quasi-norm induced method termed isomeric dictionary pursuit (IsoDP), and we show that IsoDP exhibits some distinct behaviors absent in the traditional bilinear programs.


### 57
##### 10.1109/TPAMI.2019.2937294
#### Deep Differentiable Random Forests for Age Estimation


0	  Age estimation from facial images is typically cast as a label distribution learning or regression problem, since aging is a gradual progress.
0	  Its main challenge is the facial feature space w.r.t. ages is inhomogeneous, due to the large variation in facial appearance across different persons of the same age and the non-stationary property of aging.
1	  In this paper, we propose two Deep Differentiable Random Forests methods, Deep Label Distribution Learning Forest (DLDLF) and Deep Regression Forest (DRF), for age estimation.
1	  Both of them connect split nodes to the top layer of convolutional neural networks (CNNs) and deal with inhomogeneous data by jointly learning input-dependent data partitions at the split nodes and age distributions at the leaf nodes.
1	  This joint learning follows an alternating strategy: (1) Fixing the leaf nodes and optimizing the split nodes and the CNN parameters by Back-propagation; (2) Fixing the split nodes and optimizing the leaf nodes by Variational Bounding.
2	  Two Deterministic Annealing processes are introduced into the learning of the split and leaf nodes, respectively, to avoid poor local optima and obtain better estimates of tree parameters free of initial values.
2	  Experimental results show that DLDLF and DRF achieve state-of-the-art results on three age estimation datasets.


### 58
##### 10.1109/TPAMI.2019.2937515
#### Reconstruction Geometric and Optical Parameters of Non-Planar Objects with Thin Film


0	  Here, we propose a novel method to estimate the parameters of non-planar objects with thin film surfaces.
0	  Being able to estimate the optical parameters of objects with thin film surfaces has a wide range of applications from industrial inspections to biological and archaeology research.
0	  However, there are many challenging issues that need to be overcome to model such parameters.
0	  The appearance of thin film objects is highly dependent on the surface orientation and optical parameters such as the refractive index and film thickness.
1	  First, we therefore analyzed the optical parameters of non-planar objects with thin film surfaces.
1	  Next, we proposed and implemented an analysis procedure and demonstrated its effectiveness for studying planar objects with thin film surfaces.
1	  Finally, we developed a device to acquire the shapes and optical parameters of objects with thin film surfaces using a camera and demonstrated the effectiveness of our method experimentally.
1	  Then, we surveyed the errors caused by the light source.
1	  We discussed the difference between the theoretically obtained parameters and experimental data obtained using a hyper spectral camera.


### 59
##### 10.1109/TPAMI.2019.2937086
#### Mask TextSpotter: An End-to-End Trainable Neural Network for Spotting Text with Arbitrary Shapes


0	  Unifying text detection and text recognition in an end-to-end training fashion has become a new trend for reading text in the wild, as these two tasks are highly relevant and complementary.
0	  In this paper, we investigate the problem of scene text spotting, which aims at simultaneous text detection and recognition in natural images.
1	  An end-to-end trainable neural network named as Mask TextSpotter is presented.
1	  Different from the previous text spotters that follow the pipeline consisting of a proposal generation network and a sequence-to-sequence recognition network, Mask TextSpotter enjoys a simple and smooth end-to-end learning procedure, in which both detection and recognition can be achieved directly from two-dimensional space via semantic segmentation.
1	  Further, a spatial attention module is proposed to enhance the performance and universality.
2	  Benefiting from the proposed two-dimensional representation on both detection and recognition, it easily handles text instances of irregular shapes, for instance, curved text.
2	  We evaluate it on four English datasets and one multi-language dataset, achieving consistently superior performance over state-of-the-art methods in both detection and end-to-end text recognition tasks.
2	  Moreover, we further investigate the recognition module of our method separately, which significantly outperforms state-of-the-art methods on both regular and irregular text datasets for scene text recognition.


### 60
##### 10.1109/TPAMI.2019.2937292
#### Discriminative Video Representation Learning Using Support Vector Classifiers


0	  Most popular deep models for action recognition in videos generate independent predictions for short clips, which are then pooled heuristically to assign an action label to the full video segment.
0	  As not all frames may characterize the underlying action, pooling schemes that impose equal importance on all frames might be unfavorable.
1	  To tackle this problem, we propose discriminative pooling, based on the notion that among the features generated on all short clips, there is at least one that characterizes the action.
1	  To identify useful features, we resort to a negative bag consisting of features that are known to be irrelevant.
1	  With the features from the video as a positive bag and the irrelevant features as the negative bag, we cast an objective to learn a (nonlinear) hyperplane that separates the unknown useful feature from the rest in a multiple instance learning formulation within a support vector machine setup.
1	  We use the parameters of this separating hyperplane as descriptors for the video.
1	  Since these parameters are directly related to the support vectors in a max-margin framework, they can be treated as weighted-average-pooling of the features from the bags.
2	  We report results from experiments on eight computer vision benchmarks demonstrating state-of-the-art performance across these tasks.


### 61
##### 10.1109/TPAMI.2019.2936841
#### Neural Image Compression for Gigapixel Histopathology Image Analysis


1	  We propose Neural Image Compression (NIC), a two-step method to build convolutional neural networks for gigapixel image analysis solely using weak image-level labels.
1	  First, gigapixel images are compressed using a neural network trained in an unsupervised fashion, retaining high-level information while suppressing pixel-level noise.
1	  Second, a convolutional neural network (CNN) is trained on these compressed image representations to predict image-level labels, avoiding the need for fine-grained manual annotations.
1	  We compared several encoding strategies, namely reconstruction error minimization, contrastive training and adversarial feature learning, and evaluated NIC on a synthetic task and two public histopathology datasets.
2	  We found that NIC can exploit visual cues associated with image-level labels successfully, integrating both global and local visual information.
2	  Furthermore, we visualized the regions of the input gigapixel images where the CNN attended to, and confirmed that they overlapped with annotations from human experts.


### 62
##### 10.1109/TPAMI.2019.2936378
#### Adversarial Attack Type I: Cheat Classifiers by Significant Changes


0	  Despite the great success of deep neural networks, the adversarial attack can cheat some well-trained classifiers by small permutations.
0	  In this paper, we propose another type of adversarial attack that can cheat classifiers by significant changes.
0	  For example, we can significantly change a face but well-trained neural networks still recognize the adversarial and the original example as the same person.
0	  Statistically, the existing adversarial attack increases Type II error and the proposed one aims at Type I error, which are hence named as Type II and Type I adversarial attack, respectively.
0	  The two types of attack are equally important but are essentially different, which are intuitively explained and numerically evaluated.
1	  To implement the proposed attack, a supervised variation autoencoder is designed and then the classifier is attacked by updating the latent variables using gradient information.
2	  Experimental results show that our method is practical and effective to generate Type I adversarial examples on large-scale image datasets.
2	  Most of these generated examples can pass detectors designed for defending Type II attack and the strengthening strategy is only efficient with a specific type attack, both implying that the underlying reasons for Type I and Type II attack are different.


### 63
##### 10.1109/TPAMI.2019.2936024
#### Semi-Supervised Adversarial Monocular Depth Estimation


0	  In this paper, we address the problem of monocular depth estimation when only a limited amount of training image-depth pairs are available.
0	  To achieve high regression accuracy, state-of-the-art estimation methods rely on CNNs trained with a vast amount of image-depth pairs, which are prohibitively costly or even infeasible to acquire.
1	  Aiming to break the bottleneck of such expensive data collections, in this paper, we propose a semi-supervised adversarial learning framework, which only utilizes a small amount of image-depth pairs with a large amount of cheaply-available monocular images to pursuit high accuracy.
1	  In particular, we use one generator to regress the depth and two discriminators to evaluate the predicted depth.
2	  These two discriminators provide their feedbacks to the generator as the loss to generate more realistic and accurate depth predictions.
2	  Experiments show that the proposed approach can (1) improve most state-of-the-art models on NYUD v2 dataset by effectively leveraging additional unlabeled data sources; (2) reach state-of-the-art accuracy when the training set is small, e.g., on Make3D dataset; (3) adapts well to an unseen new dataset (Make3D in our case) after training on an annotated dataset (KITTI in our case).


### 64
##### 10.1109/TPAMI.2019.2935715
#### Saliency Prediction in the Deep Learning Era: Successes and Limitations


0	  Visual saliency models have enjoyed a big leap in performance in recent years, thanks to advances in deep learning and large scale annotated data.
0	  Despite enormous effort and huge breakthroughs, however, models still fall short in reaching human-level accuracy.
0	  In this work, I explore the landscape of the field emphasizing on new deep saliency models, benchmarks, and datasets.
1	  A large number of image and video saliency models are reviewed and compared over two image benchmarks and two large scale video datasets.
2	  Further, I identify factors that contribute to the gap between models and humans and discuss remaining issues that need to be addressed to build the next generation of more powerful saliency models.
2	  Some specific questions that are addressed include: in what ways current models fail, how to remedy them, what can be learned from cognitive studies of attention, how explicit saliency judgments relate to fixations, how to conduct fair model comparison, and what are the emerging applications of saliency models.


### 65
##### 10.1109/TPAMI.2019.2934852
#### Learning Energy-based Spatial-Temporal Generative ConvNets for Dynamic Patterns


0	  Video sequences contain rich dynamic patterns, such as dynamic texture patterns that exhibit stationarity in the temporal domain, and action patterns that are non-stationary in either spatial or temporal domain.
2	  We show that an energy-based spatial-temporal generative ConvNet can be used to model and synthesize dynamic patterns.
1	  The model defines a probability distribution on the video sequence, and the log probability is defined by a spatial-temporal ConvNet that consists of multiple layers of spatial-temporal filters to capture spatial-temporal patterns of different scales.
1	  The model can be learned from the training video sequences by an "analysis by synthesis" learning algorithm that iterates the following two steps.
1	  Step 1 synthesizes video sequences from the currently learned model.
1	  Step 2 then updates the model parameters based on the difference between the synthesized video sequences and the observed training sequences.
2	  We show that the learning algorithm can synthesize realistic dynamic patterns.
2	  We also show that it is possible to learn the model from incomplete training sequences with either occluded pixels or missing frames, so that model learning and pattern completion can be accomplished simultaneously.


### 66
##### 10.1109/TPAMI.2019.2934455
#### Polyhedral Conic Classifiers for Computer Vision Applications and Open Set Recognition


0	  This paper introduces a family of quasi-linear discriminants that outperform current large-margin methods in sliding window visual object detection and open set recognition tasks.
0	  In these applications, the classification problems are both numerically imbalanced -- positive training and test windows are much rarer than negative ones -- and geometrically asymmetric -- the positive samples typically form compact, visually-coherent groups while negatives are much more diverse, including anything at all that is not a well-centered sample from the target class.
0	  For such tasks, there is a need for discriminants whose decision regions focus on tightly circumscribing the positive class, while still taking account of negatives in zones where the two classes overlap.
1	  To this end, we propose a family of quasi-linear polyhedral conic discriminants whose positive regions are distorted L1 or L2 balls.
1	  In addition, we also integrated the proposed classification loss into deep neural networks so that both the features and classifier can be learned simultaneously end-to-end fashion to improve the classification accuracies.
1	  The methods can be trained from either binary or positive-only samples using constrained quadratic programs related to SVMs.
2	  Our experiments show that they significantly outperform linear SVMs, deep neural networks using softmax loss function and existing one-class discriminants.


### 67
##### 10.1109/TPAMI.2019.2934052
#### Matching Seqlets: An Unsupervised Approach for Locality Preserving Sequence Matching


1	  In this paper, we propose a novel unsupervised approach for sequence matching by explicitly accounting for the locality properties in the sequences.
0	  In contrast to conventional approaches that rely on frame-to-frame matching, we conduct matching using sequencelet or seqlet, a sub-sequence wherein the frames share strong similarities and are thus grouped together.
1	  The optimal seqlets and matching between them are learned jointly, without any supervision from users.
1	  The learned seqlets preserve the locality information at the scale of interest and resolve the ambiguities during matching, which are omitted by frame-based matching methods.
2	  We show that our proposed approach outperforms the state-of-the-art ones on datasets of different domains including human actions, facial expressions, speech, and character strokes.


### 68
##### 10.1109/TPAMI.2019.2933829
#### Inferring Latent Domains for Unsupervised Deep Domain Adaptation


0	  Unsupervised Domain Adaptation (UDA) refers to the problem of learning a model in a target domain where labeled data are not available by leveraging information from annotated data in a source domain.
0	  Most deep UDA approaches operate in a single-source, single-target scenario, i.e.
0	  they assume that the source and the target samples arise from a single distribution.
0	  However, in practice most datasets can be regarded as mixtures of multiple domains.
0	  In these cases, exploiting traditional single-source, single-target methods for learning classification models may lead to poor results.
0	  Furthermore, it is often difficult to provide the domain labels for all data points, i.e. latent domains should be automatically discovered.
1	  This paper introduces a novel deep architecture which addresses the problem of UDA by automatically discovering latent domains in visual datasets and exploiting this information to learn robust target classifiers.
1	  Specifically, our architecture is based on two main components, i.e. a side branch that automatically computes the assignment of each sample to its latent domain and novel layers that exploit domain membership information to appropriately align the distribution of the CNN internal feature representations to a reference distribution.
2	  We evaluate our approach on publicly available benchmarks, showing that it outperforms state-of-the-art domain adaptation methods.


### 69
##### 10.1109/TPAMI.2019.2933841
#### Faster First-Order Methods for Stochastic Non-Convex Optimization on Riemannian Manifolds


0	  First-order non-convex Riemannian optimization algorithms have gained recent popularity in structured machine learning problems including principal component analysis and low-rank matrix completion.
1	  The current paper presents an efficient Riemannian Stochastic Path Integrated Differential EstimatoR (R-SPIDER) algorithm to solve the finite-sum and online Riemannian non-convex minimization problems.
1	  At the core of R-SPIDER is a recursive semi-stochastic gradient estimator that can accurately estimate Riemannian gradient under not only exponential mapping and parallel transport, but also general retraction and vector transport operations.
2	  Compared with prior Riemannian algorithms, such a recursive gradient estimation mechanism endows R-SPIDER with lower computational cost in first-order oracle complexity.
2	  Specifically, for finite-sum problems with $n$ components, R-SPIDER is proved to converge to an $\epsilon$-approximate stationary point within $\mathcal{O}\big(\min\big(n+\frac{\sqrt{n}}{\epsilon^2},\frac{1}{\epsilon^3}\big)\big)$ stochastic gradient evaluations, beating the best-known complexity $\mathcal{O}\big(n+\frac{1}{\epsilon^4}\big)$; for online optimization, R-SPIDER is shown to converge with $\mathcal{O}\big(\frac{1}{\epsilon^3}\big)$ complexity which is, to the best of our knowledge, the first non-asymptotic result for online Riemannian optimization.
1	  For the special case of gradient dominated functions, we further develop a variant of R-SPIDER with improved linear rate of convergence.
2	  Extensive experimental results demonstrate the advantage of the proposed algorithms over the state-of-the-art Riemannian non-convex optimization methods.


### 70
##### 10.1109/TPAMI.2019.2933818
#### Virtual Point Removal for Large-Scale 3D Point Clouds With Multiple Glass Planes


0	  Large-scale 3D point clouds (LS3DPCs) captured by terrestrial LiDAR scanners often include virtual points which are generated by glass reflection.
0	  The virtual points may degrade the performance of various computer vision techniques when applied to LS3DPCs.
1	  In this paper, we propose a virtual point removal algorithm for LS3DPCs with multiple glass planes.
1	  We first estimate multiple glass regions by modeling the reliability with respect to each glass plane, respectively, such that the regions are assigned high reliability when they have multiple echo pulses for each emitted laser pulse.
1	  Then we detect each point whether it is a virtual point or not.
1	  For a given point, we recursively traverse all the possible trajectories of reflection, and select the optimal trajectory which provides a point with a similar geometric feature to a given point at the symmetric location.
1	  We evaluate the performance of the proposed algorithm on various LS3DPC models with diverse numbers of glass planes.
2	  Experimental results show that the proposed algorithm estimates multiple glass regions faithfully and detects the virtual points successfully.
2	  Moreover, we also show that the proposed algorithm yields a much better performance of reflection artifact removal compared with the existing method qualitatively and quantitatively.



### 71
##### 10.1109/TPAMI.2019.2933510
#### P-CNN: Part-Based Convolutional Neural Networks for Fine-Grained Visual Categorization


1 	  This paper proposes an end-to-end fine-grained visual categorization system, termed Part-based Convolutional Neural Network (P-CNN), which consists of three modules.
1	  The first module is a Squeeze-and-Excitation (SE) block, which learns to recalibrate channel-wise feature responses by emphasizing informative channels and suppressing less useful ones.
1	  The second module is a Part Localization Network (PLN) used to locate distinctive object parts, through which a bank of convolutional filters are learned as discriminative part detectors.
1	  Thus, a group of informative parts can be discovered by convolving the feature maps with each part detector.
1	  The third module is a Part Classification Network (PCN) that has two streams.
1	  The first stream classifies each individual object part into image-level categories.
1	  The second stream concatenates part features and global feature into a joint feature for the final classification.
1	  In order to learn powerful part features and boost the joint feature capability, we propose a Duplex Focal Loss used for metric learning and part classification, which focuses on training hard examples.
1	  We further merge PLN and PCN into a unified network for an end-to-end training process via a simple training technique.
2	  Comprehensive experiments and comparisons with state-of-the-art methods on three benchmark datasets demonstrate the effectiveness of our proposed method.


### 72
##### 10.1109/TPAMI.2019.2933209
#### Parallel and Scalable Heat Methods for Geodesic Distance Computation


0	  In this paper, we propose a parallel and scalable approach for geodesic distance computation on triangle meshes.
2	  Our key observation is that the recovery of geodesic distance in the heat method can be reformulated as an optimization of its gradients subject to integrability, which can be solved using an efficient fixed-order method that requires no linear system solving and converges quickly.
1	  Afterwards, the geodesic distance is efficiently recovered by parallel integration of the optimized gradients in breadth-first order.
1	  Moreover, we employ a similar breadth-first strategy to derive a parallel Gauss-Seidel solver for the diffusion step in the heat method.
1	  To further lower the memory consumption from gradient optimization on faces, we also propose a formulation that optimizes the projected gradients on edges, further reducing the memory footprint by about 50%.
2	  Our approach is trivially parallelizable, with a low memory footprint that grows linearly with respect to the model size.
2	  This makes it particularly suitable for handling large models.
2	  Experimental results show that it can efficiently compute geodesic distance on meshes with more than 200 million vertices on a desktop PC with 128GB RAM, outperforming the original heat method and other state-of-the-art geodesic distance solvers.


### 73
##### 10.1109/TPAMI.2019.2932979
#### Sparse Coding of Shape Trajectories for Facial Expression and Action Recognition


0	  The detection and tracking of human landmarks in video streams has gained in reliability partly due to the availability of affordable RGB-D sensors.
0	  The analysis of such time-varying geometric data is playing an important role in automatic human behavior understanding.
0	  However, suitable shape representations as well as their temporal evolution, termed trajectories, often lie to nonlinear manifolds.
0	  This puts an additional constraint (i.e., nonlinearity) in using conventional Machine Learning techniques.
1	  As a solution, this paper accommodates the well-known Sparse Coding and Dictionary Learning approach to study time-varying shapes on the Kendall shape spaces of 2D and 3D landmarks.
1	  We illustrate effective coding of 3D skeletal sequences for action recognition and 2D facial landmark sequences for macro- and micro-expression recognition.
1	  To overcome the inherent nonlinearity of the shape spaces, intrinsic and extrinsic solutions were explored.
2	  As main results, shape trajectories give rise to more discriminative time-series with suitable computational properties, including sparsity and vector space structure.
2	  Extensive experiments conducted on commonly-used datasets demonstrate the competitiveness of the proposed approaches with respect to state-of-the-art.


### 74
##### 10.1109/TPAMI.2019.2932429
#### DAC-SDC Low Power Object Detection Challenge for UAV Applications


0	  The 55th Design Automation Conference (DAC) held its first System Design Contest (SDC) in 2018.
1	  SDC'18 features a lower power object detection challenge (LPODC) on designing and implementing novel algorithms based object detection in images taken from unmanned aerial vehicles (UAV).
1	  The dataset includes 95 categories and 150k images, and the hardware platforms include Nvidia's TX2 and Xilinx's PYNQ Z1.
1	  DAC-SDC'18 attracted more than 110 entries from 12 countries.
1	  This paper presents in detail the dataset and evaluation procedure.
2	  It further discusses the methods developed by some of the entries as well as representative results.
2	  The paper concludes with directions for future improvements.


### 75
##### 10.1109/TPAMI.2019.2932976
#### Multivariate Extension of Matrix-based Renyi's α-order Entropy Functional


0	  The matrix-based Renyi's α-order entropy functional was recently introduced using the normalized eigenspectrum of a Hermitian matrix of the projected data in a reproducing kernel Hilbert space (RKHS).
0	  However, the current theory in the matrix-based Renyi's α-order entropy functional only defines the entropy of a single variable or mutual information between two random variables.
0	  In information theory and machine learning communities, one is also frequently interested in multivariate information quantities, such as the multivariate joint entropy and different interactive quantities among multiple variables.
1	  In this paper, we first define the matrix-based Renyi's α-order joint entropy among multiple variables.
1	  We then show how this definition can ease the estimation of various information quantities that measure the interactions among multiple variables, such as interactive information and total correlation.
1	  We finally present an application to feature selection to show how our definition provides a simple yet powerful way to estimate a widely-acknowledged intractable quantity from data.
2	  A real example on hyperspectral image (HSI) band selection is also provided.


### 76
##### 10.1109/TPAMI.2019.2932415
#### SafePredict: A Meta-Algorithm for Machine Learning That Uses Refusals to Guarantee Correctness


0	  SafePredict is a novel meta-algorithm that works with any base prediction algorithm for online data to guarantee an arbitrarily chosen correctness rate, $1-\in$, by allowing refusals.
0	  Allowing refusals means that the meta-algorithm may refuse to emit a prediction produced by the base algorithm on occasion so that the error rate on non-refused predictions does not exceed $\in$.
1	  The SafePredict error bound does not rely on any assumptions on the data distribution or the base predictor.
1	  When the base predictor happens not to exceed the target error rate $\in$, SafePredict refuses only a finite number of times.
1	  When the error rate of the base predictor changes through time SafePredict makes use of a weight-shifting heuristic that adapts to these changes without knowing when the changes occur yet still maintains the correctness guarantee.
2	  Empirical results show that (i) SafePredict compares favorably with state-of-the art confidence based refusal mechanisms which fail to offer robust error guarantees; and (ii) combining SafePredict with such refusal mechanisms can in many cases further reduce the number of refusals.
#	  Our software is included in the supplementary material.


### 77
##### 10.1109/TPAMI.2019.2930192
#### Pattern of Local Gravitational Force(PLGF): A novel Local Image Descriptor


1	  This paper presents a novel local image descriptor called Pattern of Local Gravitational Force (PLGF).
1	  It is inspired by Law of Universal Gravitation.
1	  PLGF is a hybrid descriptor, which is a combination of two feature components: one is the Pattern of Local Gravitational Force Magnitude (PLGFM), and another is Pattern of Local Gravitational Force Angle (PLGFA).
1	  PLGFM encodes the local gravitational force magnitude, and PLGFA encodes the local gravitational force angle that the center pixel exerts on all other pixels within a local neighborhood.
1	  We propose a novel noise resistance and the edge-preserving binary pattern called neighbors to center difference binary pattern (NCDBP) for gravitational force magnitude encoding.
2	  Finally, the histograms of the two components are concatenated to construct the PLGF descriptor.
2	  Experimental results on the existing face recognition databases, texture database, and biomedical image database show that PLGF is an effective image descriptor, and it outperforms other widely used existing descriptors.
2	  Even if in complicated variations like noise, and illumination with smaller databases, a combination of PLGF and convolutional neural network (CNN) performs consistently better than other state-of-the-art techniques.


### 78
##### 10.1109/TPAMI.2019.2932058
#### Hierarchical Deep Click Feature Prediction for Fine-grained Image Recognition


0	  The click feature of an image, defined as the user-click-frequency vector of the image on a pre-defined word vocabulary, is known to effectively reduce the semantic gap for fine-grained image recognition.
0	  Unfortunately, user-click-frequency data are usually absent in practice.
0	  It remains challenging to predict the click feature from the visual feature, because the user-click-frequency vector of an image is always noisy and sparse.
1	  In this paper, we devise a Hierarchical Deep Word Embedding (HDWE) model by integrating sparse constraints and an improved RELU operator to address click feature prediction from visual features.
1	  HDWE is a coarse-to-fine click feature predictor that is learned with the help of an auxiliary image dataset containing click information.
1	  It can therefore discover the hierarchy of word semantics.
2	  We evaluate HDWE on three dog and one bird image datasets, in which Clickture-Dog and Clickture-Bird are respectively utilized as auxiliary datasets to provide click data.
2	  Our empirical studies show that HDWE has 1) higher recognition accuracy, 2) a larger compression ratio, and 3) good one-shot learning ability and scalability to unseen categories.


### 79
##### 10.1109/TPAMI.2019.2931897
#### Selfie Video Stabilization


0	  We propose a novel algorithm for stabilizing selfie videos.
0	  Our goal is to automatically generate stabilized video that has optimal smooth motion in the sense of both foreground and background.
0	  The key insight is that non-rigid foreground motion in selfie videos can be analyzed using a 3D face model, and background motion can be analyzed using optical flow.
1	  We use second derivative of temporal trajectory of selected pixels as the measure of smoothness.
2	  Our algorithm stabilizes selfie videos by minimizing the smoothness measure of the background, regularized by the motion of the foreground.
2	  Experiments show that our method outperforms state-of-the-art general video stabilization techniques in selfie videos.


### 80
##### 10.1109/TPAMI.2019.2932062
#### Switchable Normalization for Learning-to-Normalize Deep Representation


1	  We address a learning-to-normalize problem by proposing Switchable Normalization (SN), which learns to select different normalizers for different normalization layers of a deep neural network.
1	  SN employs three distinct scopes to compute statistics (means and variances) including a channel, a layer, and a minibatch.
1	  SN switches between them by learning their importance weights in an end-to-end manner.
1	  It has several good properties.
1	  First, it adapts to various network architectures and tasks.
1	  Second, it is robust to a wide range of batch sizes, maintaining high performance even when small minibatch is presented (eg 2 images/GPU).
1	  Third, SN does not have sensitive hyper-parameter, unlike group normalization that searches the number of groups as a hyper-parameter.
1	  Without bells and whistles, SN outperforms its counterparts on various challenging benchmarks, such as ImageNet, COCO, CityScapes, ADE20K, MegaFace and Kinetics.
2	  Analyses of SN are also presented to answer the following three questions: 
2     (a) Is it useful to allow each normalization layer to select its own normalizer?
2	  (b) What impacts the choices of normalizers?
2	  (c) Do different tasks and datasets prefer different normalizers?
2	  We hope SN will help ease the usage and understand the normalization techniques in deep learning.
#	  The code of SN has been released at https://github.com/switchablenorms.


### 81
##### 10.1109/TPAMI.2019.2931577
#### Absolute Pose Estimation of Central Cameras Using Planar Regions


1	  A novel method is proposed for the absolute pose estimation of a central 2D camera with respect to 3D depth data without the use of any dedicated calibration pattern or explicit point correspondences.
1	  The proposed method has no specific assumption about the data source: plain depth information is expected from the 3D sensing device and a central camera is used to capture the 2D images.
1	  Both the perspective and omnidirectional central cameras are handled within a single generic camera model.
1	  Pose estimation is formulated as a 2D-3D nonlinear shape registration task which is solved without point correspondences or complex similarity metrics.
1	  It relies on a set of corresponding planar regions, and the pose parameters are obtained by solving an overdetermined system of nonlinear equations.
2	  The efficiency and robustness of the proposed method were confirmed on both large scale synthetic data and on real data acquired from various types of sensors.


### 82
##### 10.1109/TPAMI.2019.2931569
#### Dual Adversarial Transfer for Sequence Labeling


0	  We propose a new architecture for addressing sequence labeling, termed Dual Adversarial Transfer Network (DATNet).
1	  Specifically, the proposed DATNet includes two variants, i.e., DATNet-F and DATNet-P, which are proposed to explore effective feature fusion between high and low resource.
1	  To address the noisy and imbalanced training data, we propose a novel Generalized Resource-Adversarial Discriminator (GRAD) and adopt adversarial training to boost model generalization.
2	  We investigate the effects of different components of DATNet across different domains and languages, and show that significant improvement can be obtained especially for low-resource data.
2	  Without augmenting any additional hand-crafted features, we achieve state-of-the-art performances on CoNLL, Twitter, PTB-WSJ, OntoNotes and Universal Dependencies with three popular sequence labeling tasks, i.e. Named entity recognition (NER), Part-of-Speech (POS) Tagging and Chunking.


### 83
##### 10.1109/TPAMI.2019.2929519
#### Graph Embedding Using Frequency Filtering


0	  The target of graph embedding is to embed graphs in vector space such that the embedded feature vectors follow the differences and similarities of the source graphs.
1	  In this paper, a novel method named Frequency Filtering Embedding (FFE) is proposed which uses graph Fourier transform and Frequency filtering as a graph Fourier domain operator for graph feature extraction.
0	  Frequency filtering amplifies or attenuates selected frequencies using appropriate filter functions.
1	  Here, heat, anti-heat, part-sine and identity filter sets are proposed as the filter functions.
1	  A generalized version of FFE named GeFFE is also proposed by defining pseudo-Fourier operators.
1	  This method can be considered as a general framework for formulating some previously defined invariants in other works by choosing a suitable filter bank and defining suitable pseudo-Fourier operators.
1	  This flexibility empowers GeFFE to adapt itself to the properties of each graph dataset unlike the previous spectral embedding methods and leads to superior classification accuracy relative to the others.
2	  Utilizing the proposed part-sine filter set which its members filter different parts of the spectrum in turn improves the classification accuracy of GeFFE method.
2	  Additionally GeFFE resolves the cospectrality problem entirely in tested datasets.


### 84
##### 10.1109/TPAMI.2019.2931317
#### Reconstruct as Far as You Can: Consensus of Non-Rigid Reconstruction from Feasible Regions


0	  Much progress has been made for non-rigid structure from motion during the last two decades, which made it possible to provide reasonable solutions for benchmark data.
0	  In order to utilize NRSfM techniques in realistic situations, however, we are facing two problems that must be solved: First, general scenes contain complex deformations as well as multiple objects, which violates usual assumptions of previous proposals.
0	  Second, there are many unreconstructable regions in the video, either because of discontinued tracks of 2D trajectories or those regions static towards camera, which require careful manipulations.
1	  In this paper, we show that a consensus-based reconstruction framework can handle these issues effectively.
1	  Even though the entire scene is complex, its parts usually have simpler deformations, and even though there are some unreconstructable parts, they can be weeded out to reduce harmful effect on the entire reconstruction.
1	  The main difficulty lies in identifying appropriate parts, however, it can be effectively avoided by sampling parts stochastically and then aggregate their reconstructions afterwards.
2	  Experimental results show that the proposed method renews the state-of-the-art for popular benchmark data under much harsher environments, i.e, narrow camera view ranges, and it can reconstruct real-word videos effectively for as many areas as it can.


### 85
##### 10.1109/TPAMI.2019.2930985
#### Learning Continuous Face Age Progression: A Pyramid of GANs


0	  The two underlying requirements of face age progression, i.e., aging accuracy and identity permanence, are not well studied in the literature.
1	  This paper presents a novel generative adversarial network based approach to address the issues in a coupled manner.
1	  It separately models the constraints for intrinsic subject-specific characteristics and age-specific facial changes w.r.t. the elapsed time, ensuring that the generated faces present desired aging effects while keeping personalized properties stable.
1	  To render photo-realistic facial details, high-level age-specific features conveyed by the synthesized face are estimated by a pyramidal adversarial discriminator at multiple scales, which simulates aging effects in a finer way.
1	  Further, an adversarial learning scheme is introduced to simultaneously train a single generator and multiple parallel discriminators, resulting in smooth continuous face aging sequences.
2	  Our method is applicable even in the presence of variations in pose, expression, makeup, etc., achieving remarkably vivid aging effects.
2	  Quantitative evaluations by a COTS face recognition system demonstrate that the target age distributions are accurately recovered, and 99.88% and 99.98% age progressed faces are correctly verified at 0.001% FAR after transformations of approximately 28 and 23 years on MORPH and CACD, respectively.
2	  Both visual and quantitative assessments show that the approach advances the state-of-the-art.


### 86
##### 10.1109/TPAMI.2019.2929034
#### Visual Tracking via Dynamic Memory Networks


0	  Template-matching methods for visual tracking have gained popularity recently due to their good performance and fast speed.
0	  However, they lack effective ways to adapt to changes in the target object's appearance, making their tracking accuracy still far from state-of-the-art.
1	  In this paper, we propose a dynamic memory network to adapt the template to the target's appearance variations during tracking.
1	  The reading and writing process of the external memory is controlled by an LSTM network with the search feature map as input.
1	  A spatial attention mechanism is applied to concentrate the LSTM input on the potential target as the location of the target is at first unknown.
1	  To prevent aggressive model adaptivity, we apply gated residual template learning to control the amount of retrieved memory that is used to combine with the initial template.
1	  In order to alleviate the drift problem, we also design a "negative" memory unit that stores templates for distractors, which are used to cancel out wrong responses from the object template.
1	  To further boost the tracking performance, an auxiliary classification loss is added after the feature extractor part.
1	  Unlike tracking-by-detection methods where the object's information is maintained by the weight parameters of neural networks, which requires expensive online fine-tuning to be adaptable, our tracker runs completely feed-forward and adapts to the target's appearance changes by updating the external memory.
2	  Moreover, the capacity of our model is not determined by the network size as with other trackers - the capacity can be easily enlarged as the memory requirements of a task increase, which is favorable for memorizing long-term object information.
2	  Extensive experiments on the OTB and VOT datasets demonstrate that our trackers perform favorably against state-of-the-art tracking methods while retaining real-time speed.


### 87
##### 10.1109/TPAMI.2019.2930051
#### Rotation Averaging with the Chordal Distance: Global Minimizers and Strong Duality


0	  In this paper we explore the role of duality principles within the problem of rotation averaging, a fundamental task in a wide range of applications.
0	  In its conventional form, rotation averaging is stated as a minimization over multiple rotation constraints.
0	  As these constraints are non-convex, this problem is generally considered challenging to solve globally.
1	  We show how to circumvent this difficulty through the use of Lagrangian duality.
1	  While such an approach is well-known it is normally not guaranteed to provide a tight relaxation.
1	  Based on spectral graph theory, we analytically prove that in many cases there is no duality gap unless the noise levels are severe.
1	  This allows us to obtain certifiably global solutions to a class of important non-convex problems in polynomial time.
2	  We also propose an efficient, scalable algorithm that outperforms general purpose numerical solvers by a large margin and compares favourably to current state-of-the-art.
2	  Further, our approach is able to handle the large problem instances commonly occurring in structure from motion settings and it is trivially parallelizable.
2	  Experiments are presented for a number of different instances of both synthetic and real-world data.


### 88
##### 10.1109/TPAMI.2019.2930258
#### Every Pixel Counts ++: Joint Learning of Geometry and Motion with 3D Holistic Understanding


0	  Learning to estimate 3D geometry in a single frame and optical flow from consecutive frames by watching unlabeled videos via deep convolutional network has made significant progress recently.
0	  Current state-of-the-art (SoTA) methods treat the two tasks independently.
0	  One important assumption of the existing depth estimation methods is that the scenes contain no moving object.
1	  In this paper, we propose to address the two tasks as a whole, i.e. to jointly understand per-pixel 3D geometry and motion.
1	  This eliminates the need of static scene assumption and enforces the inherent geometrical consistency during the learning process, yielding significantly improved results for both tasks.
1	  We call our method as "Every Pixel Counts++" or "EPC++".
1	  Various loss terms are formulated to jointly supervise the learning across geometrical cues and effective adaptive training strategy is proposed to achieve better performance.
2	  Comprehensive experiments were conducted on datasets with different scenes, including driving scenario (KITTI 2012 and KITTI 2015 datasets), mixed outdoor/indoor scenes (Make3D) and synthetic animation (MPI Sintel dataset).
2	  Performance on the five tasks of depth estimation, optical flow estimation, odometry, moving object segmentation and scene flow estimation shows that our approach outperforms other SoTA methods, demonstrating the effectiveness of each module of our proposed method.


### 89
##### 10.1109/TPAMI.2019.2930501
#### Fast Exact Evaluation of Univariate Kernel Sums


1	  This paper presents new methodology for computationally efficient evaluation of univariate kernel sums.
1	  It is shown that a rich class of kernels allows for exact evaluation of functions expressed as a sum of kernels using simple recursions.
1	  Given an ordered sample the computational complexity is linear in the sample size.
2	  Direct applications to the estimation of denisties and their derivatives shows that the proposed approach is competitive with the state-of-the-art.
2	  Extensions to multivariate problems including independent component analysis and spatial smoothing illustrate the versatility of univariate kernel estimators, and highlight the efficiency and accuracy of the proposed approach.
2	  Multiple applications in image processing, including image deconvolution; denoising; and reconstruction are considered, showing that the proposed approach offers very promising potential in these fields.


### 90
##### 10.1109/TPAMI.2019.2929520
#### Deep Affinity Network for Multiple Object Tracking


0	  Multiple Object Tracking (MOT) plays an important role in solving many fundamental problems in video analysis and computer vision.
0	  Most MOT methods employ two steps: Object Detection and Data Association.
0	  The first step detects objects of interest in every frame of a video, and the second establishes correspondence between the detected objects in different frames to obtain their tracks.
0	  Object detection has made tremendous progress in the last few years due to deep learning.
0	  However, data association for tracking still relies on hand crafted constraints such as appearance, motion, spatial proximity, grouping etc. to compute affinities between the objects in different frames.
1	  In this paper, we harness the power of deep learning for data association in tracking by jointly modeling object appearances and their affinities between different frames in an end-to-end fashion.
1	  The proposed Deep Affinity Network (DAN) learns compact, yet comprehensive features of pre-detected objects at several levels of abstraction, and performs exhaustive pairing permutations of those features in any two frames to infer object affinities.
1	  DAN also accounts for multiple objects appearing and disappearing between video frames.
1	  We exploit the resulting efficient affinity computations to associate objects in the current frame deep into the previous frames for reliable on-line tracking.
2	  Our technique is evaluated on popular multiple object tracking challenges MOT15, MOT17 and UA-DETRAC.
2	  Comprehensive benchmarking under twelve evaluation metrics demonstrates that our approach is among the best performing techniques on the leader board for these challenges.
#	  The open source implementation of our work is available at https://github.com/shijieS/SST.git.


### 91
##### 10.1109/TPAMI.2019.2928806
#### Distributed Variational Representation Learning


0	  The problem of distributed representation learning is one in which multiple observations $X_1,\ldots,X_K$ are processed separately to learn as much information as possible about some source $Y$.
1	  We investigate this problem from information-theoretic grounds, through a generalization of Tishby's centralized Information Bottleneck (IB) method to the distributed setting.
1	  Specifically, $K\geq 2$ encoders, compress their observations $X_1,\ldots,X_K$ separately such that, collectively, the produced representations preserve as much information as possible about $Y$.
1	  We study both discrete memoryless (DM) and vector Gaussian data models.
1	  For the discrete model, we establish a single-letter characterization of the optimal tradeoff for a class of memoryless sources.
1	  For the vector Gaussian model, we provide an explicit characterization of the optimal complexity-relevance tradeoff.
1	  Furthermore, we develop a variational bound on the complexity-relevance tradeoff which generalizes the evidence lower bound (ELBO) to the distributed setting.
1	  We provide two algorithms to compute this bound: i) a Blahut-Arimoto type iterative algorithm which computes optimal complexity-relevance mappings by iterating over a set of self-consistent equations, and ii) a variational inference type algorithm in which the encoding mappings are parametrized by neural networks, the bound approximated by Markov sampling and optimized with stochastic gradient descent.
2	  Numerical results on synthetic and real datasets are provided to support the efficiency of the approaches and algorithms developed in this paper.


### 92
##### 10.1109/TPAMI.2019.2928550
#### Stereo Matching Using Multi-level Cost Volume and Multi-scale Feature Constancy


0	  For CNNs based stereo matching methods, cost volumes play an important role in achieving good matching accuracy.
1	  In this paper, we present an end-to-end trainable convolution neural network to fully use cost volumes for stereo matching.
1	  Our network consists of three sub-modules, i.e., shared feature extraction, initial disparity estimation, and disparity refinement.
1	  These sub-modules of our network are tightly-coupled, making it compact and easy to train.
1	  Moreover, we investigate the problem of developing a robust model to perform well across multiple datasets with different characteristics.
1	  We achieve this by introducing a two-stage finetuning scheme to gently transfer the model to target datasets.
1	  Specifically, in the first stage, the model is finetuned using both a large synthetic dataset and the target datasets with a relatively large learning rate, while in the second stage the model is trained using only the target datasets with a small learning rate.
2	  The proposed method is tested on several benchmarks including the Middlebury 2014, KITTI 2015, ETH3D 2017, and SceneFlow datasets.
2	  Experimental results show that our method achieves the state-of-the-art performance on all the datasets.
2	  The proposed method also won the 1st prize on the Stereo task of Robust Vision Challenge 2018.


### 93
##### 10.1109/TPAMI.2019.2929170
#### Confidence Propagation through CNNs for Guided Sparse Depth Regression


0	  Generally, convolutional neural networks (CNNs) process data on a regular grid, e.g. data generated by ordinary cameras.
0	  Designing CNNs for sparse and irregularly spaced input data is still an open research problem with numerous applications in autonomous driving, robotics, and surveillance.
1	  In this paper, we propose an algebraically-constrained normalized convolution layer for CNNs with highly sparse input that has a smaller number of network parameters compared to related work.
1	  We propose novel strategies for determining the confidence from the convolution operation and propagating it to consecutive layers.
1	  We also propose an objective function that simultaneously minimizes the data error while maximizing the output confidence.
1	  To integrate structural information, we also investigate fusion strategies to combine depth and RGB information in our normalized convolution network framework.
1	  In addition, we introduce the use of output confidence as an auxiliary information to improve the results.
2	  The capabilities of our normalized convolution network framework are demonstrated for the problem of scene depth completion.
2	  Comprehensive experiments are performed on the KITTI-Depth and the NYU-Depth-v2 datasets.
2	  The results clearly demonstrate that the proposed approach achieves superior performance while requiring only about 1-5% of the number of parameters compared to the state-of-the-art methods.


### 94
##### 10.1109/TPAMI.2019.2929166
#### Multiset Feature Learning for Highly Imbalanced Data Classification


0	  With the expansion of data, increasing imbalanced data has emerged.
0	  When the imbalance ratio (IR) of data is high, most existing imbalanced learning methods decline seriously in classification performance.
1	  In this paper, we systematically investigate the highly imbalanced data classification problem, and propose an uncorrelated cost-sensitive multiset learning (UCML) approach for it.
1	  Specifically, UCML first constructs multiple balanced subsets through random partition, and then employs the multiset feature learning (MFL) to learn discriminant features from the constructed multiset.
1	  To enhance the usability of each subset and deal with the non-linearity issue existed in each subset, we further propose a deep metric based UCML (DM-UCML) approach.
1	  DM-UCML introduces the generative adversarial network technique into the multiset constructing process, such that each subset can own similar distribution with the original dataset.
1	  To cope with the non-linearity issue, DM-UCML integrates deep metric learning with MFL, such that more favorable performance can be achieved.
1	  In addition, DM-UCML designs a new discriminant term to enhance the discriminability of learned metrics.
2	  Experiments on eight traditional highly class-imbalanced datasets and two large-scale datasets indicate that: the proposed approaches outperform state-of-the-art highly imbalanced learning methods and are more robust to high IR.


### 95
##### 10.1109/TPAMI.2019.2929257
#### OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields


0	  Realtime multi-person 2D pose estimation is a key component in enabling machines to have an understanding of people in images and videos.
1	  In this work, we present a realtime approach to detect the 2D pose of multiple people in an image.
1	  The proposed method uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image.
2	  This bottom-up system achieves high accuracy and realtime performance, regardless of the number of people in the image.
0	  In previous work, PAFs and body part location estimation were refined simultaneously across training stages.
2	  We demonstrate that using a PAF-only refinement is able to achieve a substantial increase in both runtime performance and accuracy.
2	  We also present the first combined body and foot keypoint detector, based on an annotated foot dataset that we have publicly released.
2	  We show that the combined detector not only reduces the inference time compared to running them sequentially, but also maintains the accuracy of each component individually.
2	  This work has culminated in the release of OpenPose, the first open-source realtime system for multi-person 2D pose detection, including body, foot, hand, and facial keypoints.


### 96
##### 10.1109/TPAMI.2019.2929146
#### 3D Rigid Motion Segmentation with Mixed and Unknown Number of Models


0	  Many real-world video sequences cannot be conveniently categorized as general or degenerate; in such cases, imposing a false dichotomy in using the fundamental matrix or homography model for motion segmentation on video sequences would lead to difficulty.
0	  Even when we are confronted with a general scene-motion, the fundamental matrix approach as a model for motion segmentation still suffers from several defects, which we discuss in this paper.
0	  The full potential of the fundamental matrix approach could only be realized if we judiciously harness information from the simpler homography model.
1	  From these considerations, we propose a multi-model spectral clustering framework that synergistically combines multiple models (homography and fundamental matrix) together.
1	  We show that the performance can be substantially improved in this way.
0	  For general motion segmentation tasks, the number of independently moving objects is often unknown a priori and needs to be estimated from the observations.
0	  This is referred to as model selection and it is essentially still an open research problem.
1	  In this work, we propose a set of model selection criteria balancing data fidelity and model complexity.
2	  We perform extensive testing on existing motion segmentation datasets with both segmentation and model selection tasks, achieving state-of-the-art performance on all of them; we also put forth a more realistic and challenging dataset adapted from the KITTI benchmark, containing real-world effects such as strong perspectives and strong forward translations not seen in the traditional datasets.


### 97
##### 10.1109/TPAMI.2019.2929038
#### Learning with privileged information via adversarial discriminative modality distillation


0	  Heterogeneous data modalities can provide complementary cues for several tasks, usually leading to more robust algorithms and better performance.
0	  However, while training data can be accurately collected to include a variety of sensory modalities, it is often the case that not all of them are available in real life (testing) scenarios, where a model has to be deployed.
0	  This raises the challenge of how to extract information from multimodal data in the training stage, in a form that can be exploited at test time, considering limitations such as noisy or missing modalities.
1	  This paper presents a new approach in this direction for RGB-D vision tasks, developed within the adversarial learning and privileged information frameworks.
1	  We consider the practical case of learning representations from depth and RGB videos, while relying only on RGB data at test time.
1	  We propose a new approach to train a hallucination network that learns to distill depth information via adversarial learning, resulting in a clean approach without several losses to balance or hyperparameters.
2	  We report state-of-the-art results for object classification on the NYUD dataset, and video action recognition on the largest multimodal dataset available for this task, the NTU RGB+D, as well as on the Northwestern-UCLA.


### 98
##### 10.1109/TPAMI.2019.2929036
#### Pose-Guided Representation Learning for Person Re-Identification


0	  The large pose variations and misalignment errors exhibited by person images significantly increase the difficulty of person Re-Identification (ReID).
0	  Existing works commonly apply extra operations like pose estimation, part segmentation, etc., to alleviate those issues and improve the robustness of pedestrian representations.
0	  While boosting the ReID accuracy, those operations introduce considerable computational overheads and make the deep models complex and hard to tune.
1	  To chase a more efficient solution, we propose a Part-Guided Representation (PGR) composed of Pose Invariant Feature (PIF) and Local Descriptive Feature (LDF), respectively.
1	  We call PGR "Part-Guided" because it is trained and supervised by local part cues.
1	  Specifically, PIF approximates a pose invariant representation inferred by pose estimation and pose normalization.
1	  LDF focuses on discriminative body parts by approximating a representation learned with body region segmentation.
1	  In this way, extra pose extraction is only introduced during the training stage to supervise the learning of PGR, but is not required during the testing stage for feature extraction.
2	  Extensive comparisons with recent works on five widely used datasets demonstrate the competitive accuracy and efficiency of PGR.


### 99
##### 10.1109/TPAMI.2019.2929043
#### Robust Low-Rank Tensor Recovery with Rectification and Alignment


0	  Low-rank tensor recovery in the presence of sparse but arbitrary errors is an important problem with many practical applications.
1	  In this work, we propose a general framework that recovers low-rank tensors, in which the data can be deformed by some unknown transformations and corrupted by arbitrary sparse errors.
1	  We give a unified presentation of the surrogate-based formulations that incorporate the features of rectification and alignment simultaneously, and establish worst-case error bounds of the recovered tensor.
0	  In this context, the state-of-the-art methods ‘RASL’ and ‘TILT’ can be viewed as two special cases of our work, and yet each only performs part of the function of our method.
1	  Subsequently, we study the optimization aspects of the problem in detail by deriving two algorithms, one based on the alternating direction method of multipliers (ADMM) and the other based on proximal gradient.
1	  We provide convergence guarantees for the latter algorithm, and demonstrate the performance of the former through in-depth simulations.
2	  Finally, we present extensive experimental results on public datasets to demonstrate the effectiveness and efficiency of the proposed framework and algorithms.


### 100
##### 10.1109/TPAMI.2019.2927975
#### On Learning 3D Face Morphable Model from In-the-wild Images


0	  As a classic statistical model of 3D facial shape and albedo, 3D Morphable Model (3DMM) is widely used in facial analysis, e.g., model fitting, image synthesis.
0	  Conventional 3DMM is learned from a set of well-controlled 2D face images with associated 3D face scans, and represented by two sets of PCA basis functions.
0	  Due to the type and amount of training data, as well as, the linear bases, the representation power of 3DMM can be limited.
1	  To address these problems, this paper proposes an innovative framework to learn a nonlinear 3DMM model from a large set of unconstrained face images, without collecting 3D face scans.
1	  Specifically, given a face image as input, a network encoder estimates the projection, illumination, shape and albedo parameters.
1	  Two decoders serve as the nonlinear 3DMM to map from the shape and albedo parameters to the 3D shape and albedo, respectively.
1	  With the projection parameter, lighting, 3D shape, and albedo, a novel analytically-differentiable rendering layer is designed to reconstruct the original input face.
1	  The entire network is end-to-end trainable with only weak supervision.
2	  We demonstrate the superior representation power of our nonlinear 3DMM over its linear counterpart, and its contribution to face alignment and 3D reconstruction.


### 101
##### 10.1109/TPAMI.2019.2928294
#### Leader-based Multi-Scale Attention Deep Architecture for Person Re-identification


0	  Person Re-identification (re-id) aims to match people across non-overlapping camera views in a public space.
0	  It is a challenging problem because many people captured in surveillance videos often wear similar clothes.
0	  Consequently, the differences in their appearance are typically subtle and only detectable at the right locations and scales.
1	  In this paper, a deep re-id network is proposed consisting of two novel components: a multi-scale deep learning layer and a leader-based attention learning layer.
1	  With these components, our model is able to learn deep discriminative feature representations at different scales and automatically determine the optimal weightings for each scale when combining them.
1	  The importance of different spatial locations for extracting discriminative features is also learned explicitly via our leader-based attention module.
2	  Extensive experiments are carried out to demonstrate that the proposed model outperforms the state-of-the-art on a number of benchmarks, and has a better generalization ability under a domain generalization setting.


### 102
##### 10.1109/TPAMI.2019.2928296
#### DoubleFusion: Real-time Capture of Human Performances with Inner Body Shapes from a Single Depth Sensor


1	  We propose DoubleFusion, a new real-time system that combines volumetric non-rigid reconstruction with data-driven template fitting to simultaneously reconstruct detailed surface geometry, large non-rigid motion, and the optimized human body shape from a single depth camera.
1	  One of the key contributions of this method is a double-layer representation consisting of a complete parametric body model inside, and a gradually fused detailed surface outside.
2	  A pre-defined node graph on the body parameterizes the non-rigid deformations near the body, and a free-form dynamically changing graph parameterizes the outer surface layer far from the body, which allows more general reconstruction.
1	  We further propose a joint motion tracking method based on the double-layer representation to enable robust and fast motion tracking performance.
1	  Moreover, the inner parametric body is optimized online and forced to fit inside the outer surface layer as well as the live depth input.
2	  Overall, our method enables increasingly denoised, detailed and complete surface reconstructions, fast motion tracking performance and plausible inner body shape reconstruction in real-time.
2	  Experiments and comparisons show improved fast motion tracking and loop closure performance on more challenging scenarios.
2	  Two extended applications including body measurement and shape retargeting show the potential of our system in terms of practical use.


### 103
##### 10.1109/TPAMI.2019.2928540
#### Coherence Constrained Graph LSTM for Group Activity Recognition


0	  This work aims to address the group activity recognition problem by exploring human motion characteristics.
0	  Traditional methods hold that the motions of all persons contribute equally to the group activity, which suppresses the contributions of some relevant motions to the whole activity while overstates some irrelevant motions.
1	  To handle this problem, we present a Spatio-Temporal Context Coherence (STCC) constraint and a Global Context Coherence (GCC) constraint to capture the relevant motions and quantify their contributions to the group activity, respectively.
1	  Based on this, we propose a novel Coherence Constrained Graph LSTM (CCG-LSTM) with STCC and GCC to effectively recognize group activity, by modeling the relevant motions of individuals while suppressing the irrelevant motions.
1	  Specifically, to capture the relevant motions, we build the CCG-LSTM with a temporal confidence gate and a spatial confidence gate to control the memory state updating in terms of the temporally previous state and the spatially neighboring states, respectively.
1	  Besides, an attention mechanism is employed to quantify the contribution of a certain motion by measuring the consistency between itself and the whole activity at each time step.
2	  Finally, we conduct experiments on two widely-used datasets to illustrate the effectiveness of the proposed CCG-LSTM compared with the state-of-the-arts methods.


### 104
##### 10.1109/TPAMI.2019.2926728
#### Joint Task-Recursive Learning for RGB-D Scene Understanding


0	  RGB-D scene understanding under monocular camera is an emerging and challenging topic with many potential applications.
1	  In this paper, we propose a novel Task-Recursive Learning (TRL) framework to jointly and recurrently conduct three representative tasks therein containing depth estimation, surface normal prediction and semantic segmentation.
1	  TRL recursively refines the prediction results through a series of task-level interactions, where one-time cross-task interaction is abstracted as one network block of one time stage.
1	  In each stage, we serialize multiple tasks into a sequence and then recursively perform their interactions.
1	  To adaptively enhance counterpart patterns, we encapsulate interactions into a specific Task-Attentional Module (TAM) to mutually-boost the tasks from each other.
1	  Across stages, the historical experiences of previous states of tasks are selectively propagated into the next stages by using Feature-Selection unit (FS-Unit), which takes advantage of complementary information across tasks.
2	  The sequence of task-level interactions are also evolved along a coarse-to-fine scale space such that the required details may be refined progressively.
2	  Finally the task-abstracted sequence problem of multi-task prediction is framed into a recursive network.
2	  Extensive experiments on NYU-Depth v2 and SUN RGB-D datasets demonstrate that our method can recursively refines the results of the triple tasks and achieves state-of-the-art performance.


### 105
##### 10.1109/TPAMI.2019.2927909
#### A Microfacet-based Model for Photometric Stereo with General Isotropic Reflectance


1	  This paper presents a precise, stable, and invertible reflectance model for photometric stereo.
1	  This microfacet-based model is applicable to all types of isotropic surface reflectance, covering cases from diffusion to specular reflections.
1	  We introduce a single variable to physically quantify the surface smoothness, and by monotonically sliding this variable between 0 and 1, our model enables a versatile representation that can smoothly transform between an ellipsoid of revolution and the equation for Lambertian reflectance.
1	  In the inverse domain, this model offers a compact and physically interpretable formulation, for which we introduce a fast and lightweight solver that allows accurate estimations for both surface smoothness and surface shape.
2	  Finally, extensive experiments on the appearances of synthesized and real objects evidence that this model is state-of-the-art in our off-the-shelf solution.


### 106
##### 10.1109/TPAMI.2019.2927476
#### Recipe1M+: A Dataset for Learning Cross-Modal Embeddings for Cooking Recipes and Food Images


1	  In this paper, we introduce Recipe1M+, a new large-scale, structured corpus of over one million cooking recipes and 13 million food images.
1	  As the largest publicly available collection of recipe data, Recipe1M+ affords the ability to train high-capacity models on aligned, multi-modal data.
1	  Using these data, we train a neural network to learn a joint embedding of recipes and images that yields impressive results on an image-recipe retrieval task.
2	  Moreover, we demonstrate that regularization via the addition of a high-level classification objective both improves retrieval performance to rival that of humans and enables semantic vector arithmetic.
2	  We postulate that these embeddings will provide a basis for further exploration of the Recipe1M+ dataset and food and cooking in general.
#	  Code, data and models are publicly available.


### 107
##### 10.1109/TPAMI.2019.2927311
#### Blind Deblurring of Barcodes via Kullback-Leibler Divergence


0	  Barcode encoding schemes impose symbolic constraints which fix certain segments of the image.
1	  We present, implement, and assess a method for blind deblurring and denoising based entirely on Kullback-Leibler divergence.
1	  The method is designed to incorporate and exploit the full strength of barcode symbologies.
1	  Via both standard barcode reading software and smartphone apps, we demonstrate the remarkable ability of our method to blindly recover simulated images of highly blurred and noisy barcodes.
2	  As proof of concept, we present one application on a real-life out of focus camera image.


### 108
##### 10.1109/TPAMI.2019.2927203
#### Relative Saliency and Ranking: Models, Metrics, Data and Benchmarks


0	  Salient object detection is a problem that has been considered in detail and many solutions have been proposed.
0	  In this paper, we argue that work to date has addressed a problem that is relatively ill-posed.
0	  Specifically, there is not universal agreement about what constitutes a salient object when multiple observers are queried.
0	  This implies that some objects are more likely to be judged salient than others, and implies a relative rank exists on salient objects.
1	  Initially, we present a novel deep learning solution based on a hierarchical representation of relative saliency and stage-wise refinement.
1	  Further to this, we present data, analysis and baseline benchmark results towards addressing the problem of salient object ranking.
1	  Methods for deriving suitable ranked salient object instances are presented, along with metrics suitable to measuring algorithm performance.
1	  In addition, we show how a derived dataset can be successively refined to provide cleaned results that correlate well with pristine ground truth in its characteristics and value for training and testing models.
2	  Finally, we provide a comparison among prevailing algorithms that address salient object ranking or detection to establish initial baselines providing a basis for comparison with future efforts addressing this problem.
2	  The source code and data are publicly available via our project page: ryersonvisionlab.github.io/cocosalrank


### 109
##### 10.1109/TPAMI.2019.2926266
#### Variational Context: Exploiting Visual and Textual Context for Grounding Referring Expressions


0	  We focus on grounding (i.e., localizing or linking) referring expressions in images, e.g., "largest elephant standing behind baby elephant".
0	  This is a general yet challenging vision-language task since it does not only require the localization of objects, but also the multimodal comprehension of context - visual attributes (e.g., "largest", "baby") and relationships (e.g., "behind") that help to distinguish the referent from other objects, especially those of the same category.
0	  Due to the exponential complexity involved in modeling the context associated with multiple image regions, existing work oversimplifies this task to pairwise region modeling by multiple instance learning.
1	  In this paper, we propose a variational Bayesian method, called Variational Context, to solve the problem of complex context modeling in referring expression grounding.
1	  Our model exploits the reciprocal relation between the referent and context, i.e., either of them influences estimation of the posterior distribution of the other, and thereby the search space of context can be greatly reduced.
2	  In addition, our proposed Variational Context framework can automatically unify both referring expression comprehension and generation.Context-aware referring exrepssion generation helps to evaluate the estimated context and punish false gounding results in the unified framework.


### 110
##### 10.1109/TPAMI.2019.2926459
#### Local LDA: Open-Ended Learning of Latent Topics for 3D Object Recognition


0	  Service robots are expected to be more autonomous and work effectively in human-centric environments.
0	  This implies that robots should have special capabilities, such as learning from past experiences and real-time object category recognition.
1	  This paper proposes an open-ended 3D object recognition system which concurrently learns both the object categories and the statistical features for encoding objects.
1	  In particular, we propose an extension of Latent Dirichlet Allocation to learn structural semantic features (i.e. visual topics), from low-level feature co-occurrences, for each category independently.
1	  Moreover, topics in each category are discovered in an unsupervised fashion and are updated incrementally using new object views.
1	  In this way, the advantages of both the (hand-crafted) local features and the (learned) structural semantic features have been considered and combined in an efficient way.
2	  An extensive set of experiments has been performed to assess the performance of the proposed Local-LDA in terms of descriptiveness, scalability, and computation time.
2	  Experimental results show that the overall classification performance obtained with Local-LDA is clearly better than the best performances obtained with the state-of-the-art approaches.
2	  Moreover, the best scalability, in terms of number of learned categories, was obtained with the proposed Local-LDA approach, closely followed by a Bag-of-Words (BoW) approach.


### 111
##### 10.1109/TPAMI.2019.2926463
#### The ApolloScape Open Dataset for Autonomous Driving and its Application


0	  Autonomous driving has attracted tremendous attention especially in the past few years.
0	  The key techniques for a self-driving car include solving tasks like 3D map construction, self-localization, parsing the driving road and understanding objects, which enable vehicles to reason and act.
0	  However, large scale data set for training and system evaluation is still a bottleneck for developing robust perception models.
1	  In this paper, we present the ApolloScape dataset [1] and its applications for autonomous driving.
1	  Compared with existing public datasets from real scenes, e.g., KITTI [2] or Cityscapes [3], ApolloScape contains much large and richer labelling including holistic semantic dense point cloud for each site, stereo, per-pixel semantic labelling, lanemark labelling, instance segmentation, 3D car instance, high accurate location for every frame in various driving videos from multiple sites, cities and daytimes.
1	  For each task, it contains at lease 15x larger amount of images than SOTA datasets.
1	  To label such a complete dataset, we develop various tools and algorithms specified for each task to accelerate the labelling process, such as joint 3D-2D segment labeling, active labelling in videos etc.
1	  Depend on ApolloScape, we are able to develop algorithms jointly consider the learning and inference of multiple tasks.
1	  In this paper, we provide a sensor fusion scheme integrating camera videos, consumer-grade motion sensors (GPS/IMU), and a 3D semantic map in order to achieve robust self-localization and semantic segmentation for autonomous driving.
2	  We show that practically, sensor fusion and joint learning of multiple tasks are beneficial to achieve a more robust and accurate system.
2	  We expect our dataset and proposed relevant algorithms can support and motivate researchers for further development of multi-sensor fusion and multi-task learning in the field of computer vision.


### 112
##### 10.1109/TPAMI.2019.2926357
#### Simultaneous Fidelity and Regularization Learning for Image Restoration


0	  Most existing non-blind restoration methods are based on the assumption that a precise degradation model is known.
0	  As the degradation process can only partially known or inaccurately modeled, images may not be well restored.
0	  For rain streak removal, although an input image can be decomposed into a scene layer and a rain streak layer, there exists no explicit formulation for modeling rain streaks and the composition with scene layer.
0	  For blind deconvolution, as estimation error of blur kernel is usually introduced, the subsequent non-blind deconvolution process does not restore the latent image well.
1	  We propose a principled algorithm within the maximum a posterior framework to tackle image restoration with a partially known or inaccurate degradation model.
1	  The residual caused by a partially known or inaccurate degradation model is spatially dependent and complexly distributed.
1	  With a training set of degraded and ground-truth image pairs, we parameterize and learn the fidelity term for a degradation model in a task-driven manner.
1	  The regularization term can also be learned along with the fidelity term, thereby forming a simultaneous fidelity and regularization learning model.
2	  Extensive experimental results demonstrate the effectiveness of the proposed model for image deconvolution with inaccurate blur kernels and rain streak removal.


### 113
##### 10.1109/TPAMI.2019.2926033
#### Community Detection Using Restrained Random-walk Similarity


1	  In this paper, we propose a restrained random-walk similarity method for detecting the community structures of graphs.
1	  The basic premise of our method is that the starting vertices of finite-length random walks are judged to be in the same community if the walkers pass similar sets of vertices.
1	  This idea is based on our consideration that a random walker tends to move in the community including the walker's starting vertex for some time after starting the walk.
1	  Therefore, the sets of vertices passed by random walkers starting from vertices in the same community must be similar.
1	  The idea is reinforced with two conditions.
1	  First, we exclude abnormal random walks.
1	  Random walks that depart from each vertex are executed many times, and vertices that are rarely passed by the walkers are excluded from the set of vertices that the walkers may pass.
1	  Second, we forcibly restrain random walks to an appropriate length.
1	  In our method, a random walk is terminated when the walker repeatedly visits vertices that they have already passed.
2	  Experiments on real-world networks demonstrate that our method outperforms previous techniques in terms of accuracy.


### 114
##### 10.1109/TPAMI.2019.2919824
#### Providing a Single Ground-truth for Illuminant Estimation for the ColorChecker Dataset


0	  The ColorChecker dataset is one of the most widely used image sets for evaluating and ranking illuminant estimation algorithms.
0	  However, this single set of images has at least 3 different sets of ground-truth (i.e.  correct answers) associated with it.
0	  In the literature it is often asserted that one algorithm is better than another when the algorithms in question have been tuned and tested with the different ground-truths.
1	  In this short correspondence we present some of the background as to why the 3 existing ground-truths are different and go on to make a new single and recommended set of correct answers.
2	  Experiments reinforce the importance of this work in that we show that the total ordering of a set of algorithms may be reversed depending on whether we use the new or legacy ground-truth data.


### 115
##### 10.1109/TPAMI.2019.2925793
#### A General Decoupled Learning Framework for Parameterized Image Operators


0	  Many different deep networks have been used to approximate, accelerate or improve traditional image operators.
0	  Among these traditional operators, many contain parameters which need to be tweaked to obtain the satisfactory results, which we refer to as "parameterized image operators".
0	  However, most existing deep networks trained for these operators are only designed for one specific parameter configuration, which does not meet the needs of real scenarios that usually require flexible parameters settings.
1	  To overcome this limitation, we propose a new decoupled learning algorithm to learn from the operator parameters to dynamically adjust the weights of a deep network for image operators, denoted as the base network.
1	  The learned algorithm is formed as another network, namely the weight learning network, which can be end-to-end jointly trained with the base network.
2	  Experiments demonstrate that the proposed framework can be successfully applied to many traditional parameterized image operators.
1	  To accelerate the parameter tuning for practical scenarios, the proposed framework can be further extended to dynamically change the weights of only one single layer of the base network while sharing most computation cost.
2	  We demonstrate that this cheap parameter-tuning extension of the proposed decoupled learning framework even outperforms the state-of-the-art alternative approaches.


### 116
##### 10.1109/TPAMI.2019.2924953
#### Border-Peeling Clustering


1	  In this paper, we present a novel non-parametric clustering technique.
1	  Our technique is based on the notion that each latent cluster is comprised of layers that surround its core, where the external layers, or border points, implicitly separate the clusters.
1	  Unlike previous techniques, such as DBSCAN, where the cores of the clusters are defined directly by their densities, here the latent cores are revealed by a progressive peeling of the border points.
1	  Analyzing the density of the local neighborhoods allows identifying the border points and associating them with points of inner layers.
2	  We show that the peeling process adapts to the local densities and characteristics to successfully separates adjacent clusters (of possibly different densities).
1	  We extensively tested our technique on large sets of labeled data, including high-dimensional datasets of deep features that were trained by a convolutional neural network.
2	  We show that our technique is competitive to other state-of-the-art non-parametric methods, using a fixed set of parameters throughout the experiments.


### 117
##### 10.1109/TPAMI.2019.2925347
#### Asymmetric Mapping Quantization for Nearest Neighbor Search


0	  Nearest neighbor search is a fundamental problem in computer vision and machine learning.
0	  The straightforward solution, linear scan, is both computationally and memory intensive in large scale high-dimensional cases, hence is not preferable in practice.
0	  Therefore, there have been a lot of interests in algorithms that perform approximate nearest neighbor (ANN) search.
1	  In this paper, we propose a novel addition-based vector quantization algorithm, Asymmetric Mapping Quantization (AMQ), to efficiently conduct ANN search.
1	  Unlike existing addition-based quantization methods that suffer from handling the problem caused by the norm of database vector, we map the query vector and database vector using different mapping functions to transform the computation of L-2 distance to inner product similarity, thus do not need to evaluate the norm of database vector.
1	  Moreover, we further propose Distributed Asymmetric Mapping Quantization (DAMQ) to enable AMQ to work on very large dataset by distributed learning.
2	  Extensive experiments on approximate nearest neighbor search and image retrieval validate the merits of the proposed AMQ and DAMQ.


### 118
##### 10.1109/TPAMI.2019.2924417
#### Revisiting Video Saliency Prediction in the Deep Learning Era


0	  Predicting where people look in static scenes, a.k.a visual saliency, has received significant research interests recently.
0	  However, relatively less effort has been spent in understanding visual attention over dynamic scenes.
0	  This work makes three contributions to video saliency research.
1	  First, we introduce a new benchmark, called DHF1K, for predicting fixations during dynamic scene free-viewing, which is a long-time need in this field.
1	  DHF1K consists of 1K high-quality, elaborately selected videos annotated by 17 observers using an eye tracker.
1	  The videos span a wide range of scenes, motions, object types and backgrounds.
1	  Second, we propose a novel video saliency model, called ACLNet, that augments the CNN-LSTM network with a supervised attention mechanism to enable fast end-to-end learning.
1	  The attention mechanism explicitly encodes static saliency information, thus allowing LSTM to focus on learning a more flexible temporal saliency representation.
1	  Such a design leverages existing large-scale static fixation datasets, avoids overfitting, and significantly improves training efficiency and testing performance.
1	  Third, we perform an extensive evaluation of state-of-the-art saliency models on three current datasets (i.e., DHF1K, Hollywood2, UCF sports).
2	  Experimental results over more than 1.2K testing videos containing 400K frames demonstrate that ACLNet outperforms other contenders and has a fast processing speed (40fps).


### 119
##### 10.1109/TPAMI.2019.2924428
#### A Framework of Composite Functional Gradient Methods for Generative Adversarial Models


0	  Generative adversarial networks (GAN) are trained through a minimax game between a generator and a discriminator to generate data that mimics observations.
0	  While being widely used, GAN training is known to be empirically unstable.
1	  This paper presents a new theory for generative adversarial methods that does not rely on the traditional minimax formulation.
1	  Our theory shows that with a strong discriminator, a good generator can be obtained by composite functional gradient learning, so that several distance measures (including the KL divergence and the JS divergence) between the probability distributions of real data and generated data are simultaneously improved after each functional gradient step until converging to zero.
2	  This new point of view leads to stable procedures for training generative models.
2	  It also gives a new theoretical insight into the original GAN.
2	  Empirical results on image generation show the effectiveness of our new method.


### 120
##### 10.1109/TPAMI.2019.2923998
#### Ranking-Preserving Cross-Source Learning for Image Retargeting Quality Assessment


0	  Image retargeting techniques adjust images into different sizes and have attracted much attention recently.
0	  Objective quality assessment (OQA) of image retargeting results is often desired to automatically select the best results.
0	  Existing OQA methods train a model using some benchmarks (e.g., RetargetMe), in which subjective scores evaluated by users are provided.
1	  Observing that it is challenging even for human subjects to give consistent scores for retargeting results of different source images (diff-source-results), in this paper we propose a learning-based OQA method that trains a General Regression Neural Network (GRNN) model based on relative scores --- which preserve the ranking --- of retargeting results of the same source image (same-source-results).
1	  In particular, we develop a novel training scheme with provable convergence that learns a common base scalar for same-source-results.
1	  With this source specific offset, our computed scores not only preserve the ranking of subjective scores for same-source-results, but also provide a reference to compare the diff-source-results.
1	  We train and evaluate our GRNN model using human preference data collected in RetargetMe.
1	  Moreover, we introduce a further subjective benchmark to evaluate the generalizability of different OQA methods.
2	  Experimental results demonstrate that our method outperforms ten representative OQA methods in ranking prediction.


### 121
##### 10.1109/TPAMI.2019.2923240
#### Bayesian Low-Tubal-Rank Robust Tensor Factorization with Multi-Rank Determination


0	  Robust tensor factorization is a fundamental problem in machine learning and computer vision, which aims at recovering tensors corrupted with outliers as a sum of the low-rank and sparse components.
0	  However, existing methods either suffer from limited modeling power in preserving low-rank structures, or have difficulties in determining the target tensor rank and the trade-off between the low-rank and sparse components.
1	  To address these problems, we propose a fully Bayesian treatment of robust tensor factorization along with a generalized sparsity-inducing prior.
1	  By adapting the recently proposed low-tubal-rank model in a generative manner, our method is effective in preserving low-rank structures.
1	  Moreover, benefiting from the proposed prior and the Bayesian framework, the proposed method can automatically determine the tensor rank while inferring the trade-off between the low-rank and sparse components.
1	  For model estimation, we develop a variational inference algorithm, and further improve its efficiency by reformulating the variational updates in the frequency domain.
2	  Experimental results on both synthetic and real-world datasets demonstrate the effectiveness of the proposed method in multi-rank determination as well as its superiority in image denoising and background modeling over state-of-the-art approaches.


### 122
##### 10.1109/TPAMI.2019.2923513
#### Zig-Zag Network for Semantic Segmentation of RGB-D Images


0	  Semantic segmentation of images requires an understanding of appearances of objects and their spatial relationships in scenes.
0	  The fully convolutional network (FCN) has been successfully applied to recognize objects' appearances, which are represented with RGB channels.
0	  Images augmented with depth channels provide more understanding of the geometric information of the scene in an image.
1	  In this paper, we present a multiple-branch neural network to utilize depth information to assist in the semantic segmentation of images.
1	  Our approach splits the image into layers according to the "scene-scale".
1	  We introduce the context-aware receptive field (CARF), which provides better control of the relevant context information of learned features.
1	  Each branch of the network is equipped with CARF to adaptively aggregate the context information of image regions, leading to a more focused domain that is easier to learn.
1	  Furthermore, we propose a new zig-zag architecture to exchange information between the feature maps at different levels, augmented by the CARFs of the backbone network and decoder network.
1	  With the flexible information propagation allowed by our zig-zag network, we enrich the context information of feature maps for the segmentation.
2	  We show that the zig-zag network achieves state-of-the-art performances on several public datasets.


### 123
##### 10.1109/TPAMI.2019.2923621
#### Photometric Depth Super-Resolution


1	  This study explores the use of photometric techniques (shape-from-shading and uncalibrated photometric stereo) for upsampling the low-resolution depth map from an RGB-D sensor to the higher resolution of the companion RGB image.
1	  A single-shot variational approach is first put forward, which is effective as long as the target's reflectance is piecewise-constant.
1	  It is then shown that this dependency upon a specific reflectance model can be relaxed by focusing on a specific class of objects (e.g., faces), and delegate reflectance estimation to a deep neural network.
2	  A multi-shot strategy based on randomly varying lighting conditions is eventually discussed.
2	  It requires no training or prior on the reflectance, yet this comes at the price of a dedicated acquisition setup.
2	  Both quantitative and qualitative evaluations illustrate the effectiveness of the proposed methods on synthetic and real-world scenarios.


### 124
##### 10.1109/TPAMI.2019.2923201
#### Clouds of Oriented Gradients for 3D Detection of Objects, Surfaces, and Indoor Scene Layouts


0	  We develop new representations and algorithms for three-dimensional (3D) object detection and spatial layout prediction in cluttered indoor scenes.
1	  We first propose a clouds of oriented gradient (COG) descriptor that links the 2D appearance and 3D pose of object categories, and thus accurately models how perspective projection affects perceived image gradients.
1	  To better represent the 3D visual styles of large objects and provide contextual cues to improve the detection of small objects, we introduce latent support surfaces.
1	  We then propose a "Manhattan voxel" representation which better captures the 3D room layout geometry of common indoor environments.
1	  Effective classification rules are learned via a latent structured prediction framework.
2	  Contextual relationships among categories and layout are captured via a cascade of classifiers, leading to holistic scene hypotheses that exceed the state-of-the-art on the SUN RGB-D database.


### 125
##### 10.1109/TPAMI.2019.2922640
#### RotationNet for Joint Object Categorization and Unsupervised Pose Estimation from Multi-view Images


1	  We propose a Convolutional Neural Network (CNN)-based model "RotationNet," which takes multi-view images of an object as input and jointly estimates its pose and object category.
1	  Unlike previous approaches that use known viewpoint labels for training, our method treats the viewpoint labels as latent variables, which are learned in an unsupervised manner during the training using an unaligned object dataset.
1	  RotationNet uses only a partial set of multi-view images for inference, and this property makes it useful in practical scenarios where only partial views are available.
1	  Moreover, our pose alignment strategy enables one to obtain view-specific feature representations shared across classes, which is important to maintain high accuracy in both object categorization and pose estimation.
2	  Effectiveness of RotationNet is demonstrated by its superior performance to the state-of-the-art methods of 3D object classification on 10- and 40-class ModelNet datasets.
2	  We also show that RotationNet, even trained without known poses, achieves comparable performance to the state-of-the-art methods on an object pose estimation dataset.
2	  Furthermore, our object ranking method based on classification by RotationNet achieved the first prize in two tracks of the 3D Shape Retrieval Contest (SHREC) 2017.
2	  Finally, we demonstrate the performance of real-world applications of RotationNet trained with our newly created multi-view image dataset using a moving USB camera.


### 126
##### 10.1109/TPAMI.2019.2922959
#### One shot segmentation: unifying rigid detection and non-rigid segmentation using elastic regularization


0	  This paper proposes a novel approach for the non-rigid segmentation of deformable objects in image sequences, which is based on one-shot segmentation that unifies rigid detection and non-rigid segmentation using elastic regularization.
0	  The domain of application is the segmentation of a visual object that temporally undergoes a rigid transformation (e.g., affine transformation) and a non-rigid transformation (i.e., contour deformation).
0	  The majority of segmentation approaches to solve this problem are generally based on two steps that run in sequence: a rigid detection, followed by a non-rigid segmentation.
1	  In this paper, we propose a new approach, where both the rigid and non-rigid segmentation are performed in a single shot using a sparse low-dimensional manifold that represents the visual object deformations.
1	  Given the multi-modality of these deformations, the manifold partitions the training data into several patches, where each patch provides a segmentation proposal during the inference process.
1	  These multiple segmentation proposals are merged using the classification results produced by deep belief networks (DBN) that compute the confidence on each segmentation proposal.
1	  Thus, an ensemble of DBN classifiers is used for estimating the final segmentation.
2	  Compared to current methods proposed in the field, our proposed approach is advantageous in four aspects: 
2	  (i) it is a unified framework to produce rigid and non-rigid segmentations; 
2	  (ii) it uses an ensemble classification process, which can help the segmentation robustness; 
2	  (iii) it provides a significant reduction in terms of the number of dimensions of the rigid and non-rigid segmentations search spaces, compared to current approaches that divide these two problems; and 
2	  (iv) this lower dimensionality of the search space can also reduce the need for large annotated training sets to be used for estimating the DBN models.
2	  Experiments on the problem of left ventricle endocardial segmentation from ultrasound images, and lip segmentation from frontal facial images using the extended Cohn-Kanade (CK+) database, demonstrate the potential of the methodology through qualitative and quantitative evaluations, and the ability to reduce the search and training complexities without a significant impact on the segmentation accuracy.


### 127
##### 10.1109/TPAMI.2019.2922396
#### Towards Safe Weakly Supervised Learning


0	  In this paper, we study weakly supervised learning where a large amount of label information is not accessible.
0	  This includes incomplete supervision such as semi-supervised learning and domain adaptation; inexact supervision, such as multi-instance learning and inaccurate supervision, such as label noise learning.
0	  Unlike supervised learning, weakly supervised learning, however, may sometimes even degenerate performance.
0	  Such deficiency definitely hinders the deployment of weakly supervised learning to real applications.
0	  For this reason, it is desired to study safe weakly supervised learning.
1	  In this paper we present a generic ensemble learning scheme to derive the safe prediction.
1	  We consider optimizing the worst-case performance gain which leads to a maximin optimization.
2	  Our resultant formulation brings multiple advantages.
2	  Firstly, for many commonly used convex loss functions in classification and regression tasks, our formulation is guaranteed to derive a safe prediction under a mild condition.
2	  Secondly, prior knowledge related to the weight of the base weakly supervised learners can be flexibly embedded.
2	  Thirdly, our formulation can be globally and efficiently addressed.
2	  Finally, it is in an intuitive geometric interpretation.
2	  Extensive experiments on multiple weakly supervised learning tasks clearly demonstrate the effectiveness of our proposal algorithms.


### 128
##### 10.1109/TPAMI.2019.2922175
#### Vocabulary-informed Zero-shot and Open-set Learning


0	  Despite significant progress in object categorization, in recent years, a number of important challenges remain; mainly, ability to learn from limited labeled data and ability to recognize object classes within large, potentially open, set of labels.
0	  Zero-shot learning is one way of addressing these challenges, but it has only been shown to work with limited sized class vocabularies and typically requires separation between supervised and unsupervised classes, allowing former to inform the latter but not vice versa.
1	  We propose the notion of vocabulary-informed learning to alleviate the above mentioned challenges and address problems of supervised, zero-shot, generalized zero-shot and open set recognition using a unified framework.
1	  Specifically, we propose a weighted maximum margin framework for semantic manifold-based recognition that incorporates distance constraints from (both supervised and unsupervised) vocabulary atoms.
1	  Distance constraints ensure that labeled samples are projected closer to their correct prototypes, in the embedding space, than to others.
2	  We illustrate that resulting model shows improvements in supervised, zero-shot, generalized zero-shot, and large open set recognition, with up to 310K class vocabulary on AwA and ImageNet datasets.


### 129
##### 10.1109/TPAMI.2019.2922181
#### Object Detection from Scratch with Deep Supervision


0	  We propose Deeply Supervised Object Detectors (DSOD), an object detection framework that can be trained from scratch.
0	  Recent advances in object detection heavily depend on the off-the-shelf models pre-trained on large-scale classification datasets like ImageNet and OpenImage.
0	  However, one problem is that adopting pre-trained models from classification to detection task may incur learning bias due to the different objective function and diverse distributions of object categories.
0	  Techniques like fine-tuning on detection task could alleviate this issue to some extent but are still not fundamental.
0	  Furthermore, transferring these pre-trained models cross discrepant domains will be more difficult (e.g., from RGB to depth images).
0	  Thus, a better solution to handle these critical problems is to train object detectors from scratch, which motivates our proposed method.
1	  In DSOD, we contribute a set of design principles for learning object detectors from scratch.
1	  One of the key principles is the deep supervision, enabled by layer-wise dense connections in both backbone networks and prediction layers, plays a critical role in learning good detectors from scratch.
2	  We evaluate our method on PASCAL VOC 2007, 2012 and COCO datasets.
2	  DSOD achieves consistently better results than the state-of-the-art methods with much more compact models.


### 130
##### 10.1109/TPAMI.2019.2921960
#### Semantic Fisher Scores for Task Transfer: Using Objects to Classify Scenes


1	  The tranfer of a neural network (CNN) trained to recognize objects to the task of scene classification is considered.
1	  A Bag-of-Semantics (BoS) representation is first induced, by feeding scene image patches to the object CNN, and representing the scene image by the ensuing bag of posterior class probability vectors (semantic posteriors).
1	  The encoding of the BoS with a Fisher vector (FV) is then studied.
1	  A link is established between the FV of any probabilistic model and the Q-function of the expectation-maximization (EM) algorithm used to estimate its parameters by maximum likelihood.
1	  This enables 1) imediate derivation of FVs for any model for which an EM algorithm exists, and 2) leveraging efficient implementations from the EM literature for the computation of FVs.
2	  It is then shown that standard FVs, such as those derived from Gaussian or even Dirichelet mixtures, are unsuccessful for the transfer of semantic posteriors, due to the highly non-linear nature of the probability simplex.
2	  The analysis of these FVs shows that significant benefits can ensue by 1) designing FVs in the natural parameter space of the multinomial distribution, and 2) adopting sophisticated probabilistic models of semantic feature covariance.
2	  The combination of these two insights leads to the encoding of the BoS in the natural parameter space of the multinomial, using a vector of Fisher scores derived from a mixture of factor analyzers (MFA).
1	  A network implementation of the MFA Fisher Score (MFA-FS), denoted as the MFAFSNet, is finally proposed to enable end-to-end training.
2	  Experiments with various object CNNs and datasets show that the approach has state-of-the-art transfer performance.
2	  Somewhat surprisingly, the scene classification results are superior to those of a CNN explicitly trained for scene classification, using a large scene dataset (Places).
2	  This suggests that holistic analysis is insufficient for scene classification.
2	  The modeling of local object semantics appears to be at least equally important.
2	  The two approaches are also shown to be strongly complementary, leading to very large scene classification gains when combined, and outperforming all previous scene classification approaches by a sizeable margin.


### 131
##### 10.1109/TPAMI.2019.2921539
#### Two-Stream Region Convolutional 3D Network for Temporal Activity Detection


0	  We address the problem of temporal activity detection in continuous, untrimmed video streams.
0	  This is a difficult task that requires extracting meaningful spatio-temporal features to capture activities, accurately localizing the start and end times of each activity.
1	  We introduce a new model, Region Convolutional 3D Network (R-C3D), which encodes the video streams using a three-dimensional fully convolutional network, then generates candidate temporal regions containing activities and finally classifies selected regions into specific activities.
1	  Computation is saved due to the sharing of convolutional features between the proposal and the classification pipelines.
1	  We further improve the detection performance by efficiently integrating an optical flow based motion stream with the original RGB stream.
1	  The two-stream network is jointly optimized by fusing the flow and RGB feature maps at different levels.
1	  Additionally, the training stage incorporates an online hard example mining strategy to address the extreme foreground-background imbalance typically observed in any detection pipeline.
1	  Instead of heuristically sampling the candidate segments for the final activity classification stage, we rank them according to their performance and only select the worst performers to update the model.
1	  This improves the model without heavy hyper-parameter tuning.
2	  Extensive experiments on three benchmark datasets are carried out to show superior performance over existing temporal activity detection methods.
2	  Our model achieves state-of-the-art results on the THUMOS'14 and Charades datasets.
2	  We further demonstrate that our model is a general temporal activity detection framework that does not rely on assumptions about particular dataset properties by evaluating our approach on the ActivityNet dataset.


### 132
##### 10.1109/TPAMI.2019.2921548
#### Real-world Image Denoising with Deep Boosting


1	  We propose a Deep Boosting Framework (DBF) for real-world image denoising by integrating the deep learning technique into the boosting algorithm.
1	  The DBF replaces conventional handcrafted boosting units by elaborate convolutional neural networks, which brings notable advantages in terms of both performance and speed.
1	  We design a lightweight Dense Dilated Fusion Network (DDFN) as an embodiment of the boosting unit, which addresses the vanishing of gradients during training due to the cascading of networks while promoting the efficiency of limited parameters.
1	  The capabilities of the proposed method are first validated on several representative simulation tasks including non-blind and blind Gaussian denoising and JPEG image deblocking.
1	  We then focus on a practical scenario to tackle with the complex and challenging real-world noise.
1	  To facilitate leaning-based methods including ours, we build a new Real-world Image Denoising (RID) dataset, which contains 200 pairs of high-resolution images with diverse scene content under various shooting conditions.
1	  Moreover, we conduct comprehensive analysis on the domain shift issue for real-world denoising and propose an effective one-shot domain transfer scheme to address this issue.
2	  Comprehensive experiments on widely used benchmarks demonstrate that the proposed method significantly surpasses existing methods on the task of real-world image denoising.


### 133
##### 10.1109/TPAMI.2019.2921543
#### Guided Attention Inference Network


0	  With only coarse labels, weakly supervised learning typically uses top-down attention maps generated by back-propagating gradients as priors for tasks such as object localization and semantic segmentation.
0	  While these attention maps are intuitive and informative explanations of deep neural network, there is no effective mechanism to manipulate the network attention during learning process.
1	  In this paper, we address three shortcomings of previous approaches in modeling such attention maps in one common framework.
1	  First, we make attention maps a natural and explicit component in the training pipeline such that they are end-to-end trainable.
1	  Moreover, we provide self-guidance directly on these maps by exploring supervision from the network itself to improve them towards specific target tasks.
1	  Lastly, we proposed a design to seamlessly bridge the gap between using weak and extra supervision if available.
2	  Despite its simplicity, experiments on the semantic segmentation task demonstrate the effectiveness of our methods.
2	  Besides, the proposed framework provides a way not only explaining the focus of the learner but also feeding back with direct guidance towards specific tasks.
2	  Under mild assumptions our method can also be understood as a plug-in to existing convolutional neural networks to improve their generalization performance.


### 134
##### 10.1109/TPAMI.2019.2921574
#### CoRRN: Cooperative Reflection Removal Network


0	  Removing the undesired reflections from images taken through the glass is of broad application to various computer vision tasks.
0	  Non-learning based methods utilize different handcrafted priors such as the separable sparse gradients caused by different levels of blurs, which often fail due to their limited description capability to the properties of real-world reflections.
1	  In this paper, we propose a network with the feature-sharing strategy to tackle this problem in a cooperative and unified framework, by integrating image context information and the multi-scale gradient information.
1	  To remove the strong reflections existed in some local regions, we propose a statistic loss by considering the gradient level statistics between the background and reflections.
1	  Our network is trained on a new dataset with 3250 reflection images taken under diverse real-world scenes.
2	  Experiments on a public benchmark dataset show that the proposed method performs favorably against state-of-the-art methods.


### 135
##### 10.1109/TPAMI.2019.2921327
#### Joint Segmentation and Path Classification of Curvilinear Structures


0	  Detection of curvilinear structures in images has long been of interest.
0	  One of the most challenging aspects of this problem is inferring the graph representation of the curvilinear network.
0	  Most existing delineation approaches first perform binary segmentation of the image and then refine it using either a set of hand-designed heuristics or a separate classifier that assigns likelihood to paths extracted from the pixel-wise prediction.
1	  In our work, we bridge the gap between segmentation and path classification by training a deep network that performs those two tasks simultaneously.
2	  We show that this approach is beneficial because it enforces consistency across the whole processing pipeline.
1	  We apply our approach on roads and neurons datasets.


### 136
##### 10.1109/TPAMI.2019.2917908
#### Learning and Tracking the 3D Body Shape of Freely Moving Infants from RGB-D sequences


0	  Statistical models of the human body surface are generally learned from thousands of high-quality 3D scans in predefined poses to cover the wide variety of human body shapes and articulations.
0	  Acquisition of such data requires expensive equipment, calibration procedures, and is limited to cooperative subjects who can understand and follow instructions, such as adults.
1	  We present a method for learning a statistical 3D Skinned Multi-Infant Linear body model (SMIL) from incomplete, low-quality RGB-D sequences of freely moving infants.
1	  Quantitative experiments show that SMIL faithfully represents the RGB-D data and properly factorizes the shape and pose of the infants.
2	  To demonstrate the applicability of SMIL, we fit the model to RGB-D sequences of freely moving infants and show, with a case study, that our method captures enough motion detail for General Movements Assessment (GMA), a method used in clinical practice for early detection of neurodevelopmental disorders in infants.
2	  SMIL provides a new tool for analyzing infant shape and movement and is a step towards an automated system for GMA.


### 137
##### 10.1109/TPAMI.2019.2921031
#### Trace Quotient with Sparsity Priors for Learning Low Dimensional Image Representations


0	  This work studies the problem of learning appropriate low dimensional image representations.
1	  We propose a generic algorithmic framework, which leverages two classic representation learning paradigms, i.e., sparse representation and the trace quotient criterion, to disentangle underlying factors of variation in high dimensional images.
1	  Specifically, we aim to learn simple representations of low dimensional, discriminant factors by applying the trace quotient criterion to well-engineered sparse representations.
1	  We construct a unified cost function, coined as the SPARse LOW dimensional representation (SparLow) function, for jointly learning both a sparsifying dictionary and a dimensionality reduction transformation.
0	  The SparLow function is widely applicable for developing various algorithms in three classic machine learning scenarios, namely, unsupervised, supervised, and semi-supervised learning.
1	  In order to develop efficient joint learning algorithms for maximizing the SparLow function, we deploy a framework of sparse coding with appropriate convex priors to ensure the sparse representations to be locally differentiable.
1	  Moreover, we develop an efficient geometric conjugate gradient algorithm to maximize the SparLow function on its underlying Riemannian manifold.
2	  Performance of the proposed SparLow algorithmic framework is investigated on several image processing tasks, such as 3D data visualization, face/digit recognition, and object/scene categorization.


### 138
##### 10.1109/TPAMI.2019.2920899
#### Reconstruct and Represent Video Contents for Captioning via Reinforcement Learning


0	  In this paper, the problem of describing visual contents of a video sequence with natural language is addressed.
1	  Unlike previous video captioning work mainly exploiting the cues of video contents to make a description, we propose a reconstruction network (RecNet) in a novel encoder-decoder-reconstructor architecture, which leverages both forward (video to sentence) and backward (sentence to video) flows for video captioning.
1	  Specifically, the encoder-decoder component makes use of the forward flow to produce a sentence description based on the encoded video semantic features.
1	  Two types of reconstructors are subsequently proposed to employ backward flow and reproduce the video features from local and global perspectives, respectively, capitalizing on the hidden state sequence generated by the decoder.
1	  Moreover, in order to make a comprehensive reconstruction of the video features, we propose to fuse the two types of reconstructors together.
1	  The generation loss yielded by the encoder-decoder component and the reconstruction loss introduced by the reconstructor are jointly cast into training the proposed RecNet in an end-to-end fashion.
1	  Additionally, the RecNet is finetuned by CIDEr optimization via reinforcement learning, which significantly boosts the captioning performance.
2	  Experimental results on benchmark datasets demonstrate that the proposed reconstructor can boost the performance of video captioning consistently.


### 139
##### 10.1109/TPAMI.2019.2920821
#### Local Deformable 3D Reconstruction with Cartan's Connections


0	  3D reconstruction of deformable objects using inter-image visual motion from monocular images has been studied under Shape-from- Template (SfT) and Non-Rigid Structure-from-Motion (NRSfM).
0	  Most methods have been developed for simple deformation models, primarily isometry.
0	  They may treat a surface as a discrete set of points and draw constraints from the points only or they may use a non- parametric representation and use both points and differentials to express constraints.
1	  We propose a differential framework based on Cartan's theory of connections and moving frames.
1	  It is applicable to SfT and NRSfM, and to deformation models other than isometry.
1	  It utilises infinitesimal-level assumptions on the surface's geometry and mappings.
1	  It has the following properties.
1	  1) It allows one to derive existing solutions in a simpler way.
1	  2) It models SfT and NRSfM in a unified way.
1	  3) It allows us to introduce a new skewless deformation model and solve SfT and NRSfM for it.
1	  4) It facilitates a generic solution to SfT which does not require deformation modeling.
1	  Our framework is complete: it solves deformable 3D reconstruction for a whole class of algebraic deformation models including isometry.
2	  We compared our solutions with the state-of-the-art methods and show that ours outperform in terms of both accuracy and computation time.


### 140
##### 10.1109/TPAMI.2019.2920636
#### Gravitational Laws of Focus of Attention


0	  The understanding of the mechanisms behind focus of attention in a visual scene is a problem of great interest in visual perception and computer vision.
1	  In this paper, we describe a model of scanpath as a dynamic process which can be interpreted as a variational law somehow related to mechanics, where the focus of attention is subject to a gravitational field.
1	  The distributed virtual mass that drives eye movements is associated with the presence of details and motion in the video.
1	  Unlike most current models, the proposed approach does not estimate directly the saliency map, but the prediction of eye movements allows us to integrate over time the positions of interest.
1	  The process of inhibition-of-return is also supported in the same dynamic model with the purpose of simulating fixations and saccades.
1	  The differential equations of motion of the proposed model are numerically integrated to simulate scanpaths on both images and videos.
2	  Experimental results for the tasks of saliency and scanpath prediction on a wide collection of datasets are presented to support the theory.
2	  Top level performances are achieved especially in the prediction of scanpaths, which is the primary purpose of the proposed model.


### 141
##### 10.1109/TPAMI.2019.2920591
#### On the Convergence of Learning-based Iterative Methods for Nonconvex Inverse Problems


0	  Numerous tasks at the core of statistics, learning and vision areas are specific cases of ill-posed inverse problems.
0	  Recently, learning-based (e.g., deep) iterative methods have been empirically shown to be useful for these problems.
0	  Nevertheless, integrating learnable structures into iterations is still a laborious process, which can only be guided by intuitions or empirical insights.
0	  Moreover, there is a lack of rigorous analysis about the convergence behaviors of these reimplemented iterations, and thus the significance of such methods is a little bit vague.
1	  This paper moves beyond these limits and proposes Flexible Iterative Modularization Algorithm (FIMA), a generic and provable paradigm for nonconvex inverse problems.
2	  Our theoretical analysis reveals that FIMA allows us to generate globally convergent trajectories for learning-based iterative methods.
2	  Meanwhile, the devised scheduling policies on flexible modules should also be beneficial for classical numerical methods in the nonconvex scenario.
2	  Extensive experiments on real applications verify the superiority of FIMA.


### 142
##### 10.1109/TPAMI.2019.2919707
#### On the Robustness of Semantic Segmentation Models to Adversarial Attacks


0	  Deep Neural Networks (DNNs) have demonstrated exceptional performance on most recognition tasks such as image classification and segmentation.
0	  However, they have also been shown to be vulnerable to adversarial examples.
0	  This phenomenon has recently attracted a lot of attention but it has not been extensively studied on multiple, large-scale datasets and structured prediction tasks such as semantic segmentation which often require more specialised networks with additional components such as CRFs, dilated convolutions, skip-connections and multiscale processing.
1	  In this paper, we present what to our knowledge is the first rigorous evaluation of adversarial attacks on modern semantic segmentation models, using two large-scale datasets.
1	  We analyse the effect of different network architectures, model capacity and multiscale processing, and show that many observations made on the task of classification do not always transfer to this more complex task.
2	  Furthermore, we show how mean-field inference in deep structured models, multiscale processing (and more generally, input transformations) naturally implement recently proposed adversarial defenses.
2	  Our observations will aid future efforts in understanding and defending against adversarial examples.
2	  Moreover, in the shorter term, we show how to effectively benchmark robustness and show which segmentation models should currently be preferred in safety-critical applications due to their inherent robustness.


### 143
##### 10.1109/TPAMI.2019.2919597
#### MOSES: A Streaming Algorithm for Linear Dimensionality Reduction


1	  This paper introduces Memory-limited Online Sub- space Estimation Scheme (MOSES) for both estimating the principal components of streaming data and reducing its dimension.
1	  More specifically, in various applications such as sensor net- works, the data vectors are presented sequentially to a user who has limited storage and processing time available.
1	  Applied to such problems, MOSES can provide a running estimate of leading principal components of the data that has arrived so far and also reduce its dimension.
1	  MOSES generalises the popular incremental Singular Vale Decomposition (iSVD) to handle thin blocks of data, rather than just vectors.
1	  This minor generalisation in part allows us to complement MOSES with a comprehensive statistical analysis, thus providing the first theoretically-sound variant of iSVD, which has been lacking despite the empirical success of this method.
1	  This generalisation also enables us to concretely interpret MOSES as an approximate solver for the underlying non-convex optimisation program.
2	  We find that MOSES consistently surpasses the state of the art in our numerical experiments with both synthetic and real-world datasets, while being computationally inexpensive.


### 144
##### 10.1109/TPAMI.2019.2919616
#### Direction-aware Spatial Context Features for Shadow Detection and Removal


0	  Shadow detection and shadow removal are fundamental and challenging tasks, requiring an understanding of the global image semantics.
1	  This paper presents a novel deep neural network design for shadow detection and removal by analyzing the spatial image context in a direction-aware manner.
1	  To achieve this, we first formulate the direction-aware attention mechanism in a spatial recurrent neural network (RNN) by introducing attention weights when aggregating spatial context features in the RNN.
2	  By learning these weights through training, we can recover direction-aware spatial context (DSC) for detecting and removing shadows.
1	  This design is developed into the DSC module and embedded in a convolutional neural network to learn the DSC features at different levels.
1	  Moreover, we design a weighted cross entropy loss to make effective the training for shadow detection and further adopt the network for shadow removal by using a Euclidean loss function and formulating a color transfer function to address the color and luminosity inconsistencies in the training pairs.
1	  We employed two shadow detection benchmark datasets and two shadow removal benchmark datasets, and performed various experiments to evaluate our method.
2	  Experimental results show that our method performs favorably against the state-of-the-art methods for both shadow detection and shadow removal.


### 145
##### 10.1109/TPAMI.2019.2919301
#### DART: Distribution Aware Retinal Transform for Event-based Cameras


1	  We introduce a generic visual descriptor, termed as distribution aware retinal transform (DART), that encodes the structural context using log-polar grids for event cameras.
1	  The DART descriptor is applied to four different problems, namely object classification, tracking, detection and feature matching: 
1	  (1) The DART features are directly employed as local descriptors in a bag-of-words classification framework and testing is carried out on four standard event-based object datasets (N-MNIST, MNIST-DVS, CIFAR10-DVS, NCaltech-101); 
1	  (2) Extending the classification system, tracking is demonstrated using two key novelties: 
1	  (i) Statistical bootstrapping is leveraged with online learning for overcoming the low-sample problem during the one-shot learning of the tracker, 
1	  (ii) Cyclical shifts are induced in the log-polar domain of the DART descriptor to achieve robustness to object scale and rotation variations; 
1	  (3) To solve the long-term object tracking problem, an object detector is designed using the principle of cluster majority voting.
2	  The detection scheme is then combined with the tracker to result in a high intersection-over-union score with augmented ground truth annotations on the publicly available event camera dataset; (4) Finally, the event context encoded by DART greatly simplifies the feature correspondence problem, especially for spatio-temporal slices far apart in time, which has not been explicitly tackled in the event-based vision domain.


### 146
##### 10.1109/TPAMI.2019.2919303
#### Learning Low-Dimensional Temporal Representations with Latent Alignments


0	  Low-dimensional discriminative representations enhance machine learning methods in both performance and complexity.
0	  This has motivated supervised dimensionality reduction (DR), which transforms high-dimensional data into a discriminative subspace.
0	  Most DR methods require data to be i.i.d.
0	  However, in some domains, data naturally appear in sequences, where the observations are temporally correlated.
1	  We propose a DR method, namely, latent temporal linear discriminant analysis (LT-LDA), to learn low-dimensional temporal representations.
1	  We construct the separability among sequence classes by lifting the holistic temporal structures, which are established based on temporal alignments and may change in different subspaces.
1	  We jointly learn the subspace and the associated latent alignments by optimizing an objective that favors easily separable temporal structures.
1	  We show that this objective is connected to the inference of alignments and thus allows for an iterative solution.
2	  We provide both theoretical insight and empirical evaluations on several real-world sequence datasets to show the applicability of our method.


### 147
##### 10.1109/TPAMI.2019.2919308
#### A Functional Representation for Graph Matching


0	  Graph matching is an important and persistent problem in computer vision and pattern recognition for finding node-to-node correspondence between graphs.
0	  However, graph matching that incorporates pairwise constraints can be formulated as a quadratic assignment problem (QAP), which is NP-complete and results in intrinsic computational difficulties.
1	  This paper presents a functional representation for graph matching (FRGM) that aims to provide more geometric insights on the problem and reduce the space and time complexities.
1	  To achieve these goals, we represent each graph by a linear function space equipped with a functional such as inner product or metric, that has an explicit geometric meaning.
1	  Consequently, the correspondence matrix between graphs can be represented as a linear representation map.
1	  Furthermore, this map can be reformulated as a new parameterization for matching graphs in Euclidean space such that it is consistent with graphs under rigid or nonrigid deformations.
1	  This allows us to estimate the correspondence matrix and geometric deformations simultaneously.
2	  We use the representation of edge-attributes rather than the affinity matrix to reduce the space complexity and propose an efficient optimization strategy to reduce the time complexity.
2	  The experimental results on both synthetic and real-world datasets show that the FRGM can achieve state-of-the-art performance.


### 148
##### 10.1109/TPAMI.2019.2919284
#### Robust RGB-D Face Recognition Using Attribute-Aware Loss


0	  Existing convolutional neural network (CNN) based face recognition algorithms typically learn a discriminative feature mapping, using a loss function that enforces separation of features from different classes and/or aggregation of features within the same class.
0	  However, they may suffer from bias in the training data such as uneven sampling density, because they optimize the adjacency relationship of the learned features without considering the proximity of the underlying faces.
0	  Moreover, since they only use facial images for training, the learned feature mapping may not correctly indicate the relationship of other attributes such as gender and ethnicity, which can be important for some face recognition applications.
1	  In this paper, we propose a new CNN-based face recognition approach that incorporates such attributes into the training process.
2	  Using an attribute-aware loss function that regularizes the feature mapping using attribute proximity, our approach learns more discriminative features that are correlated with the attributes.
1	  We train our face recognition model on a large-scale RGB-D data set with over 100K identities captured under real application conditions.
2	  By comparing our approach with other methods on a variety of experiments, we demonstrate that depth channel and attribute-aware loss greatly improve the accuracy and robustness of face recognition.


### 149
##### 10.1109/TPAMI.2019.2918284
#### Convolutional Networks with Dense Connectivity


0	  Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output.
1	  In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion.
1	  Whereas traditional convolutional networks with L layers have L connections -- one between each layer and its subsequent layer -- our network has $\frac{L(L+1)}{2}$ direct connections.
1	  For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers.
0	  DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially improve parameter efficiency.
2	  We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet).
2	  DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less parameters and computation to achieve high performance.


### 150
##### 10.1109/TPAMI.2019.2918208
#### Learning Visual Instance Retrieval from Failure: Efficient Online Local Metric Adaptation from Negative Samples


0	  Existing visual instance retrieval (VIR) approaches attempt to learn a faithful global matching metric or discriminative feature embedding offline to cover enormous visual appearance variations, so as to directly use it online on various unseen probes for retrieval.
0	  However, their requirement for a huge set of positive training pairs is very demanding in practice and the performance is largely constrained for the unseen testing samples due to the severe data shifting issue.
1	  In contrast, this paper advocates a different paradigm: part of the learning can be performed online but with nominal costs, so as to achieve online metric adaptation for different query probes.
1	  By exploiting easily-available negative samples, we propose a novel solution to achieve the optimal local metric adaptation effectively and efficiently.
2	  The insight of our method is the local hard negative samples can actually provide tight constraints to fine tune the metric locally.
2	  Our local metric adaptation method is generally applicable to be used on top of any offline-learned baselines.
2	  In addition, this paper gives in-depth theoretical analyses of the proposed method to guarantee the reduction of the classification error both asymptotically and practically.
2	  Extensive experiments on various VIR tasks have confirmed our effectiveness and superiority.
